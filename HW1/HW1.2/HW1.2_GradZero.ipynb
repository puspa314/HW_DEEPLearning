{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d55a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from autograd_lib import autograd_lib\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856504ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "result_folder = \"finalResult_with_hessian4_final1\"\n",
    "\n",
    "if not os.path.exists(result_folder):\n",
    "    os.mkdir(result_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37952a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_func = lambda x : (torch.sin(5*np.pi*x)) /(5*np.pi*x) \n",
    "num_of_rows = 300\n",
    "X= torch.unsqueeze(torch.linspace(-1,1,num_of_rows),dim=1)\n",
    "Y = Y_func(X)\n",
    "dataset = TensorDataset(X,Y)\n",
    "data_loader = DataLoader(dataset,1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92633c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(X, Y, color = \"magenta\")\n",
    "plt.title('Function Plotting')\n",
    "plt.xlabel('Independent Varible')\n",
    "plt.ylabel('Dependent Varible')\n",
    "plt.savefig(result_folder+'/func1_plot1.2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d9beb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathRegressor(nn.Module):\n",
    "    def __init__(self, num_hidden=128):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def training_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                \n",
    "        loss = loss_fn(out, targets)    \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 \n",
    "        loss = loss_fn(out, targets)   \n",
    "        return {'val_loss': loss.detach()}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def train_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                \n",
    "        loss = loss_fn(out, targets)    \n",
    "        return {'train_loss': loss.detach()}\n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        batch_losses = [x['train_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  \n",
    "        return {'train_loss': epoch_loss.item()}\n",
    "    \n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch, result['val_loss']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "956ab334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for p in model.regressor.children():\n",
    "        if isinstance(p, nn.Linear):\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "\n",
    "    grad_mean = np.mean(grads) \n",
    "\n",
    "    return grad_mean\n",
    "\n",
    "def save_activations(layer, A, _):\n",
    "    activations[layer] = A\n",
    "\n",
    "def compute_hess(layer, _, B):\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl,ni->nli', B, A) \n",
    "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA)\n",
    "    \n",
    "def compute_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    \n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    with autograd_lib.module_hook(compute_hess):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "\n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        \n",
    "        # Replacing torch.symeig with torch.linalg.eigvalsh\n",
    "        h_eig = torch.linalg.eigvalsh(h, UPLO='U')  # 'U' for upper triangular matrix\n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "\n",
    "    ratio_mean = np.mean(minimum_ratio) \n",
    "\n",
    "    return ratio_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfad0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_minimal_ratio(model,criterion):\n",
    "\n",
    "    gradient_norm = compute_gradient_norm(model, criterion, X, Y)\n",
    "    minimum_ratio = compute_minimum_ratio(model, criterion, X, Y)\n",
    "\n",
    "    print('gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))\n",
    "    result = {}\n",
    "    result[\"grad_norm\"] = gradient_norm\n",
    "    result[\"ratio\"] = minimum_ratio\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e38ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, val_loader):\n",
    "    outputs = [model.validation_step(batch, loss_fn) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def evaluate2(model, loss_fn, train_loader):\n",
    "    outputs = [model.train_step(batch, loss_fn) for batch in train_loader]\n",
    "    return model.train_epoch_end(outputs)\n",
    "\n",
    "def get_grad_norm(model):\n",
    "    grad_all = 0.0\n",
    "    grad = 0\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad = (p.grad.cpu().data.numpy() ** 2).sum()\n",
    "        grad_all += grad\n",
    "        \n",
    "    grad_norm = grad_all ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def fit(epochs, lr, model, data_loader, criterion, opt_func):\n",
    "    history = []\n",
    "    comparing_epoch_loss = 1000.0\n",
    "    grad_norm_per_epoch = {}\n",
    "\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X)\n",
    "        loss = criterion(prediction, Y)\n",
    "        loss.backward()\n",
    "        grad_norm_per_epoch[epoch] = get_norm_minimal_ratio(model, criterion)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        result = evaluate(model, criterion, data_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        \n",
    "        if epoch == 900:\n",
    "            comparing_epoch_loss = result[\"val_loss\"]\n",
    "  \n",
    "    return history, grad_norm_per_epoch, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a77cd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_rows = 300\n",
    "lr = 0.0004\n",
    "gamma_lr_scheduler = 0.1 \n",
    "weight_decay = 1e-4\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2500\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\"\n",
    "filename = criterion_name+ optimizer_name+\".png\"\n",
    "grad_norm_name = \"_grad_norm_name1_2.png\"\n",
    "result_folder_name = \"result3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d32ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2000\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\"\n",
    "input_size=1\n",
    "output_size=1\n",
    "model= MathRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "633cc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd_lib.register(model_1)\n",
    "activations = defaultdict(int)\n",
    "hess = defaultdict(float)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d077eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.4016718566417694}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1 = evaluate(model,criterion,data_loader)\n",
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b36eb9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = criterion_name+ optimizer_name+\".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "201af232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,target = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "376f9391-d7c1-4ac3-a5fb-8c3b6506276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm: 2.383659452199936, minimum ratio: 0.703125\n",
      "Epoch [0], val_loss: 0.3954\n",
      "gradient norm: 2.450784344226122, minimum ratio: 0.6953125\n",
      "Epoch [1], val_loss: 0.4111\n",
      "gradient norm: 2.5180913619697094, minimum ratio: 0.72265625\n",
      "Epoch [2], val_loss: 0.4272\n",
      "gradient norm: 2.5854485370218754, minimum ratio: 0.71484375\n",
      "Epoch [3], val_loss: 0.4438\n",
      "gradient norm: 2.652941904962063, minimum ratio: 0.7421875\n",
      "Epoch [4], val_loss: 0.4607\n",
      "gradient norm: 2.720516160130501, minimum ratio: 0.69921875\n",
      "Epoch [5], val_loss: 0.4782\n",
      "gradient norm: 2.7881786674261093, minimum ratio: 0.734375\n",
      "Epoch [6], val_loss: 0.4960\n",
      "gradient norm: 2.8559238128364086, minimum ratio: 0.71875\n",
      "Epoch [7], val_loss: 0.5143\n",
      "gradient norm: 2.9237753264606, minimum ratio: 0.73828125\n",
      "Epoch [8], val_loss: 0.5330\n",
      "gradient norm: 2.991733245551586, minimum ratio: 0.73046875\n",
      "Epoch [9], val_loss: 0.5522\n",
      "gradient norm: 3.059774350374937, minimum ratio: 0.75390625\n",
      "Epoch [10], val_loss: 0.5718\n",
      "gradient norm: 3.127897422760725, minimum ratio: 0.74609375\n",
      "Epoch [11], val_loss: 0.5918\n",
      "gradient norm: 3.19608923047781, minimum ratio: 0.71875\n",
      "Epoch [12], val_loss: 0.6123\n",
      "gradient norm: 3.2643523551523685, minimum ratio: 0.7265625\n",
      "Epoch [13], val_loss: 0.6332\n",
      "gradient norm: 3.3327057994902134, minimum ratio: 0.73046875\n",
      "Epoch [14], val_loss: 0.6545\n",
      "gradient norm: 3.4011525362730026, minimum ratio: 0.71875\n",
      "Epoch [15], val_loss: 0.6763\n",
      "gradient norm: 3.469749730080366, minimum ratio: 0.72265625\n",
      "Epoch [16], val_loss: 0.6986\n",
      "gradient norm: 3.5383895710110664, minimum ratio: 0.73046875\n",
      "Epoch [17], val_loss: 0.7213\n",
      "gradient norm: 3.6071263402700424, minimum ratio: 0.7421875\n",
      "Epoch [18], val_loss: 0.7444\n",
      "gradient norm: 3.675946056842804, minimum ratio: 0.75390625\n",
      "Epoch [19], val_loss: 0.7680\n",
      "gradient norm: 3.7448699697852135, minimum ratio: 0.7265625\n",
      "Epoch [20], val_loss: 0.7920\n",
      "gradient norm: 3.8138678669929504, minimum ratio: 0.73046875\n",
      "Epoch [21], val_loss: 0.8165\n",
      "gradient norm: 3.883002392947674, minimum ratio: 0.7421875\n",
      "Epoch [22], val_loss: 0.8414\n",
      "gradient norm: 3.952174298465252, minimum ratio: 0.7265625\n",
      "Epoch [23], val_loss: 0.8668\n",
      "gradient norm: 4.0215198546648026, minimum ratio: 0.71875\n",
      "Epoch [24], val_loss: 0.8926\n",
      "gradient norm: 4.090937778353691, minimum ratio: 0.7265625\n",
      "Epoch [25], val_loss: 0.9189\n",
      "gradient norm: 4.16046417504549, minimum ratio: 0.7265625\n",
      "Epoch [26], val_loss: 0.9456\n",
      "gradient norm: 4.230110287666321, minimum ratio: 0.72265625\n",
      "Epoch [27], val_loss: 0.9728\n",
      "gradient norm: 4.299836881458759, minimum ratio: 0.7265625\n",
      "Epoch [28], val_loss: 1.0005\n",
      "gradient norm: 4.369661584496498, minimum ratio: 0.71875\n",
      "Epoch [29], val_loss: 1.0286\n",
      "gradient norm: 4.439679600298405, minimum ratio: 0.74609375\n",
      "Epoch [30], val_loss: 1.0571\n",
      "gradient norm: 4.509776666760445, minimum ratio: 0.73828125\n",
      "Epoch [31], val_loss: 1.0862\n",
      "gradient norm: 4.579967133700848, minimum ratio: 0.73828125\n",
      "Epoch [32], val_loss: 1.1156\n",
      "gradient norm: 4.650265634059906, minimum ratio: 0.71875\n",
      "Epoch [33], val_loss: 1.1456\n",
      "gradient norm: 4.720716953277588, minimum ratio: 0.75390625\n",
      "Epoch [34], val_loss: 1.1760\n",
      "gradient norm: 4.791325636208057, minimum ratio: 0.72265625\n",
      "Epoch [35], val_loss: 1.2069\n",
      "gradient norm: 4.862018249928951, minimum ratio: 0.7578125\n",
      "Epoch [36], val_loss: 1.2382\n",
      "gradient norm: 4.932861939072609, minimum ratio: 0.74609375\n",
      "Epoch [37], val_loss: 1.2701\n",
      "gradient norm: 5.003791645169258, minimum ratio: 0.734375\n",
      "Epoch [38], val_loss: 1.3024\n",
      "gradient norm: 5.074883036315441, minimum ratio: 0.7578125\n",
      "Epoch [39], val_loss: 1.3351\n",
      "gradient norm: 5.146124266088009, minimum ratio: 0.71484375\n",
      "Epoch [40], val_loss: 1.3684\n",
      "gradient norm: 5.2174337431788445, minimum ratio: 0.75390625\n",
      "Epoch [41], val_loss: 1.4021\n",
      "gradient norm: 5.288908198475838, minimum ratio: 0.7265625\n",
      "Epoch [42], val_loss: 1.4363\n",
      "gradient norm: 5.360563986003399, minimum ratio: 0.72265625\n",
      "Epoch [43], val_loss: 1.4710\n",
      "gradient norm: 5.432384513318539, minimum ratio: 0.7109375\n",
      "Epoch [44], val_loss: 1.5062\n",
      "gradient norm: 5.504337921738625, minimum ratio: 0.76953125\n",
      "Epoch [45], val_loss: 1.5418\n",
      "gradient norm: 5.5764153227210045, minimum ratio: 0.73046875\n",
      "Epoch [46], val_loss: 1.5780\n",
      "gradient norm: 5.648655630648136, minimum ratio: 0.73828125\n",
      "Epoch [47], val_loss: 1.6146\n",
      "gradient norm: 5.721155673265457, minimum ratio: 0.7421875\n",
      "Epoch [48], val_loss: 1.6517\n",
      "gradient norm: 5.793700635433197, minimum ratio: 0.71484375\n",
      "Epoch [49], val_loss: 1.6893\n",
      "gradient norm: 5.8664218708872795, minimum ratio: 0.7578125\n",
      "Epoch [50], val_loss: 1.7274\n",
      "gradient norm: 5.939337119460106, minimum ratio: 0.734375\n",
      "Epoch [51], val_loss: 1.7660\n",
      "gradient norm: 6.012430056929588, minimum ratio: 0.73828125\n",
      "Epoch [52], val_loss: 1.8052\n",
      "gradient norm: 6.085621543228626, minimum ratio: 0.73828125\n",
      "Epoch [53], val_loss: 1.8448\n",
      "gradient norm: 6.158989205956459, minimum ratio: 0.75\n",
      "Epoch [54], val_loss: 1.8849\n",
      "gradient norm: 6.232504948973656, minimum ratio: 0.73828125\n",
      "Epoch [55], val_loss: 1.9255\n",
      "gradient norm: 6.306240797042847, minimum ratio: 0.74609375\n",
      "Epoch [56], val_loss: 1.9666\n",
      "gradient norm: 6.380253925919533, minimum ratio: 0.72265625\n",
      "Epoch [57], val_loss: 2.0083\n",
      "gradient norm: 6.454407215118408, minimum ratio: 0.734375\n",
      "Epoch [58], val_loss: 2.0504\n",
      "gradient norm: 6.528844386339188, minimum ratio: 0.75\n",
      "Epoch [59], val_loss: 2.0931\n",
      "gradient norm: 6.603374347090721, minimum ratio: 0.75\n",
      "Epoch [60], val_loss: 2.1363\n",
      "gradient norm: 6.6781047731637955, minimum ratio: 0.71484375\n",
      "Epoch [61], val_loss: 2.1800\n",
      "gradient norm: 6.75303815305233, minimum ratio: 0.7421875\n",
      "Epoch [62], val_loss: 2.2242\n",
      "gradient norm: 6.828132018446922, minimum ratio: 0.73828125\n",
      "Epoch [63], val_loss: 2.2690\n",
      "gradient norm: 6.903432637453079, minimum ratio: 0.734375\n",
      "Epoch [64], val_loss: 2.3143\n",
      "gradient norm: 6.978922516107559, minimum ratio: 0.73828125\n",
      "Epoch [65], val_loss: 2.3601\n",
      "gradient norm: 7.054721638560295, minimum ratio: 0.7578125\n",
      "Epoch [66], val_loss: 2.4065\n",
      "gradient norm: 7.130680337548256, minimum ratio: 0.73828125\n",
      "Epoch [67], val_loss: 2.4534\n",
      "gradient norm: 7.206823140382767, minimum ratio: 0.74609375\n",
      "Epoch [68], val_loss: 2.5009\n",
      "gradient norm: 7.283098071813583, minimum ratio: 0.7421875\n",
      "Epoch [69], val_loss: 2.5488\n",
      "gradient norm: 7.359678953886032, minimum ratio: 0.72265625\n",
      "Epoch [70], val_loss: 2.5974\n",
      "gradient norm: 7.4364058673381805, minimum ratio: 0.7421875\n",
      "Epoch [71], val_loss: 2.6465\n",
      "gradient norm: 7.513354405760765, minimum ratio: 0.734375\n",
      "Epoch [72], val_loss: 2.6961\n",
      "gradient norm: 7.590603247284889, minimum ratio: 0.7265625\n",
      "Epoch [73], val_loss: 2.7463\n",
      "gradient norm: 7.668080046772957, minimum ratio: 0.7578125\n",
      "Epoch [74], val_loss: 2.7970\n",
      "gradient norm: 7.7456483989953995, minimum ratio: 0.73046875\n",
      "Epoch [75], val_loss: 2.8484\n",
      "gradient norm: 7.823465064167976, minimum ratio: 0.73828125\n",
      "Epoch [76], val_loss: 2.9002\n",
      "gradient norm: 7.901533126831055, minimum ratio: 0.7578125\n",
      "Epoch [77], val_loss: 2.9527\n",
      "gradient norm: 7.979807734489441, minimum ratio: 0.7578125\n",
      "Epoch [78], val_loss: 3.0057\n",
      "gradient norm: 8.058434888720512, minimum ratio: 0.74609375\n",
      "Epoch [79], val_loss: 3.0593\n",
      "gradient norm: 8.137294009327888, minimum ratio: 0.75390625\n",
      "Epoch [80], val_loss: 3.1134\n",
      "gradient norm: 8.21619063615799, minimum ratio: 0.73828125\n",
      "Epoch [81], val_loss: 3.1682\n",
      "gradient norm: 8.295537948608398, minimum ratio: 0.74609375\n",
      "Epoch [82], val_loss: 3.2235\n",
      "gradient norm: 8.374999761581421, minimum ratio: 0.75\n",
      "Epoch [83], val_loss: 3.2794\n",
      "gradient norm: 8.454612106084824, minimum ratio: 0.73828125\n",
      "Epoch [84], val_loss: 3.3359\n",
      "gradient norm: 8.534583747386932, minimum ratio: 0.76171875\n",
      "Epoch [85], val_loss: 3.3930\n",
      "gradient norm: 8.614812359213829, minimum ratio: 0.7734375\n",
      "Epoch [86], val_loss: 3.4507\n",
      "gradient norm: 8.695441499352455, minimum ratio: 0.7421875\n",
      "Epoch [87], val_loss: 3.5090\n",
      "gradient norm: 8.77610468864441, minimum ratio: 0.74609375\n",
      "Epoch [88], val_loss: 3.5679\n",
      "gradient norm: 8.857058107852936, minimum ratio: 0.734375\n",
      "Epoch [89], val_loss: 3.6274\n",
      "gradient norm: 8.938151314854622, minimum ratio: 0.76953125\n",
      "Epoch [90], val_loss: 3.6875\n",
      "gradient norm: 9.019691109657288, minimum ratio: 0.76171875\n",
      "Epoch [91], val_loss: 3.7483\n",
      "gradient norm: 9.10153517127037, minimum ratio: 0.73828125\n",
      "Epoch [92], val_loss: 3.8096\n",
      "gradient norm: 9.183568730950356, minimum ratio: 0.76171875\n",
      "Epoch [93], val_loss: 3.8716\n",
      "gradient norm: 9.26579201221466, minimum ratio: 0.7734375\n",
      "Epoch [94], val_loss: 3.9342\n",
      "gradient norm: 9.348306953907013, minimum ratio: 0.73828125\n",
      "Epoch [95], val_loss: 3.9974\n",
      "gradient norm: 9.431285560131073, minimum ratio: 0.7421875\n",
      "Epoch [96], val_loss: 4.0612\n",
      "gradient norm: 9.514306470751762, minimum ratio: 0.765625\n",
      "Epoch [97], val_loss: 4.1257\n",
      "gradient norm: 9.597608998417854, minimum ratio: 0.71875\n",
      "Epoch [98], val_loss: 4.1908\n",
      "gradient norm: 9.681269243359566, minimum ratio: 0.7421875\n",
      "Epoch [99], val_loss: 4.2566\n",
      "gradient norm: 9.76526328921318, minimum ratio: 0.75\n",
      "Epoch [100], val_loss: 4.3230\n",
      "gradient norm: 9.849454298615456, minimum ratio: 0.74609375\n",
      "Epoch [101], val_loss: 4.3901\n",
      "gradient norm: 9.933882862329483, minimum ratio: 0.74609375\n",
      "Epoch [102], val_loss: 4.4578\n",
      "gradient norm: 10.018625229597092, minimum ratio: 0.75\n",
      "Epoch [103], val_loss: 4.5262\n",
      "gradient norm: 10.103926047682762, minimum ratio: 0.75\n",
      "Epoch [104], val_loss: 4.5952\n",
      "gradient norm: 10.189312726259232, minimum ratio: 0.75\n",
      "Epoch [105], val_loss: 4.6649\n",
      "gradient norm: 10.274968177080154, minimum ratio: 0.74609375\n",
      "Epoch [106], val_loss: 4.7353\n",
      "gradient norm: 10.36083722114563, minimum ratio: 0.765625\n",
      "Epoch [107], val_loss: 4.8064\n",
      "gradient norm: 10.44721993803978, minimum ratio: 0.73828125\n",
      "Epoch [108], val_loss: 4.8781\n",
      "gradient norm: 10.533587276935577, minimum ratio: 0.73828125\n",
      "Epoch [109], val_loss: 4.9505\n",
      "gradient norm: 10.620439499616623, minimum ratio: 0.75390625\n",
      "Epoch [110], val_loss: 5.0236\n",
      "gradient norm: 10.707615673542023, minimum ratio: 0.75\n",
      "Epoch [111], val_loss: 5.0974\n",
      "gradient norm: 10.795046359300613, minimum ratio: 0.7578125\n",
      "Epoch [112], val_loss: 5.1719\n",
      "gradient norm: 10.882850974798203, minimum ratio: 0.7421875\n",
      "Epoch [113], val_loss: 5.2471\n",
      "gradient norm: 10.97094014286995, minimum ratio: 0.76953125\n",
      "Epoch [114], val_loss: 5.3230\n",
      "gradient norm: 11.059223294258118, minimum ratio: 0.7578125\n",
      "Epoch [115], val_loss: 5.3996\n",
      "gradient norm: 11.14782640337944, minimum ratio: 0.75\n",
      "Epoch [116], val_loss: 5.4769\n",
      "gradient norm: 11.237056255340576, minimum ratio: 0.76171875\n",
      "Epoch [117], val_loss: 5.5550\n",
      "gradient norm: 11.326317727565765, minimum ratio: 0.74609375\n",
      "Epoch [118], val_loss: 5.6337\n",
      "gradient norm: 11.41589805483818, minimum ratio: 0.73046875\n",
      "Epoch [119], val_loss: 5.7132\n",
      "gradient norm: 11.505623161792755, minimum ratio: 0.75390625\n",
      "Epoch [120], val_loss: 5.7934\n",
      "gradient norm: 11.59595301747322, minimum ratio: 0.76953125\n",
      "Epoch [121], val_loss: 5.8744\n",
      "gradient norm: 11.686492949724197, minimum ratio: 0.75390625\n",
      "Epoch [122], val_loss: 5.9561\n",
      "gradient norm: 11.777477383613586, minimum ratio: 0.7734375\n",
      "Epoch [123], val_loss: 6.0385\n",
      "gradient norm: 11.868691831827164, minimum ratio: 0.734375\n",
      "Epoch [124], val_loss: 6.1217\n",
      "gradient norm: 11.960382729768753, minimum ratio: 0.7578125\n",
      "Epoch [125], val_loss: 6.2057\n",
      "gradient norm: 12.052390336990356, minimum ratio: 0.7578125\n",
      "Epoch [126], val_loss: 6.2904\n",
      "gradient norm: 12.144598722457886, minimum ratio: 0.73828125\n",
      "Epoch [127], val_loss: 6.3759\n",
      "gradient norm: 12.237121313810349, minimum ratio: 0.734375\n",
      "Epoch [128], val_loss: 6.4621\n",
      "gradient norm: 12.330158650875092, minimum ratio: 0.76953125\n",
      "Epoch [129], val_loss: 6.5491\n",
      "gradient norm: 12.42351296544075, minimum ratio: 0.76953125\n",
      "Epoch [130], val_loss: 6.6369\n",
      "gradient norm: 12.51720416545868, minimum ratio: 0.75\n",
      "Epoch [131], val_loss: 6.7255\n",
      "gradient norm: 12.611348956823349, minimum ratio: 0.76171875\n",
      "Epoch [132], val_loss: 6.8149\n",
      "gradient norm: 12.705711126327515, minimum ratio: 0.7421875\n",
      "Epoch [133], val_loss: 6.9051\n",
      "gradient norm: 12.800436109304428, minimum ratio: 0.7578125\n",
      "Epoch [134], val_loss: 6.9961\n",
      "gradient norm: 12.895476818084717, minimum ratio: 0.75390625\n",
      "Epoch [135], val_loss: 7.0878\n",
      "gradient norm: 12.990723073482513, minimum ratio: 0.76171875\n",
      "Epoch [136], val_loss: 7.1804\n",
      "gradient norm: 13.086650788784027, minimum ratio: 0.7265625\n",
      "Epoch [137], val_loss: 7.2738\n",
      "gradient norm: 13.182730466127396, minimum ratio: 0.75\n",
      "Epoch [138], val_loss: 7.3681\n",
      "gradient norm: 13.279376536607742, minimum ratio: 0.7421875\n",
      "Epoch [139], val_loss: 7.4631\n",
      "gradient norm: 13.376194804906845, minimum ratio: 0.75390625\n",
      "Epoch [140], val_loss: 7.5590\n",
      "gradient norm: 13.473459720611572, minimum ratio: 0.75390625\n",
      "Epoch [141], val_loss: 7.6557\n",
      "gradient norm: 13.57116910815239, minimum ratio: 0.7578125\n",
      "Epoch [142], val_loss: 7.7533\n",
      "gradient norm: 13.669098436832428, minimum ratio: 0.75390625\n",
      "Epoch [143], val_loss: 7.8517\n",
      "gradient norm: 13.767444670200348, minimum ratio: 0.73828125\n",
      "Epoch [144], val_loss: 7.9510\n",
      "gradient norm: 13.866072177886963, minimum ratio: 0.74609375\n",
      "Epoch [145], val_loss: 8.0511\n",
      "gradient norm: 13.965332984924316, minimum ratio: 0.76171875\n",
      "Epoch [146], val_loss: 8.1521\n",
      "gradient norm: 14.064931064844131, minimum ratio: 0.7421875\n",
      "Epoch [147], val_loss: 8.2539\n",
      "gradient norm: 14.164690375328064, minimum ratio: 0.74609375\n",
      "Epoch [148], val_loss: 8.3567\n",
      "gradient norm: 14.265070587396622, minimum ratio: 0.7734375\n",
      "Epoch [149], val_loss: 8.4603\n",
      "gradient norm: 14.36589965224266, minimum ratio: 0.75\n",
      "Epoch [150], val_loss: 8.5648\n",
      "gradient norm: 14.467091470956802, minimum ratio: 0.7421875\n",
      "Epoch [151], val_loss: 8.6702\n",
      "gradient norm: 14.568401247262955, minimum ratio: 0.74609375\n",
      "Epoch [152], val_loss: 8.7765\n",
      "gradient norm: 14.670134007930756, minimum ratio: 0.75390625\n",
      "Epoch [153], val_loss: 8.8836\n",
      "gradient norm: 14.772428214550018, minimum ratio: 0.74609375\n",
      "Epoch [154], val_loss: 8.9917\n",
      "gradient norm: 14.874686151742935, minimum ratio: 0.765625\n",
      "Epoch [155], val_loss: 9.1007\n",
      "gradient norm: 14.977690786123276, minimum ratio: 0.74609375\n",
      "Epoch [156], val_loss: 9.2107\n",
      "gradient norm: 15.081046640872955, minimum ratio: 0.7421875\n",
      "Epoch [157], val_loss: 9.3215\n",
      "gradient norm: 15.184893876314163, minimum ratio: 0.78515625\n",
      "Epoch [158], val_loss: 9.4333\n",
      "gradient norm: 15.28910705447197, minimum ratio: 0.74609375\n",
      "Epoch [159], val_loss: 9.5461\n",
      "gradient norm: 15.393801867961884, minimum ratio: 0.7578125\n",
      "Epoch [160], val_loss: 9.6598\n",
      "gradient norm: 15.498691648244858, minimum ratio: 0.76171875\n",
      "Epoch [161], val_loss: 9.7744\n",
      "gradient norm: 15.604354530572891, minimum ratio: 0.734375\n",
      "Epoch [162], val_loss: 9.8900\n",
      "gradient norm: 15.710083037614822, minimum ratio: 0.76953125\n",
      "Epoch [163], val_loss: 10.0065\n",
      "gradient norm: 15.816643983125687, minimum ratio: 0.75390625\n",
      "Epoch [164], val_loss: 10.1241\n",
      "gradient norm: 15.923375844955444, minimum ratio: 0.7578125\n",
      "Epoch [165], val_loss: 10.2426\n",
      "gradient norm: 16.030416429042816, minimum ratio: 0.7578125\n",
      "Epoch [166], val_loss: 10.3620\n",
      "gradient norm: 16.137880980968475, minimum ratio: 0.765625\n",
      "Epoch [167], val_loss: 10.4825\n",
      "gradient norm: 16.245849311351776, minimum ratio: 0.75390625\n",
      "Epoch [168], val_loss: 10.6040\n",
      "gradient norm: 16.35425692796707, minimum ratio: 0.74609375\n",
      "Epoch [169], val_loss: 10.7264\n",
      "gradient norm: 16.46319943666458, minimum ratio: 0.765625\n",
      "Epoch [170], val_loss: 10.8499\n",
      "gradient norm: 16.57271718978882, minimum ratio: 0.765625\n",
      "Epoch [171], val_loss: 10.9744\n",
      "gradient norm: 16.682234048843384, minimum ratio: 0.74609375\n",
      "Epoch [172], val_loss: 11.0999\n",
      "gradient norm: 16.792250275611877, minimum ratio: 0.76171875\n",
      "Epoch [173], val_loss: 11.2264\n",
      "gradient norm: 16.902564227581024, minimum ratio: 0.76953125\n",
      "Epoch [174], val_loss: 11.3540\n",
      "gradient norm: 17.013811111450195, minimum ratio: 0.7734375\n",
      "Epoch [175], val_loss: 11.4825\n",
      "gradient norm: 17.125120878219604, minimum ratio: 0.76171875\n",
      "Epoch [176], val_loss: 11.6122\n",
      "gradient norm: 17.23724192380905, minimum ratio: 0.75390625\n",
      "Epoch [177], val_loss: 11.7429\n",
      "gradient norm: 17.349246203899384, minimum ratio: 0.73828125\n",
      "Epoch [178], val_loss: 11.8746\n",
      "gradient norm: 17.462013959884644, minimum ratio: 0.7421875\n",
      "Epoch [179], val_loss: 12.0074\n",
      "gradient norm: 17.574851632118225, minimum ratio: 0.75390625\n",
      "Epoch [180], val_loss: 12.1413\n",
      "gradient norm: 17.688296258449554, minimum ratio: 0.734375\n",
      "Epoch [181], val_loss: 12.2762\n",
      "gradient norm: 17.802481770515442, minimum ratio: 0.74609375\n",
      "Epoch [182], val_loss: 12.4122\n",
      "gradient norm: 17.917126715183258, minimum ratio: 0.76171875\n",
      "Epoch [183], val_loss: 12.5494\n",
      "gradient norm: 18.031993806362152, minimum ratio: 0.75\n",
      "Epoch [184], val_loss: 12.6876\n",
      "gradient norm: 18.14726048707962, minimum ratio: 0.75390625\n",
      "Epoch [185], val_loss: 12.8269\n",
      "gradient norm: 18.263128519058228, minimum ratio: 0.75390625\n",
      "Epoch [186], val_loss: 12.9673\n",
      "gradient norm: 18.37922638654709, minimum ratio: 0.7421875\n",
      "Epoch [187], val_loss: 13.1089\n",
      "gradient norm: 18.49575763940811, minimum ratio: 0.75\n",
      "Epoch [188], val_loss: 13.2516\n",
      "gradient norm: 18.61288857460022, minimum ratio: 0.75390625\n",
      "Epoch [189], val_loss: 13.3954\n",
      "gradient norm: 18.730428218841553, minimum ratio: 0.75390625\n",
      "Epoch [190], val_loss: 13.5403\n",
      "gradient norm: 18.84868174791336, minimum ratio: 0.7734375\n",
      "Epoch [191], val_loss: 13.6864\n",
      "gradient norm: 18.967199444770813, minimum ratio: 0.7421875\n",
      "Epoch [192], val_loss: 13.8337\n",
      "gradient norm: 19.086146235466003, minimum ratio: 0.74609375\n",
      "Epoch [193], val_loss: 13.9821\n",
      "gradient norm: 19.205619037151337, minimum ratio: 0.75390625\n",
      "Epoch [194], val_loss: 14.1316\n",
      "gradient norm: 19.32554066181183, minimum ratio: 0.7265625\n",
      "Epoch [195], val_loss: 14.2824\n",
      "gradient norm: 19.446500301361084, minimum ratio: 0.76171875\n",
      "Epoch [196], val_loss: 14.4343\n",
      "gradient norm: 19.56724661588669, minimum ratio: 0.75390625\n",
      "Epoch [197], val_loss: 14.5874\n",
      "gradient norm: 19.688432216644287, minimum ratio: 0.74609375\n",
      "Epoch [198], val_loss: 14.7417\n",
      "gradient norm: 19.81023073196411, minimum ratio: 0.76171875\n",
      "Epoch [199], val_loss: 14.8972\n",
      "gradient norm: 19.93262219429016, minimum ratio: 0.75\n",
      "Epoch [200], val_loss: 15.0539\n",
      "gradient norm: 20.055569410324097, minimum ratio: 0.77734375\n",
      "Epoch [201], val_loss: 15.2119\n",
      "gradient norm: 20.17883324623108, minimum ratio: 0.77734375\n",
      "Epoch [202], val_loss: 15.3710\n",
      "gradient norm: 20.30256086587906, minimum ratio: 0.7578125\n",
      "Epoch [203], val_loss: 15.5314\n",
      "gradient norm: 20.426553428173065, minimum ratio: 0.76171875\n",
      "Epoch [204], val_loss: 15.6930\n",
      "gradient norm: 20.55132907629013, minimum ratio: 0.7578125\n",
      "Epoch [205], val_loss: 15.8559\n",
      "gradient norm: 20.67649668455124, minimum ratio: 0.75390625\n",
      "Epoch [206], val_loss: 16.0200\n",
      "gradient norm: 20.802008032798767, minimum ratio: 0.765625\n",
      "Epoch [207], val_loss: 16.1853\n",
      "gradient norm: 20.92853605747223, minimum ratio: 0.7578125\n",
      "Epoch [208], val_loss: 16.3520\n",
      "gradient norm: 21.05578303337097, minimum ratio: 0.7578125\n",
      "Epoch [209], val_loss: 16.5199\n",
      "gradient norm: 21.18288290500641, minimum ratio: 0.77734375\n",
      "Epoch [210], val_loss: 16.6891\n",
      "gradient norm: 21.310377955436707, minimum ratio: 0.77734375\n",
      "Epoch [211], val_loss: 16.8596\n",
      "gradient norm: 21.43846571445465, minimum ratio: 0.7421875\n",
      "Epoch [212], val_loss: 17.0314\n",
      "gradient norm: 21.56712144613266, minimum ratio: 0.78125\n",
      "Epoch [213], val_loss: 17.2045\n",
      "gradient norm: 21.69621431827545, minimum ratio: 0.7578125\n",
      "Epoch [214], val_loss: 17.3789\n",
      "gradient norm: 21.82627558708191, minimum ratio: 0.7578125\n",
      "Epoch [215], val_loss: 17.5547\n",
      "gradient norm: 21.95632392168045, minimum ratio: 0.75\n",
      "Epoch [216], val_loss: 17.7317\n",
      "gradient norm: 22.08690047264099, minimum ratio: 0.765625\n",
      "Epoch [217], val_loss: 17.9101\n",
      "gradient norm: 22.21819680929184, minimum ratio: 0.76171875\n",
      "Epoch [218], val_loss: 18.0898\n",
      "gradient norm: 22.34980845451355, minimum ratio: 0.7578125\n",
      "Epoch [219], val_loss: 18.2709\n",
      "gradient norm: 22.481708884239197, minimum ratio: 0.7734375\n",
      "Epoch [220], val_loss: 18.4534\n",
      "gradient norm: 22.614685773849487, minimum ratio: 0.75\n",
      "Epoch [221], val_loss: 18.6372\n",
      "gradient norm: 22.748426735401154, minimum ratio: 0.77734375\n",
      "Epoch [222], val_loss: 18.8224\n",
      "gradient norm: 22.88202863931656, minimum ratio: 0.75\n",
      "Epoch [223], val_loss: 19.0090\n",
      "gradient norm: 23.016200721263885, minimum ratio: 0.79296875\n",
      "Epoch [224], val_loss: 19.1969\n",
      "gradient norm: 23.150977551937103, minimum ratio: 0.78515625\n",
      "Epoch [225], val_loss: 19.3863\n",
      "gradient norm: 23.286436915397644, minimum ratio: 0.76953125\n",
      "Epoch [226], val_loss: 19.5770\n",
      "gradient norm: 23.42256671190262, minimum ratio: 0.76953125\n",
      "Epoch [227], val_loss: 19.7692\n",
      "gradient norm: 23.559049665927887, minimum ratio: 0.76953125\n",
      "Epoch [228], val_loss: 19.9628\n",
      "gradient norm: 23.695898711681366, minimum ratio: 0.75390625\n",
      "Epoch [229], val_loss: 20.1578\n",
      "gradient norm: 23.8331977725029, minimum ratio: 0.7578125\n",
      "Epoch [230], val_loss: 20.3543\n",
      "gradient norm: 23.971819281578064, minimum ratio: 0.765625\n",
      "Epoch [231], val_loss: 20.5523\n",
      "gradient norm: 24.109668135643005, minimum ratio: 0.76171875\n",
      "Epoch [232], val_loss: 20.7516\n",
      "gradient norm: 24.248371243476868, minimum ratio: 0.74609375\n",
      "Epoch [233], val_loss: 20.9525\n",
      "gradient norm: 24.38815128803253, minimum ratio: 0.7734375\n",
      "Epoch [234], val_loss: 21.1548\n",
      "gradient norm: 24.52816992998123, minimum ratio: 0.76953125\n",
      "Epoch [235], val_loss: 21.3586\n",
      "gradient norm: 24.669107019901276, minimum ratio: 0.7421875\n",
      "Epoch [236], val_loss: 21.5639\n",
      "gradient norm: 24.81032156944275, minimum ratio: 0.76953125\n",
      "Epoch [237], val_loss: 21.7707\n",
      "gradient norm: 24.95177847146988, minimum ratio: 0.765625\n",
      "Epoch [238], val_loss: 21.9790\n",
      "gradient norm: 25.09411484003067, minimum ratio: 0.7734375\n",
      "Epoch [239], val_loss: 22.1889\n",
      "gradient norm: 25.237659454345703, minimum ratio: 0.75\n",
      "Epoch [240], val_loss: 22.4002\n",
      "gradient norm: 25.380829572677612, minimum ratio: 0.765625\n",
      "Epoch [241], val_loss: 22.6131\n",
      "gradient norm: 25.524913787841797, minimum ratio: 0.74609375\n",
      "Epoch [242], val_loss: 22.8275\n",
      "gradient norm: 25.66932725906372, minimum ratio: 0.73828125\n",
      "Epoch [243], val_loss: 23.0435\n",
      "gradient norm: 25.81397569179535, minimum ratio: 0.765625\n",
      "Epoch [244], val_loss: 23.2610\n",
      "gradient norm: 25.960116624832153, minimum ratio: 0.73828125\n",
      "Epoch [245], val_loss: 23.4801\n",
      "gradient norm: 26.10612964630127, minimum ratio: 0.75390625\n",
      "Epoch [246], val_loss: 23.7008\n",
      "gradient norm: 26.253249168395996, minimum ratio: 0.7734375\n",
      "Epoch [247], val_loss: 23.9230\n",
      "gradient norm: 26.400673747062683, minimum ratio: 0.7734375\n",
      "Epoch [248], val_loss: 24.1469\n",
      "gradient norm: 26.548426032066345, minimum ratio: 0.77734375\n",
      "Epoch [249], val_loss: 24.3723\n",
      "gradient norm: 26.69710397720337, minimum ratio: 0.7578125\n",
      "Epoch [250], val_loss: 24.5994\n",
      "gradient norm: 26.84611690044403, minimum ratio: 0.73828125\n",
      "Epoch [251], val_loss: 24.8280\n",
      "gradient norm: 26.9956556558609, minimum ratio: 0.75390625\n",
      "Epoch [252], val_loss: 25.0583\n",
      "gradient norm: 27.14577305316925, minimum ratio: 0.78125\n",
      "Epoch [253], val_loss: 25.2902\n",
      "gradient norm: 27.296788573265076, minimum ratio: 0.7578125\n",
      "Epoch [254], val_loss: 25.5237\n",
      "gradient norm: 27.448116779327393, minimum ratio: 0.734375\n",
      "Epoch [255], val_loss: 25.7589\n",
      "gradient norm: 27.599910974502563, minimum ratio: 0.7890625\n",
      "Epoch [256], val_loss: 25.9958\n",
      "gradient norm: 27.752044558525085, minimum ratio: 0.75\n",
      "Epoch [257], val_loss: 26.2343\n",
      "gradient norm: 27.90491282939911, minimum ratio: 0.75\n",
      "Epoch [258], val_loss: 26.4745\n",
      "gradient norm: 28.058713793754578, minimum ratio: 0.76953125\n",
      "Epoch [259], val_loss: 26.7164\n",
      "gradient norm: 28.21313440799713, minimum ratio: 0.76171875\n",
      "Epoch [260], val_loss: 26.9600\n",
      "gradient norm: 28.367441296577454, minimum ratio: 0.765625\n",
      "Epoch [261], val_loss: 27.2053\n",
      "gradient norm: 28.523176908493042, minimum ratio: 0.74609375\n",
      "Epoch [262], val_loss: 27.4523\n",
      "gradient norm: 28.679278135299683, minimum ratio: 0.75\n",
      "Epoch [263], val_loss: 27.7011\n",
      "gradient norm: 28.83593261241913, minimum ratio: 0.75390625\n",
      "Epoch [264], val_loss: 27.9515\n",
      "gradient norm: 28.992703795433044, minimum ratio: 0.76171875\n",
      "Epoch [265], val_loss: 28.2037\n",
      "gradient norm: 29.15074932575226, minimum ratio: 0.78125\n",
      "Epoch [266], val_loss: 28.4577\n",
      "gradient norm: 29.309008836746216, minimum ratio: 0.73828125\n",
      "Epoch [267], val_loss: 28.7134\n",
      "gradient norm: 29.467867732048035, minimum ratio: 0.77734375\n",
      "Epoch [268], val_loss: 28.9709\n",
      "gradient norm: 29.627294778823853, minimum ratio: 0.765625\n",
      "Epoch [269], val_loss: 29.2302\n",
      "gradient norm: 29.78752624988556, minimum ratio: 0.75390625\n",
      "Epoch [270], val_loss: 29.4912\n",
      "gradient norm: 29.947952151298523, minimum ratio: 0.7734375\n",
      "Epoch [271], val_loss: 29.7541\n",
      "gradient norm: 30.109870076179504, minimum ratio: 0.7578125\n",
      "Epoch [272], val_loss: 30.0187\n",
      "gradient norm: 30.27177906036377, minimum ratio: 0.78515625\n",
      "Epoch [273], val_loss: 30.2852\n",
      "gradient norm: 30.433817148208618, minimum ratio: 0.7578125\n",
      "Epoch [274], val_loss: 30.5535\n",
      "gradient norm: 30.59691083431244, minimum ratio: 0.765625\n",
      "Epoch [275], val_loss: 30.8237\n",
      "gradient norm: 30.760738492012024, minimum ratio: 0.7578125\n",
      "Epoch [276], val_loss: 31.0957\n",
      "gradient norm: 30.925425171852112, minimum ratio: 0.7734375\n",
      "Epoch [277], val_loss: 31.3695\n",
      "gradient norm: 31.08996045589447, minimum ratio: 0.7734375\n",
      "Epoch [278], val_loss: 31.6453\n",
      "gradient norm: 31.255992531776428, minimum ratio: 0.74609375\n",
      "Epoch [279], val_loss: 31.9228\n",
      "gradient norm: 31.421544313430786, minimum ratio: 0.76953125\n",
      "Epoch [280], val_loss: 32.2023\n",
      "gradient norm: 31.58813488483429, minimum ratio: 0.7890625\n",
      "Epoch [281], val_loss: 32.4837\n",
      "gradient norm: 31.75609064102173, minimum ratio: 0.75390625\n",
      "Epoch [282], val_loss: 32.7669\n",
      "gradient norm: 31.92460608482361, minimum ratio: 0.74609375\n",
      "Epoch [283], val_loss: 33.0521\n",
      "gradient norm: 32.09213054180145, minimum ratio: 0.7734375\n",
      "Epoch [284], val_loss: 33.3392\n",
      "gradient norm: 32.26147019863129, minimum ratio: 0.7734375\n",
      "Epoch [285], val_loss: 33.6282\n",
      "gradient norm: 32.43180799484253, minimum ratio: 0.7421875\n",
      "Epoch [286], val_loss: 33.9192\n",
      "gradient norm: 32.6025972366333, minimum ratio: 0.7890625\n",
      "Epoch [287], val_loss: 34.2121\n",
      "gradient norm: 32.77360212802887, minimum ratio: 0.765625\n",
      "Epoch [288], val_loss: 34.5069\n",
      "gradient norm: 32.94555854797363, minimum ratio: 0.7734375\n",
      "Epoch [289], val_loss: 34.8038\n",
      "gradient norm: 33.117862582206726, minimum ratio: 0.7734375\n",
      "Epoch [290], val_loss: 35.1026\n",
      "gradient norm: 33.29011034965515, minimum ratio: 0.7578125\n",
      "Epoch [291], val_loss: 35.4034\n",
      "gradient norm: 33.46430146694183, minimum ratio: 0.74609375\n",
      "Epoch [292], val_loss: 35.7062\n",
      "gradient norm: 33.63921678066254, minimum ratio: 0.7578125\n",
      "Epoch [293], val_loss: 36.0110\n",
      "gradient norm: 33.8139865398407, minimum ratio: 0.76953125\n",
      "Epoch [294], val_loss: 36.3178\n",
      "gradient norm: 33.98874628543854, minimum ratio: 0.74609375\n",
      "Epoch [295], val_loss: 36.6266\n",
      "gradient norm: 34.16499733924866, minimum ratio: 0.77734375\n",
      "Epoch [296], val_loss: 36.9375\n",
      "gradient norm: 34.342432141304016, minimum ratio: 0.76171875\n",
      "Epoch [297], val_loss: 37.2505\n",
      "gradient norm: 34.51941645145416, minimum ratio: 0.765625\n",
      "Epoch [298], val_loss: 37.5655\n",
      "gradient norm: 34.69704282283783, minimum ratio: 0.765625\n",
      "Epoch [299], val_loss: 37.8826\n",
      "gradient norm: 34.87618160247803, minimum ratio: 0.765625\n",
      "Epoch [300], val_loss: 38.2018\n",
      "gradient norm: 35.05581259727478, minimum ratio: 0.76171875\n",
      "Epoch [301], val_loss: 38.5230\n",
      "gradient norm: 35.23571419715881, minimum ratio: 0.77734375\n",
      "Epoch [302], val_loss: 38.8464\n",
      "gradient norm: 35.41614353656769, minimum ratio: 0.75390625\n",
      "Epoch [303], val_loss: 39.1719\n",
      "gradient norm: 35.59756052494049, minimum ratio: 0.7578125\n",
      "Epoch [304], val_loss: 39.4996\n",
      "gradient norm: 35.780152559280396, minimum ratio: 0.7421875\n",
      "Epoch [305], val_loss: 39.8293\n",
      "gradient norm: 35.96264970302582, minimum ratio: 0.76953125\n",
      "Epoch [306], val_loss: 40.1613\n",
      "gradient norm: 36.14539361000061, minimum ratio: 0.77734375\n",
      "Epoch [307], val_loss: 40.4953\n",
      "gradient norm: 36.32957577705383, minimum ratio: 0.74609375\n",
      "Epoch [308], val_loss: 40.8316\n",
      "gradient norm: 36.514888644218445, minimum ratio: 0.76171875\n",
      "Epoch [309], val_loss: 41.1699\n",
      "gradient norm: 36.699989438056946, minimum ratio: 0.74609375\n",
      "Epoch [310], val_loss: 41.5105\n",
      "gradient norm: 36.88666033744812, minimum ratio: 0.76953125\n",
      "Epoch [311], val_loss: 41.8533\n",
      "gradient norm: 37.073192834854126, minimum ratio: 0.76953125\n",
      "Epoch [312], val_loss: 42.1982\n",
      "gradient norm: 37.260557413101196, minimum ratio: 0.78125\n",
      "Epoch [313], val_loss: 42.5454\n",
      "gradient norm: 37.448660135269165, minimum ratio: 0.76953125\n",
      "Epoch [314], val_loss: 42.8947\n",
      "gradient norm: 37.63686001300812, minimum ratio: 0.75390625\n",
      "Epoch [315], val_loss: 43.2463\n",
      "gradient norm: 37.826616168022156, minimum ratio: 0.75390625\n",
      "Epoch [316], val_loss: 43.6002\n",
      "gradient norm: 38.016056537628174, minimum ratio: 0.76953125\n",
      "Epoch [317], val_loss: 43.9563\n",
      "gradient norm: 38.20631456375122, minimum ratio: 0.7734375\n",
      "Epoch [318], val_loss: 44.3147\n",
      "gradient norm: 38.398481011390686, minimum ratio: 0.76171875\n",
      "Epoch [319], val_loss: 44.6753\n",
      "gradient norm: 38.59002995491028, minimum ratio: 0.75390625\n",
      "Epoch [320], val_loss: 45.0383\n",
      "gradient norm: 38.782787442207336, minimum ratio: 0.765625\n",
      "Epoch [321], val_loss: 45.4036\n",
      "gradient norm: 38.9758585691452, minimum ratio: 0.75\n",
      "Epoch [322], val_loss: 45.7711\n",
      "gradient norm: 39.17031753063202, minimum ratio: 0.76953125\n",
      "Epoch [323], val_loss: 46.1410\n",
      "gradient norm: 39.365124106407166, minimum ratio: 0.76953125\n",
      "Epoch [324], val_loss: 46.5133\n",
      "gradient norm: 39.56108832359314, minimum ratio: 0.76171875\n",
      "Epoch [325], val_loss: 46.8878\n",
      "gradient norm: 39.756930351257324, minimum ratio: 0.73828125\n",
      "Epoch [326], val_loss: 47.2648\n",
      "gradient norm: 39.95394563674927, minimum ratio: 0.75\n",
      "Epoch [327], val_loss: 47.6441\n",
      "gradient norm: 40.15107345581055, minimum ratio: 0.76171875\n",
      "Epoch [328], val_loss: 48.0257\n",
      "gradient norm: 40.34904086589813, minimum ratio: 0.76953125\n",
      "Epoch [329], val_loss: 48.4097\n",
      "gradient norm: 40.54710292816162, minimum ratio: 0.78125\n",
      "Epoch [330], val_loss: 48.7962\n",
      "gradient norm: 40.746164083480835, minimum ratio: 0.7578125\n",
      "Epoch [331], val_loss: 49.1851\n",
      "gradient norm: 40.94727087020874, minimum ratio: 0.7578125\n",
      "Epoch [332], val_loss: 49.5764\n",
      "gradient norm: 41.148630142211914, minimum ratio: 0.7578125\n",
      "Epoch [333], val_loss: 49.9701\n",
      "gradient norm: 41.34969139099121, minimum ratio: 0.74609375\n",
      "Epoch [334], val_loss: 50.3662\n",
      "gradient norm: 41.552260398864746, minimum ratio: 0.7578125\n",
      "Epoch [335], val_loss: 50.7648\n",
      "gradient norm: 41.75459623336792, minimum ratio: 0.7578125\n",
      "Epoch [336], val_loss: 51.1659\n",
      "gradient norm: 41.95848250389099, minimum ratio: 0.7890625\n",
      "Epoch [337], val_loss: 51.5694\n",
      "gradient norm: 42.16236710548401, minimum ratio: 0.765625\n",
      "Epoch [338], val_loss: 51.9754\n",
      "gradient norm: 42.36871886253357, minimum ratio: 0.76171875\n",
      "Epoch [339], val_loss: 52.3839\n",
      "gradient norm: 42.57449412345886, minimum ratio: 0.74609375\n",
      "Epoch [340], val_loss: 52.7950\n",
      "gradient norm: 42.78083682060242, minimum ratio: 0.76171875\n",
      "Epoch [341], val_loss: 53.2085\n",
      "gradient norm: 42.98734474182129, minimum ratio: 0.75390625\n",
      "Epoch [342], val_loss: 53.6246\n",
      "gradient norm: 43.19529414176941, minimum ratio: 0.75\n",
      "Epoch [343], val_loss: 54.0433\n",
      "gradient norm: 43.402700901031494, minimum ratio: 0.7578125\n",
      "Epoch [344], val_loss: 54.4645\n",
      "gradient norm: 43.61233448982239, minimum ratio: 0.765625\n",
      "Epoch [345], val_loss: 54.8883\n",
      "gradient norm: 43.823866844177246, minimum ratio: 0.76171875\n",
      "Epoch [346], val_loss: 55.3148\n",
      "gradient norm: 44.03416895866394, minimum ratio: 0.75\n",
      "Epoch [347], val_loss: 55.7438\n",
      "gradient norm: 44.24622654914856, minimum ratio: 0.765625\n",
      "Epoch [348], val_loss: 56.1754\n",
      "gradient norm: 44.45871305465698, minimum ratio: 0.765625\n",
      "Epoch [349], val_loss: 56.6096\n",
      "gradient norm: 44.671919107437134, minimum ratio: 0.77734375\n",
      "Epoch [350], val_loss: 57.0465\n",
      "gradient norm: 44.884228467941284, minimum ratio: 0.765625\n",
      "Epoch [351], val_loss: 57.4859\n",
      "gradient norm: 45.09905791282654, minimum ratio: 0.76953125\n",
      "Epoch [352], val_loss: 57.9281\n",
      "gradient norm: 45.31497931480408, minimum ratio: 0.7578125\n",
      "Epoch [353], val_loss: 58.3729\n",
      "gradient norm: 45.53135561943054, minimum ratio: 0.7578125\n",
      "Epoch [354], val_loss: 58.8203\n",
      "gradient norm: 45.7482635974884, minimum ratio: 0.75390625\n",
      "Epoch [355], val_loss: 59.2704\n",
      "gradient norm: 45.96435880661011, minimum ratio: 0.76953125\n",
      "Epoch [356], val_loss: 59.7232\n",
      "gradient norm: 46.18254518508911, minimum ratio: 0.76171875\n",
      "Epoch [357], val_loss: 60.1787\n",
      "gradient norm: 46.40163040161133, minimum ratio: 0.75\n",
      "Epoch [358], val_loss: 60.6369\n",
      "gradient norm: 46.62065768241882, minimum ratio: 0.7734375\n",
      "Epoch [359], val_loss: 61.0979\n",
      "gradient norm: 46.84077501296997, minimum ratio: 0.765625\n",
      "Epoch [360], val_loss: 61.5616\n",
      "gradient norm: 47.062350273132324, minimum ratio: 0.76171875\n",
      "Epoch [361], val_loss: 62.0281\n",
      "gradient norm: 47.284034967422485, minimum ratio: 0.78125\n",
      "Epoch [362], val_loss: 62.4973\n",
      "gradient norm: 47.50651168823242, minimum ratio: 0.76171875\n",
      "Epoch [363], val_loss: 62.9694\n",
      "gradient norm: 47.73041081428528, minimum ratio: 0.75\n",
      "Epoch [364], val_loss: 63.4443\n",
      "gradient norm: 47.95358324050903, minimum ratio: 0.75\n",
      "Epoch [365], val_loss: 63.9219\n",
      "gradient norm: 48.178462266922, minimum ratio: 0.765625\n",
      "Epoch [366], val_loss: 64.4024\n",
      "gradient norm: 48.404425859451294, minimum ratio: 0.75\n",
      "Epoch [367], val_loss: 64.8857\n",
      "gradient norm: 48.63096022605896, minimum ratio: 0.765625\n",
      "Epoch [368], val_loss: 65.3718\n",
      "gradient norm: 48.857720375061035, minimum ratio: 0.75390625\n",
      "Epoch [369], val_loss: 65.8609\n",
      "gradient norm: 49.08577370643616, minimum ratio: 0.78515625\n",
      "Epoch [370], val_loss: 66.3528\n",
      "gradient norm: 49.31451058387756, minimum ratio: 0.7578125\n",
      "Epoch [371], val_loss: 66.8475\n",
      "gradient norm: 49.543490171432495, minimum ratio: 0.75\n",
      "Epoch [372], val_loss: 67.3451\n",
      "gradient norm: 49.77356576919556, minimum ratio: 0.76953125\n",
      "Epoch [373], val_loss: 67.8456\n",
      "gradient norm: 50.00429439544678, minimum ratio: 0.76171875\n",
      "Epoch [374], val_loss: 68.3490\n",
      "gradient norm: 50.236199140548706, minimum ratio: 0.75\n",
      "Epoch [375], val_loss: 68.8553\n",
      "gradient norm: 50.469078063964844, minimum ratio: 0.78125\n",
      "Epoch [376], val_loss: 69.3646\n",
      "gradient norm: 50.701332330703735, minimum ratio: 0.75390625\n",
      "Epoch [377], val_loss: 69.8768\n",
      "gradient norm: 50.93556070327759, minimum ratio: 0.76171875\n",
      "Epoch [378], val_loss: 70.3921\n",
      "gradient norm: 51.168251276016235, minimum ratio: 0.76953125\n",
      "Epoch [379], val_loss: 70.9102\n",
      "gradient norm: 51.40399479866028, minimum ratio: 0.796875\n",
      "Epoch [380], val_loss: 71.4314\n",
      "gradient norm: 51.640867710113525, minimum ratio: 0.78515625\n",
      "Epoch [381], val_loss: 71.9555\n",
      "gradient norm: 51.8778121471405, minimum ratio: 0.78515625\n",
      "Epoch [382], val_loss: 72.4827\n",
      "gradient norm: 52.11614727973938, minimum ratio: 0.765625\n",
      "Epoch [383], val_loss: 73.0128\n",
      "gradient norm: 52.353084564208984, minimum ratio: 0.76953125\n",
      "Epoch [384], val_loss: 73.5461\n",
      "gradient norm: 52.59196639060974, minimum ratio: 0.75390625\n",
      "Epoch [385], val_loss: 74.0823\n",
      "gradient norm: 52.832587003707886, minimum ratio: 0.7578125\n",
      "Epoch [386], val_loss: 74.6217\n",
      "gradient norm: 53.07323145866394, minimum ratio: 0.78515625\n",
      "Epoch [387], val_loss: 75.1640\n",
      "gradient norm: 53.31424880027771, minimum ratio: 0.7734375\n",
      "Epoch [388], val_loss: 75.7096\n",
      "gradient norm: 53.55626392364502, minimum ratio: 0.75\n",
      "Epoch [389], val_loss: 76.2582\n",
      "gradient norm: 53.799694538116455, minimum ratio: 0.7734375\n",
      "Epoch [390], val_loss: 76.8100\n",
      "gradient norm: 54.04426550865173, minimum ratio: 0.76953125\n",
      "Epoch [391], val_loss: 77.3649\n",
      "gradient norm: 54.2896933555603, minimum ratio: 0.76953125\n",
      "Epoch [392], val_loss: 77.9229\n",
      "gradient norm: 54.534576177597046, minimum ratio: 0.7734375\n",
      "Epoch [393], val_loss: 78.4840\n",
      "gradient norm: 54.77922320365906, minimum ratio: 0.7578125\n",
      "Epoch [394], val_loss: 79.0483\n",
      "gradient norm: 55.026556730270386, minimum ratio: 0.75390625\n",
      "Epoch [395], val_loss: 79.6158\n",
      "gradient norm: 55.27536487579346, minimum ratio: 0.77734375\n",
      "Epoch [396], val_loss: 80.1865\n",
      "gradient norm: 55.523857831954956, minimum ratio: 0.76953125\n",
      "Epoch [397], val_loss: 80.7604\n",
      "gradient norm: 55.77439641952515, minimum ratio: 0.7421875\n",
      "Epoch [398], val_loss: 81.3375\n",
      "gradient norm: 56.022892236709595, minimum ratio: 0.76171875\n",
      "Epoch [399], val_loss: 81.9177\n",
      "gradient norm: 56.27314352989197, minimum ratio: 0.765625\n",
      "Epoch [400], val_loss: 82.5012\n",
      "gradient norm: 56.52516770362854, minimum ratio: 0.765625\n",
      "Epoch [401], val_loss: 83.0880\n",
      "gradient norm: 56.77766180038452, minimum ratio: 0.75390625\n",
      "Epoch [402], val_loss: 83.6780\n",
      "gradient norm: 57.03120756149292, minimum ratio: 0.7734375\n",
      "Epoch [403], val_loss: 84.2714\n",
      "gradient norm: 57.28631114959717, minimum ratio: 0.78125\n",
      "Epoch [404], val_loss: 84.8681\n",
      "gradient norm: 57.53877234458923, minimum ratio: 0.7578125\n",
      "Epoch [405], val_loss: 85.4680\n",
      "gradient norm: 57.795634031295776, minimum ratio: 0.74609375\n",
      "Epoch [406], val_loss: 86.0714\n",
      "gradient norm: 58.05189871788025, minimum ratio: 0.78125\n",
      "Epoch [407], val_loss: 86.6779\n",
      "gradient norm: 58.30952787399292, minimum ratio: 0.76171875\n",
      "Epoch [408], val_loss: 87.2879\n",
      "gradient norm: 58.567272424697876, minimum ratio: 0.75390625\n",
      "Epoch [409], val_loss: 87.9011\n",
      "gradient norm: 58.82602667808533, minimum ratio: 0.76953125\n",
      "Epoch [410], val_loss: 88.5178\n",
      "gradient norm: 59.085577964782715, minimum ratio: 0.78125\n",
      "Epoch [411], val_loss: 89.1378\n",
      "gradient norm: 59.346057415008545, minimum ratio: 0.76953125\n",
      "Epoch [412], val_loss: 89.7613\n",
      "gradient norm: 59.60784029960632, minimum ratio: 0.765625\n",
      "Epoch [413], val_loss: 90.3882\n",
      "gradient norm: 59.87087607383728, minimum ratio: 0.78125\n",
      "Epoch [414], val_loss: 91.0184\n",
      "gradient norm: 60.13318872451782, minimum ratio: 0.77734375\n",
      "Epoch [415], val_loss: 91.6521\n",
      "gradient norm: 60.3980667591095, minimum ratio: 0.7734375\n",
      "Epoch [416], val_loss: 92.2893\n",
      "gradient norm: 60.66166019439697, minimum ratio: 0.77734375\n",
      "Epoch [417], val_loss: 92.9300\n",
      "gradient norm: 60.92742300033569, minimum ratio: 0.7890625\n",
      "Epoch [418], val_loss: 93.5742\n",
      "gradient norm: 61.19349217414856, minimum ratio: 0.75390625\n",
      "Epoch [419], val_loss: 94.2217\n",
      "gradient norm: 61.459431171417236, minimum ratio: 0.765625\n",
      "Epoch [420], val_loss: 94.8728\n",
      "gradient norm: 61.72613310813904, minimum ratio: 0.76953125\n",
      "Epoch [421], val_loss: 95.5275\n",
      "gradient norm: 61.996354818344116, minimum ratio: 0.7421875\n",
      "Epoch [422], val_loss: 96.1857\n",
      "gradient norm: 62.26566791534424, minimum ratio: 0.76953125\n",
      "Epoch [423], val_loss: 96.8473\n",
      "gradient norm: 62.53561472892761, minimum ratio: 0.7265625\n",
      "Epoch [424], val_loss: 97.5125\n",
      "gradient norm: 62.80679893493652, minimum ratio: 0.76171875\n",
      "Epoch [425], val_loss: 98.1813\n",
      "gradient norm: 63.07743573188782, minimum ratio: 0.77734375\n",
      "Epoch [426], val_loss: 98.8536\n",
      "gradient norm: 63.34892821311951, minimum ratio: 0.765625\n",
      "Epoch [427], val_loss: 99.5296\n",
      "gradient norm: 63.62363529205322, minimum ratio: 0.76953125\n",
      "Epoch [428], val_loss: 100.2093\n",
      "gradient norm: 63.897340297698975, minimum ratio: 0.78125\n",
      "Epoch [429], val_loss: 100.8926\n",
      "gradient norm: 64.17355298995972, minimum ratio: 0.78515625\n",
      "Epoch [430], val_loss: 101.5795\n",
      "gradient norm: 64.44921827316284, minimum ratio: 0.76953125\n",
      "Epoch [431], val_loss: 102.2701\n",
      "gradient norm: 64.72468304634094, minimum ratio: 0.74609375\n",
      "Epoch [432], val_loss: 102.9642\n",
      "gradient norm: 65.00341153144836, minimum ratio: 0.75390625\n",
      "Epoch [433], val_loss: 103.6621\n",
      "gradient norm: 65.28257632255554, minimum ratio: 0.75390625\n",
      "Epoch [434], val_loss: 104.3637\n",
      "gradient norm: 65.5620174407959, minimum ratio: 0.75\n",
      "Epoch [435], val_loss: 105.0691\n",
      "gradient norm: 65.84201669692993, minimum ratio: 0.76171875\n",
      "Epoch [436], val_loss: 105.7782\n",
      "gradient norm: 66.12373542785645, minimum ratio: 0.76953125\n",
      "Epoch [437], val_loss: 106.4911\n",
      "gradient norm: 66.40465641021729, minimum ratio: 0.7734375\n",
      "Epoch [438], val_loss: 107.2077\n",
      "gradient norm: 66.68761920928955, minimum ratio: 0.74609375\n",
      "Epoch [439], val_loss: 107.9280\n",
      "gradient norm: 66.97050857543945, minimum ratio: 0.76171875\n",
      "Epoch [440], val_loss: 108.6522\n",
      "gradient norm: 67.25554704666138, minimum ratio: 0.75390625\n",
      "Epoch [441], val_loss: 109.3803\n",
      "gradient norm: 67.54095983505249, minimum ratio: 0.7578125\n",
      "Epoch [442], val_loss: 110.1121\n",
      "gradient norm: 67.82740592956543, minimum ratio: 0.7734375\n",
      "Epoch [443], val_loss: 110.8478\n",
      "gradient norm: 68.11342859268188, minimum ratio: 0.75\n",
      "Epoch [444], val_loss: 111.5871\n",
      "gradient norm: 68.40114545822144, minimum ratio: 0.76171875\n",
      "Epoch [445], val_loss: 112.3303\n",
      "gradient norm: 68.69062089920044, minimum ratio: 0.734375\n",
      "Epoch [446], val_loss: 113.0773\n",
      "gradient norm: 68.98026752471924, minimum ratio: 0.74609375\n",
      "Epoch [447], val_loss: 113.8284\n",
      "gradient norm: 69.27091407775879, minimum ratio: 0.7890625\n",
      "Epoch [448], val_loss: 114.5833\n",
      "gradient norm: 69.56216478347778, minimum ratio: 0.7421875\n",
      "Epoch [449], val_loss: 115.3421\n",
      "gradient norm: 69.85294914245605, minimum ratio: 0.765625\n",
      "Epoch [450], val_loss: 116.1049\n",
      "gradient norm: 70.14482879638672, minimum ratio: 0.765625\n",
      "Epoch [451], val_loss: 116.8715\n",
      "gradient norm: 70.43859100341797, minimum ratio: 0.76953125\n",
      "Epoch [452], val_loss: 117.6422\n",
      "gradient norm: 70.73418474197388, minimum ratio: 0.76953125\n",
      "Epoch [453], val_loss: 118.4168\n",
      "gradient norm: 71.03041410446167, minimum ratio: 0.7578125\n",
      "Epoch [454], val_loss: 119.1954\n",
      "gradient norm: 71.32839965820312, minimum ratio: 0.7578125\n",
      "Epoch [455], val_loss: 119.9780\n",
      "gradient norm: 71.62413311004639, minimum ratio: 0.74609375\n",
      "Epoch [456], val_loss: 120.7647\n",
      "gradient norm: 71.92159843444824, minimum ratio: 0.77734375\n",
      "Epoch [457], val_loss: 121.5554\n",
      "gradient norm: 72.22073554992676, minimum ratio: 0.75\n",
      "Epoch [458], val_loss: 122.3501\n",
      "gradient norm: 72.5205979347229, minimum ratio: 0.73828125\n",
      "Epoch [459], val_loss: 123.1489\n",
      "gradient norm: 72.82008457183838, minimum ratio: 0.76953125\n",
      "Epoch [460], val_loss: 123.9518\n",
      "gradient norm: 73.12277603149414, minimum ratio: 0.75\n",
      "Epoch [461], val_loss: 124.7587\n",
      "gradient norm: 73.42544317245483, minimum ratio: 0.76953125\n",
      "Epoch [462], val_loss: 125.5697\n",
      "gradient norm: 73.72897291183472, minimum ratio: 0.7734375\n",
      "Epoch [463], val_loss: 126.3849\n",
      "gradient norm: 74.03433179855347, minimum ratio: 0.75\n",
      "Epoch [464], val_loss: 127.2041\n",
      "gradient norm: 74.33957242965698, minimum ratio: 0.76953125\n",
      "Epoch [465], val_loss: 128.0275\n",
      "gradient norm: 74.6459436416626, minimum ratio: 0.7578125\n",
      "Epoch [466], val_loss: 128.8552\n",
      "gradient norm: 74.95230007171631, minimum ratio: 0.765625\n",
      "Epoch [467], val_loss: 129.6871\n",
      "gradient norm: 75.25855493545532, minimum ratio: 0.7734375\n",
      "Epoch [468], val_loss: 130.5232\n",
      "gradient norm: 75.56823062896729, minimum ratio: 0.77734375\n",
      "Epoch [469], val_loss: 131.3634\n",
      "gradient norm: 75.87981700897217, minimum ratio: 0.75\n",
      "Epoch [470], val_loss: 132.2079\n",
      "gradient norm: 76.19032621383667, minimum ratio: 0.75390625\n",
      "Epoch [471], val_loss: 133.0566\n",
      "gradient norm: 76.50173950195312, minimum ratio: 0.734375\n",
      "Epoch [472], val_loss: 133.9094\n",
      "gradient norm: 76.81346654891968, minimum ratio: 0.76953125\n",
      "Epoch [473], val_loss: 134.7665\n",
      "gradient norm: 77.12632036209106, minimum ratio: 0.79296875\n",
      "Epoch [474], val_loss: 135.6279\n",
      "gradient norm: 77.43747854232788, minimum ratio: 0.76171875\n",
      "Epoch [475], val_loss: 136.4936\n",
      "gradient norm: 77.75199222564697, minimum ratio: 0.76953125\n",
      "Epoch [476], val_loss: 137.3635\n",
      "gradient norm: 78.06898403167725, minimum ratio: 0.76171875\n",
      "Epoch [477], val_loss: 138.2379\n",
      "gradient norm: 78.38557147979736, minimum ratio: 0.765625\n",
      "Epoch [478], val_loss: 139.1167\n",
      "gradient norm: 78.70461416244507, minimum ratio: 0.7578125\n",
      "Epoch [479], val_loss: 139.9999\n",
      "gradient norm: 79.0244951248169, minimum ratio: 0.79296875\n",
      "Epoch [480], val_loss: 140.8874\n",
      "gradient norm: 79.34211301803589, minimum ratio: 0.78125\n",
      "Epoch [481], val_loss: 141.7793\n",
      "gradient norm: 79.66175842285156, minimum ratio: 0.7734375\n",
      "Epoch [482], val_loss: 142.6756\n",
      "gradient norm: 79.98289346694946, minimum ratio: 0.7578125\n",
      "Epoch [483], val_loss: 143.5763\n",
      "gradient norm: 80.30612564086914, minimum ratio: 0.76171875\n",
      "Epoch [484], val_loss: 144.4815\n",
      "gradient norm: 80.6290397644043, minimum ratio: 0.76171875\n",
      "Epoch [485], val_loss: 145.3911\n",
      "gradient norm: 80.95302295684814, minimum ratio: 0.7578125\n",
      "Epoch [486], val_loss: 146.3051\n",
      "gradient norm: 81.2783203125, minimum ratio: 0.78125\n",
      "Epoch [487], val_loss: 147.2237\n",
      "gradient norm: 81.60169219970703, minimum ratio: 0.76171875\n",
      "Epoch [488], val_loss: 148.1467\n",
      "gradient norm: 81.92883253097534, minimum ratio: 0.765625\n",
      "Epoch [489], val_loss: 149.0743\n",
      "gradient norm: 82.25745916366577, minimum ratio: 0.7421875\n",
      "Epoch [490], val_loss: 150.0063\n",
      "gradient norm: 82.58568859100342, minimum ratio: 0.77734375\n",
      "Epoch [491], val_loss: 150.9429\n",
      "gradient norm: 82.91600799560547, minimum ratio: 0.74609375\n",
      "Epoch [492], val_loss: 151.8839\n",
      "gradient norm: 83.24736022949219, minimum ratio: 0.78125\n",
      "Epoch [493], val_loss: 152.8296\n",
      "gradient norm: 83.5763144493103, minimum ratio: 0.78515625\n",
      "Epoch [494], val_loss: 153.7798\n",
      "gradient norm: 83.90696573257446, minimum ratio: 0.7734375\n",
      "Epoch [495], val_loss: 154.7346\n",
      "gradient norm: 84.24267625808716, minimum ratio: 0.76171875\n",
      "Epoch [496], val_loss: 155.6940\n",
      "gradient norm: 84.57646656036377, minimum ratio: 0.76171875\n",
      "Epoch [497], val_loss: 156.6580\n",
      "gradient norm: 84.91184186935425, minimum ratio: 0.76171875\n",
      "Epoch [498], val_loss: 157.6266\n",
      "gradient norm: 85.24801111221313, minimum ratio: 0.76953125\n",
      "Epoch [499], val_loss: 158.5998\n",
      "gradient norm: 85.5854058265686, minimum ratio: 0.7734375\n",
      "Epoch [500], val_loss: 159.5778\n",
      "gradient norm: 85.92085075378418, minimum ratio: 0.76171875\n",
      "Epoch [501], val_loss: 160.5605\n",
      "gradient norm: 86.25727033615112, minimum ratio: 0.7734375\n",
      "Epoch [502], val_loss: 161.5479\n",
      "gradient norm: 86.59701681137085, minimum ratio: 0.7734375\n",
      "Epoch [503], val_loss: 162.5400\n",
      "gradient norm: 86.93803930282593, minimum ratio: 0.765625\n",
      "Epoch [504], val_loss: 163.5370\n",
      "gradient norm: 87.28078317642212, minimum ratio: 0.765625\n",
      "Epoch [505], val_loss: 164.5387\n",
      "gradient norm: 87.62424945831299, minimum ratio: 0.765625\n",
      "Epoch [506], val_loss: 165.5452\n",
      "gradient norm: 87.96642780303955, minimum ratio: 0.7734375\n",
      "Epoch [507], val_loss: 166.5565\n",
      "gradient norm: 88.31021928787231, minimum ratio: 0.7734375\n",
      "Epoch [508], val_loss: 167.5726\n",
      "gradient norm: 88.65510654449463, minimum ratio: 0.76171875\n",
      "Epoch [509], val_loss: 168.5936\n",
      "gradient norm: 89.00214910507202, minimum ratio: 0.7734375\n",
      "Epoch [510], val_loss: 169.6194\n",
      "gradient norm: 89.34949254989624, minimum ratio: 0.76953125\n",
      "Epoch [511], val_loss: 170.6500\n",
      "gradient norm: 89.69781970977783, minimum ratio: 0.76171875\n",
      "Epoch [512], val_loss: 171.6854\n",
      "gradient norm: 90.04899883270264, minimum ratio: 0.7734375\n",
      "Epoch [513], val_loss: 172.7258\n",
      "gradient norm: 90.39919757843018, minimum ratio: 0.77734375\n",
      "Epoch [514], val_loss: 173.7710\n",
      "gradient norm: 90.748863697052, minimum ratio: 0.765625\n",
      "Epoch [515], val_loss: 174.8211\n",
      "gradient norm: 91.10167980194092, minimum ratio: 0.75\n",
      "Epoch [516], val_loss: 175.8762\n",
      "gradient norm: 91.45238780975342, minimum ratio: 0.73828125\n",
      "Epoch [517], val_loss: 176.9362\n",
      "gradient norm: 91.80395317077637, minimum ratio: 0.7421875\n",
      "Epoch [518], val_loss: 178.0011\n",
      "gradient norm: 92.15781307220459, minimum ratio: 0.78515625\n",
      "Epoch [519], val_loss: 179.0709\n",
      "gradient norm: 92.51254558563232, minimum ratio: 0.765625\n",
      "Epoch [520], val_loss: 180.1458\n",
      "gradient norm: 92.87196445465088, minimum ratio: 0.7578125\n",
      "Epoch [521], val_loss: 181.2256\n",
      "gradient norm: 93.22879123687744, minimum ratio: 0.75390625\n",
      "Epoch [522], val_loss: 182.3105\n",
      "gradient norm: 93.58857107162476, minimum ratio: 0.77734375\n",
      "Epoch [523], val_loss: 183.4004\n",
      "gradient norm: 93.94759607315063, minimum ratio: 0.75\n",
      "Epoch [524], val_loss: 184.4954\n",
      "gradient norm: 94.30775022506714, minimum ratio: 0.7578125\n",
      "Epoch [525], val_loss: 185.5954\n",
      "gradient norm: 94.66979455947876, minimum ratio: 0.76171875\n",
      "Epoch [526], val_loss: 186.7004\n",
      "gradient norm: 95.03053760528564, minimum ratio: 0.7578125\n",
      "Epoch [527], val_loss: 187.8106\n",
      "gradient norm: 95.39438009262085, minimum ratio: 0.74609375\n",
      "Epoch [528], val_loss: 188.9258\n",
      "gradient norm: 95.75847959518433, minimum ratio: 0.76171875\n",
      "Epoch [529], val_loss: 190.0461\n",
      "gradient norm: 96.12232637405396, minimum ratio: 0.76953125\n",
      "Epoch [530], val_loss: 191.1715\n",
      "gradient norm: 96.49117660522461, minimum ratio: 0.765625\n",
      "Epoch [531], val_loss: 192.3022\n",
      "gradient norm: 96.85404014587402, minimum ratio: 0.75\n",
      "Epoch [532], val_loss: 193.4381\n",
      "gradient norm: 97.22051095962524, minimum ratio: 0.77734375\n",
      "Epoch [533], val_loss: 194.5791\n",
      "gradient norm: 97.59006309509277, minimum ratio: 0.76171875\n",
      "Epoch [534], val_loss: 195.7254\n",
      "gradient norm: 97.96097230911255, minimum ratio: 0.76171875\n",
      "Epoch [535], val_loss: 196.8768\n",
      "gradient norm: 98.3308367729187, minimum ratio: 0.77734375\n",
      "Epoch [536], val_loss: 198.0334\n",
      "gradient norm: 98.70049333572388, minimum ratio: 0.7578125\n",
      "Epoch [537], val_loss: 199.1953\n",
      "gradient norm: 99.07344388961792, minimum ratio: 0.75390625\n",
      "Epoch [538], val_loss: 200.3626\n",
      "gradient norm: 99.4453821182251, minimum ratio: 0.7734375\n",
      "Epoch [539], val_loss: 201.5352\n",
      "gradient norm: 99.82214641571045, minimum ratio: 0.765625\n",
      "Epoch [540], val_loss: 202.7132\n",
      "gradient norm: 100.19782209396362, minimum ratio: 0.79296875\n",
      "Epoch [541], val_loss: 203.8965\n",
      "gradient norm: 100.57486057281494, minimum ratio: 0.76171875\n",
      "Epoch [542], val_loss: 205.0852\n",
      "gradient norm: 100.95313119888306, minimum ratio: 0.75390625\n",
      "Epoch [543], val_loss: 206.2791\n",
      "gradient norm: 101.33299207687378, minimum ratio: 0.765625\n",
      "Epoch [544], val_loss: 207.4785\n",
      "gradient norm: 101.71084785461426, minimum ratio: 0.76171875\n",
      "Epoch [545], val_loss: 208.6832\n",
      "gradient norm: 102.09078073501587, minimum ratio: 0.78515625\n",
      "Epoch [546], val_loss: 209.8934\n",
      "gradient norm: 102.47188520431519, minimum ratio: 0.75390625\n",
      "Epoch [547], val_loss: 211.1090\n",
      "gradient norm: 102.85548782348633, minimum ratio: 0.76171875\n",
      "Epoch [548], val_loss: 212.3300\n",
      "gradient norm: 103.24006652832031, minimum ratio: 0.78125\n",
      "Epoch [549], val_loss: 213.5566\n",
      "gradient norm: 103.62545442581177, minimum ratio: 0.765625\n",
      "Epoch [550], val_loss: 214.7885\n",
      "gradient norm: 104.0098385810852, minimum ratio: 0.74609375\n",
      "Epoch [551], val_loss: 216.0261\n",
      "gradient norm: 104.39543533325195, minimum ratio: 0.765625\n",
      "Epoch [552], val_loss: 217.2692\n",
      "gradient norm: 104.7842526435852, minimum ratio: 0.79296875\n",
      "Epoch [553], val_loss: 218.5177\n",
      "gradient norm: 105.17296266555786, minimum ratio: 0.75390625\n",
      "Epoch [554], val_loss: 219.7717\n",
      "gradient norm: 105.56052684783936, minimum ratio: 0.76953125\n",
      "Epoch [555], val_loss: 221.0314\n",
      "gradient norm: 105.95098781585693, minimum ratio: 0.75\n",
      "Epoch [556], val_loss: 222.2967\n",
      "gradient norm: 106.34277296066284, minimum ratio: 0.7734375\n",
      "Epoch [557], val_loss: 223.5676\n",
      "gradient norm: 106.73738384246826, minimum ratio: 0.77734375\n",
      "Epoch [558], val_loss: 224.8442\n",
      "gradient norm: 107.13086748123169, minimum ratio: 0.76953125\n",
      "Epoch [559], val_loss: 226.1263\n",
      "gradient norm: 107.52331829071045, minimum ratio: 0.75\n",
      "Epoch [560], val_loss: 227.4139\n",
      "gradient norm: 107.91894006729126, minimum ratio: 0.76953125\n",
      "Epoch [561], val_loss: 228.7071\n",
      "gradient norm: 108.31394529342651, minimum ratio: 0.75390625\n",
      "Epoch [562], val_loss: 230.0061\n",
      "gradient norm: 108.712571144104, minimum ratio: 0.7734375\n",
      "Epoch [563], val_loss: 231.3109\n",
      "gradient norm: 109.11020517349243, minimum ratio: 0.78515625\n",
      "Epoch [564], val_loss: 232.6213\n",
      "gradient norm: 109.50987386703491, minimum ratio: 0.76953125\n",
      "Epoch [565], val_loss: 233.9374\n",
      "gradient norm: 109.90852880477905, minimum ratio: 0.75\n",
      "Epoch [566], val_loss: 235.2592\n",
      "gradient norm: 110.3116807937622, minimum ratio: 0.76171875\n",
      "Epoch [567], val_loss: 236.5867\n",
      "gradient norm: 110.71266317367554, minimum ratio: 0.75\n",
      "Epoch [568], val_loss: 237.9201\n",
      "gradient norm: 111.11722707748413, minimum ratio: 0.75390625\n",
      "Epoch [569], val_loss: 239.2591\n",
      "gradient norm: 111.52201890945435, minimum ratio: 0.76171875\n",
      "Epoch [570], val_loss: 240.6041\n",
      "gradient norm: 111.92773294448853, minimum ratio: 0.78125\n",
      "Epoch [571], val_loss: 241.9548\n",
      "gradient norm: 112.33088493347168, minimum ratio: 0.76953125\n",
      "Epoch [572], val_loss: 243.3114\n",
      "gradient norm: 112.73785591125488, minimum ratio: 0.7734375\n",
      "Epoch [573], val_loss: 244.6740\n",
      "gradient norm: 113.14847183227539, minimum ratio: 0.78515625\n",
      "Epoch [574], val_loss: 246.0426\n",
      "gradient norm: 113.55796337127686, minimum ratio: 0.7734375\n",
      "Epoch [575], val_loss: 247.4170\n",
      "gradient norm: 113.9653754234314, minimum ratio: 0.78125\n",
      "Epoch [576], val_loss: 248.7973\n",
      "gradient norm: 114.3778977394104, minimum ratio: 0.78515625\n",
      "Epoch [577], val_loss: 250.1834\n",
      "gradient norm: 114.78942489624023, minimum ratio: 0.7578125\n",
      "Epoch [578], val_loss: 251.5756\n",
      "gradient norm: 115.2043571472168, minimum ratio: 0.7734375\n",
      "Epoch [579], val_loss: 252.9736\n",
      "gradient norm: 115.61828899383545, minimum ratio: 0.78125\n",
      "Epoch [580], val_loss: 254.3775\n",
      "gradient norm: 116.03563404083252, minimum ratio: 0.7421875\n",
      "Epoch [581], val_loss: 255.7874\n",
      "gradient norm: 116.45255756378174, minimum ratio: 0.76953125\n",
      "Epoch [582], val_loss: 257.2034\n",
      "gradient norm: 116.87035274505615, minimum ratio: 0.76953125\n",
      "Epoch [583], val_loss: 258.6255\n",
      "gradient norm: 117.28597640991211, minimum ratio: 0.76171875\n",
      "Epoch [584], val_loss: 260.0535\n",
      "gradient norm: 117.70453834533691, minimum ratio: 0.74609375\n",
      "Epoch [585], val_loss: 261.4877\n",
      "gradient norm: 118.1250638961792, minimum ratio: 0.76171875\n",
      "Epoch [586], val_loss: 262.9280\n",
      "gradient norm: 118.54733371734619, minimum ratio: 0.75390625\n",
      "Epoch [587], val_loss: 264.3744\n",
      "gradient norm: 118.9703598022461, minimum ratio: 0.78125\n",
      "Epoch [588], val_loss: 265.8268\n",
      "gradient norm: 119.39238834381104, minimum ratio: 0.76953125\n",
      "Epoch [589], val_loss: 267.2856\n",
      "gradient norm: 119.81697273254395, minimum ratio: 0.7578125\n",
      "Epoch [590], val_loss: 268.7505\n",
      "gradient norm: 120.24225425720215, minimum ratio: 0.76953125\n",
      "Epoch [591], val_loss: 270.2215\n",
      "gradient norm: 120.66893291473389, minimum ratio: 0.75\n",
      "Epoch [592], val_loss: 271.6987\n",
      "gradient norm: 121.0971565246582, minimum ratio: 0.7578125\n",
      "Epoch [593], val_loss: 273.1819\n",
      "gradient norm: 121.5257568359375, minimum ratio: 0.75390625\n",
      "Epoch [594], val_loss: 274.6714\n",
      "gradient norm: 121.95589351654053, minimum ratio: 0.77734375\n",
      "Epoch [595], val_loss: 276.1672\n",
      "gradient norm: 122.38674449920654, minimum ratio: 0.7578125\n",
      "Epoch [596], val_loss: 277.6693\n",
      "gradient norm: 122.81871223449707, minimum ratio: 0.76953125\n",
      "Epoch [597], val_loss: 279.1778\n",
      "gradient norm: 123.2513837814331, minimum ratio: 0.77734375\n",
      "Epoch [598], val_loss: 280.6924\n",
      "gradient norm: 123.68654823303223, minimum ratio: 0.7578125\n",
      "Epoch [599], val_loss: 282.2133\n",
      "gradient norm: 124.11956024169922, minimum ratio: 0.79296875\n",
      "Epoch [600], val_loss: 283.7407\n",
      "gradient norm: 124.55451583862305, minimum ratio: 0.76171875\n",
      "Epoch [601], val_loss: 285.2744\n",
      "gradient norm: 124.99168872833252, minimum ratio: 0.76171875\n",
      "Epoch [602], val_loss: 286.8147\n",
      "gradient norm: 125.4314603805542, minimum ratio: 0.78125\n",
      "Epoch [603], val_loss: 288.3614\n",
      "gradient norm: 125.87157821655273, minimum ratio: 0.74609375\n",
      "Epoch [604], val_loss: 289.9145\n",
      "gradient norm: 126.31478214263916, minimum ratio: 0.74609375\n",
      "Epoch [605], val_loss: 291.4738\n",
      "gradient norm: 126.7556381225586, minimum ratio: 0.7578125\n",
      "Epoch [606], val_loss: 293.0397\n",
      "gradient norm: 127.19762325286865, minimum ratio: 0.765625\n",
      "Epoch [607], val_loss: 294.6119\n",
      "gradient norm: 127.64164638519287, minimum ratio: 0.75\n",
      "Epoch [608], val_loss: 296.1908\n",
      "gradient norm: 128.08289432525635, minimum ratio: 0.75\n",
      "Epoch [609], val_loss: 297.7762\n",
      "gradient norm: 128.5288953781128, minimum ratio: 0.76171875\n",
      "Epoch [610], val_loss: 299.3680\n",
      "gradient norm: 128.9753761291504, minimum ratio: 0.765625\n",
      "Epoch [611], val_loss: 300.9662\n",
      "gradient norm: 129.42492485046387, minimum ratio: 0.7578125\n",
      "Epoch [612], val_loss: 302.5708\n",
      "gradient norm: 129.87546825408936, minimum ratio: 0.75\n",
      "Epoch [613], val_loss: 304.1821\n",
      "gradient norm: 130.32434844970703, minimum ratio: 0.734375\n",
      "Epoch [614], val_loss: 305.8000\n",
      "gradient norm: 130.7746343612671, minimum ratio: 0.7578125\n",
      "Epoch [615], val_loss: 307.4246\n",
      "gradient norm: 131.22593593597412, minimum ratio: 0.765625\n",
      "Epoch [616], val_loss: 309.0557\n",
      "gradient norm: 131.67305374145508, minimum ratio: 0.75\n",
      "Epoch [617], val_loss: 310.6936\n",
      "gradient norm: 132.1243715286255, minimum ratio: 0.7578125\n",
      "Epoch [618], val_loss: 312.3381\n",
      "gradient norm: 132.57940673828125, minimum ratio: 0.7421875\n",
      "Epoch [619], val_loss: 313.9893\n",
      "gradient norm: 133.03790855407715, minimum ratio: 0.74609375\n",
      "Epoch [620], val_loss: 315.6471\n",
      "gradient norm: 133.49716091156006, minimum ratio: 0.78125\n",
      "Epoch [621], val_loss: 317.3117\n",
      "gradient norm: 133.9560031890869, minimum ratio: 0.78515625\n",
      "Epoch [622], val_loss: 318.9828\n",
      "gradient norm: 134.41729831695557, minimum ratio: 0.73828125\n",
      "Epoch [623], val_loss: 320.6606\n",
      "gradient norm: 134.87801933288574, minimum ratio: 0.76953125\n",
      "Epoch [624], val_loss: 322.3451\n",
      "gradient norm: 135.33633041381836, minimum ratio: 0.76171875\n",
      "Epoch [625], val_loss: 324.0367\n",
      "gradient norm: 135.7953338623047, minimum ratio: 0.76953125\n",
      "Epoch [626], val_loss: 325.7349\n",
      "gradient norm: 136.25837898254395, minimum ratio: 0.78515625\n",
      "Epoch [627], val_loss: 327.4402\n",
      "gradient norm: 136.7243309020996, minimum ratio: 0.765625\n",
      "Epoch [628], val_loss: 329.1523\n",
      "gradient norm: 137.18968772888184, minimum ratio: 0.73828125\n",
      "Epoch [629], val_loss: 330.8712\n",
      "gradient norm: 137.65711879730225, minimum ratio: 0.78125\n",
      "Epoch [630], val_loss: 332.5971\n",
      "gradient norm: 138.1256504058838, minimum ratio: 0.76953125\n",
      "Epoch [631], val_loss: 334.3298\n",
      "gradient norm: 138.5947380065918, minimum ratio: 0.75\n",
      "Epoch [632], val_loss: 336.0693\n",
      "gradient norm: 139.0648317337036, minimum ratio: 0.76953125\n",
      "Epoch [633], val_loss: 337.8157\n",
      "gradient norm: 139.53231239318848, minimum ratio: 0.7578125\n",
      "Epoch [634], val_loss: 339.5691\n",
      "gradient norm: 140.00317001342773, minimum ratio: 0.74609375\n",
      "Epoch [635], val_loss: 341.3295\n",
      "gradient norm: 140.4775447845459, minimum ratio: 0.76953125\n",
      "Epoch [636], val_loss: 343.0970\n",
      "gradient norm: 140.9530487060547, minimum ratio: 0.74609375\n",
      "Epoch [637], val_loss: 344.8716\n",
      "gradient norm: 141.4300298690796, minimum ratio: 0.7734375\n",
      "Epoch [638], val_loss: 346.6534\n",
      "gradient norm: 141.90306568145752, minimum ratio: 0.75390625\n",
      "Epoch [639], val_loss: 348.4423\n",
      "gradient norm: 142.3819923400879, minimum ratio: 0.76171875\n",
      "Epoch [640], val_loss: 350.2382\n",
      "gradient norm: 142.86279773712158, minimum ratio: 0.75390625\n",
      "Epoch [641], val_loss: 352.0413\n",
      "gradient norm: 143.3430814743042, minimum ratio: 0.75390625\n",
      "Epoch [642], val_loss: 353.8514\n",
      "gradient norm: 143.8222131729126, minimum ratio: 0.7734375\n",
      "Epoch [643], val_loss: 355.6686\n",
      "gradient norm: 144.30152988433838, minimum ratio: 0.7578125\n",
      "Epoch [644], val_loss: 357.4930\n",
      "gradient norm: 144.78664207458496, minimum ratio: 0.7578125\n",
      "Epoch [645], val_loss: 359.3244\n",
      "gradient norm: 145.2737216949463, minimum ratio: 0.75\n",
      "Epoch [646], val_loss: 361.1632\n",
      "gradient norm: 145.75862503051758, minimum ratio: 0.78515625\n",
      "Epoch [647], val_loss: 363.0091\n",
      "gradient norm: 146.2437219619751, minimum ratio: 0.76171875\n",
      "Epoch [648], val_loss: 364.8623\n",
      "gradient norm: 146.73257064819336, minimum ratio: 0.7734375\n",
      "Epoch [649], val_loss: 366.7228\n",
      "gradient norm: 147.21960353851318, minimum ratio: 0.74609375\n",
      "Epoch [650], val_loss: 368.5906\n",
      "gradient norm: 147.70810317993164, minimum ratio: 0.76171875\n",
      "Epoch [651], val_loss: 370.4659\n",
      "gradient norm: 148.20088005065918, minimum ratio: 0.7421875\n",
      "Epoch [652], val_loss: 372.3484\n",
      "gradient norm: 148.68560981750488, minimum ratio: 0.734375\n",
      "Epoch [653], val_loss: 374.2383\n",
      "gradient norm: 149.1810817718506, minimum ratio: 0.75390625\n",
      "Epoch [654], val_loss: 376.1353\n",
      "gradient norm: 149.67544174194336, minimum ratio: 0.76953125\n",
      "Epoch [655], val_loss: 378.0396\n",
      "gradient norm: 150.17520904541016, minimum ratio: 0.765625\n",
      "Epoch [656], val_loss: 379.9513\n",
      "gradient norm: 150.67305850982666, minimum ratio: 0.76171875\n",
      "Epoch [657], val_loss: 381.8703\n",
      "gradient norm: 151.1725368499756, minimum ratio: 0.75390625\n",
      "Epoch [658], val_loss: 383.7968\n",
      "gradient norm: 151.66970252990723, minimum ratio: 0.78125\n",
      "Epoch [659], val_loss: 385.7308\n",
      "gradient norm: 152.16495323181152, minimum ratio: 0.734375\n",
      "Epoch [660], val_loss: 387.6721\n",
      "gradient norm: 152.66796970367432, minimum ratio: 0.74609375\n",
      "Epoch [661], val_loss: 389.6210\n",
      "gradient norm: 153.16966152191162, minimum ratio: 0.73828125\n",
      "Epoch [662], val_loss: 391.5775\n",
      "gradient norm: 153.66932678222656, minimum ratio: 0.7421875\n",
      "Epoch [663], val_loss: 393.5414\n",
      "gradient norm: 154.17418670654297, minimum ratio: 0.7265625\n",
      "Epoch [664], val_loss: 395.5129\n",
      "gradient norm: 154.6805591583252, minimum ratio: 0.75\n",
      "Epoch [665], val_loss: 397.4919\n",
      "gradient norm: 155.18912887573242, minimum ratio: 0.7734375\n",
      "Epoch [666], val_loss: 399.4785\n",
      "gradient norm: 155.69935035705566, minimum ratio: 0.7578125\n",
      "Epoch [667], val_loss: 401.4727\n",
      "gradient norm: 156.2043924331665, minimum ratio: 0.734375\n",
      "Epoch [668], val_loss: 403.4744\n",
      "gradient norm: 156.71589183807373, minimum ratio: 0.78515625\n",
      "Epoch [669], val_loss: 405.4839\n",
      "gradient norm: 157.2251377105713, minimum ratio: 0.73828125\n",
      "Epoch [670], val_loss: 407.5012\n",
      "gradient norm: 157.73806476593018, minimum ratio: 0.7578125\n",
      "Epoch [671], val_loss: 409.5261\n",
      "gradient norm: 158.2480983734131, minimum ratio: 0.75390625\n",
      "Epoch [672], val_loss: 411.5590\n",
      "gradient norm: 158.76079654693604, minimum ratio: 0.76953125\n",
      "Epoch [673], val_loss: 413.5996\n",
      "gradient norm: 159.2719373703003, minimum ratio: 0.75390625\n",
      "Epoch [674], val_loss: 415.6480\n",
      "gradient norm: 159.79010105133057, minimum ratio: 0.765625\n",
      "Epoch [675], val_loss: 417.7041\n",
      "gradient norm: 160.30805110931396, minimum ratio: 0.76171875\n",
      "Epoch [676], val_loss: 419.7681\n",
      "gradient norm: 160.82884120941162, minimum ratio: 0.7421875\n",
      "Epoch [677], val_loss: 421.8398\n",
      "gradient norm: 161.34808254241943, minimum ratio: 0.765625\n",
      "Epoch [678], val_loss: 423.9193\n",
      "gradient norm: 161.8728265762329, minimum ratio: 0.75\n",
      "Epoch [679], val_loss: 426.0066\n",
      "gradient norm: 162.3981647491455, minimum ratio: 0.75\n",
      "Epoch [680], val_loss: 428.1018\n",
      "gradient norm: 162.92241764068604, minimum ratio: 0.7578125\n",
      "Epoch [681], val_loss: 430.2050\n",
      "gradient norm: 163.44368839263916, minimum ratio: 0.7421875\n",
      "Epoch [682], val_loss: 432.3159\n",
      "gradient norm: 163.96714401245117, minimum ratio: 0.73828125\n",
      "Epoch [683], val_loss: 434.4348\n",
      "gradient norm: 164.49287128448486, minimum ratio: 0.7578125\n",
      "Epoch [684], val_loss: 436.5616\n",
      "gradient norm: 165.0204620361328, minimum ratio: 0.76171875\n",
      "Epoch [685], val_loss: 438.6964\n",
      "gradient norm: 165.55071640014648, minimum ratio: 0.75390625\n",
      "Epoch [686], val_loss: 440.8391\n",
      "gradient norm: 166.08056449890137, minimum ratio: 0.75390625\n",
      "Epoch [687], val_loss: 442.9901\n",
      "gradient norm: 166.61186122894287, minimum ratio: 0.75390625\n",
      "Epoch [688], val_loss: 445.1490\n",
      "gradient norm: 167.14712047576904, minimum ratio: 0.74609375\n",
      "Epoch [689], val_loss: 447.3159\n",
      "gradient norm: 167.6797571182251, minimum ratio: 0.76953125\n",
      "Epoch [690], val_loss: 449.4909\n",
      "gradient norm: 168.21324062347412, minimum ratio: 0.75390625\n",
      "Epoch [691], val_loss: 451.6741\n",
      "gradient norm: 168.74839687347412, minimum ratio: 0.75390625\n",
      "Epoch [692], val_loss: 453.8654\n",
      "gradient norm: 169.28685092926025, minimum ratio: 0.76171875\n",
      "Epoch [693], val_loss: 456.0647\n",
      "gradient norm: 169.8217945098877, minimum ratio: 0.76953125\n",
      "Epoch [694], val_loss: 458.2723\n",
      "gradient norm: 170.36086177825928, minimum ratio: 0.765625\n",
      "Epoch [695], val_loss: 460.4882\n",
      "gradient norm: 170.90283203125, minimum ratio: 0.75390625\n",
      "Epoch [696], val_loss: 462.7123\n",
      "gradient norm: 171.44616031646729, minimum ratio: 0.7421875\n",
      "Epoch [697], val_loss: 464.9447\n",
      "gradient norm: 171.99178886413574, minimum ratio: 0.72265625\n",
      "Epoch [698], val_loss: 467.1852\n",
      "gradient norm: 172.53764247894287, minimum ratio: 0.76953125\n",
      "Epoch [699], val_loss: 469.4340\n",
      "gradient norm: 173.08216190338135, minimum ratio: 0.7734375\n",
      "Epoch [700], val_loss: 471.6910\n",
      "gradient norm: 173.62462043762207, minimum ratio: 0.73828125\n",
      "Epoch [701], val_loss: 473.9562\n",
      "gradient norm: 174.17228317260742, minimum ratio: 0.75390625\n",
      "Epoch [702], val_loss: 476.2300\n",
      "gradient norm: 174.72143650054932, minimum ratio: 0.734375\n",
      "Epoch [703], val_loss: 478.5122\n",
      "gradient norm: 175.2704620361328, minimum ratio: 0.76171875\n",
      "Epoch [704], val_loss: 480.8028\n",
      "gradient norm: 175.8200273513794, minimum ratio: 0.7578125\n",
      "Epoch [705], val_loss: 483.1019\n",
      "gradient norm: 176.37398529052734, minimum ratio: 0.76953125\n",
      "Epoch [706], val_loss: 485.4095\n",
      "gradient norm: 176.93130207061768, minimum ratio: 0.75\n",
      "Epoch [707], val_loss: 487.7254\n",
      "gradient norm: 177.48903846740723, minimum ratio: 0.75390625\n",
      "Epoch [708], val_loss: 490.0498\n",
      "gradient norm: 178.03905773162842, minimum ratio: 0.76171875\n",
      "Epoch [709], val_loss: 492.3828\n",
      "gradient norm: 178.59712028503418, minimum ratio: 0.78125\n",
      "Epoch [710], val_loss: 494.7244\n",
      "gradient norm: 179.15424156188965, minimum ratio: 0.76953125\n",
      "Epoch [711], val_loss: 497.0744\n",
      "gradient norm: 179.7167510986328, minimum ratio: 0.7578125\n",
      "Epoch [712], val_loss: 499.4330\n",
      "gradient norm: 180.27805519104004, minimum ratio: 0.75\n",
      "Epoch [713], val_loss: 501.8003\n",
      "gradient norm: 180.83900356292725, minimum ratio: 0.73828125\n",
      "Epoch [714], val_loss: 504.1761\n",
      "gradient norm: 181.39863109588623, minimum ratio: 0.75\n",
      "Epoch [715], val_loss: 506.5606\n",
      "gradient norm: 181.96398258209229, minimum ratio: 0.765625\n",
      "Epoch [716], val_loss: 508.9538\n",
      "gradient norm: 182.5300817489624, minimum ratio: 0.7578125\n",
      "Epoch [717], val_loss: 511.3556\n",
      "gradient norm: 183.09763050079346, minimum ratio: 0.75390625\n",
      "Epoch [718], val_loss: 513.7659\n",
      "gradient norm: 183.66762161254883, minimum ratio: 0.75390625\n",
      "Epoch [719], val_loss: 516.1851\n",
      "gradient norm: 184.24112796783447, minimum ratio: 0.734375\n",
      "Epoch [720], val_loss: 518.6130\n",
      "gradient norm: 184.80867958068848, minimum ratio: 0.74609375\n",
      "Epoch [721], val_loss: 521.0495\n",
      "gradient norm: 185.38124561309814, minimum ratio: 0.76953125\n",
      "Epoch [722], val_loss: 523.4948\n",
      "gradient norm: 185.9513292312622, minimum ratio: 0.76171875\n",
      "Epoch [723], val_loss: 525.9488\n",
      "gradient norm: 186.5253438949585, minimum ratio: 0.75\n",
      "Epoch [724], val_loss: 528.4117\n",
      "gradient norm: 187.10463047027588, minimum ratio: 0.74609375\n",
      "Epoch [725], val_loss: 530.8832\n",
      "gradient norm: 187.6753807067871, minimum ratio: 0.75390625\n",
      "Epoch [726], val_loss: 533.3635\n",
      "gradient norm: 188.25335597991943, minimum ratio: 0.734375\n",
      "Epoch [727], val_loss: 535.8527\n",
      "gradient norm: 188.82951068878174, minimum ratio: 0.74609375\n",
      "Epoch [728], val_loss: 538.3506\n",
      "gradient norm: 189.4109525680542, minimum ratio: 0.765625\n",
      "Epoch [729], val_loss: 540.8577\n",
      "gradient norm: 189.98804473876953, minimum ratio: 0.7578125\n",
      "Epoch [730], val_loss: 543.3735\n",
      "gradient norm: 190.5704927444458, minimum ratio: 0.75\n",
      "Epoch [731], val_loss: 545.8984\n",
      "gradient norm: 191.1575689315796, minimum ratio: 0.75390625\n",
      "Epoch [732], val_loss: 548.4321\n",
      "gradient norm: 191.74062156677246, minimum ratio: 0.74609375\n",
      "Epoch [733], val_loss: 550.9750\n",
      "gradient norm: 192.3296184539795, minimum ratio: 0.75390625\n",
      "Epoch [734], val_loss: 553.5267\n",
      "gradient norm: 192.9152956008911, minimum ratio: 0.765625\n",
      "Epoch [735], val_loss: 556.0874\n",
      "gradient norm: 193.50410270690918, minimum ratio: 0.76953125\n",
      "Epoch [736], val_loss: 558.6572\n",
      "gradient norm: 194.09242057800293, minimum ratio: 0.7578125\n",
      "Epoch [737], val_loss: 561.2359\n",
      "gradient norm: 194.68553733825684, minimum ratio: 0.76171875\n",
      "Epoch [738], val_loss: 563.8238\n",
      "gradient norm: 195.2788906097412, minimum ratio: 0.77734375\n",
      "Epoch [739], val_loss: 566.4210\n",
      "gradient norm: 195.87175178527832, minimum ratio: 0.7578125\n",
      "Epoch [740], val_loss: 569.0272\n",
      "gradient norm: 196.4597396850586, minimum ratio: 0.765625\n",
      "Epoch [741], val_loss: 571.6424\n",
      "gradient norm: 197.058837890625, minimum ratio: 0.76171875\n",
      "Epoch [742], val_loss: 574.2669\n",
      "gradient norm: 197.65188217163086, minimum ratio: 0.765625\n",
      "Epoch [743], val_loss: 576.9007\n",
      "gradient norm: 198.24929809570312, minimum ratio: 0.76953125\n",
      "Epoch [744], val_loss: 579.5437\n",
      "gradient norm: 198.85034084320068, minimum ratio: 0.76953125\n",
      "Epoch [745], val_loss: 582.1959\n",
      "gradient norm: 199.4498405456543, minimum ratio: 0.76953125\n",
      "Epoch [746], val_loss: 584.8573\n",
      "gradient norm: 200.05383968353271, minimum ratio: 0.76171875\n",
      "Epoch [747], val_loss: 587.5279\n",
      "gradient norm: 200.6507043838501, minimum ratio: 0.76953125\n",
      "Epoch [748], val_loss: 590.2079\n",
      "gradient norm: 201.2571029663086, minimum ratio: 0.76953125\n",
      "Epoch [749], val_loss: 592.8969\n",
      "gradient norm: 201.8654146194458, minimum ratio: 0.74609375\n",
      "Epoch [750], val_loss: 595.5950\n",
      "gradient norm: 202.46758937835693, minimum ratio: 0.76171875\n",
      "Epoch [751], val_loss: 598.3026\n",
      "gradient norm: 203.077299118042, minimum ratio: 0.7734375\n",
      "Epoch [752], val_loss: 601.0193\n",
      "gradient norm: 203.68724822998047, minimum ratio: 0.7421875\n",
      "Epoch [753], val_loss: 603.7457\n",
      "gradient norm: 204.29768562316895, minimum ratio: 0.7734375\n",
      "Epoch [754], val_loss: 606.4817\n",
      "gradient norm: 204.91044425964355, minimum ratio: 0.75390625\n",
      "Epoch [755], val_loss: 609.2271\n",
      "gradient norm: 205.52136993408203, minimum ratio: 0.76953125\n",
      "Epoch [756], val_loss: 611.9820\n",
      "gradient norm: 206.1379566192627, minimum ratio: 0.7734375\n",
      "Epoch [757], val_loss: 614.7461\n",
      "gradient norm: 206.7569694519043, minimum ratio: 0.77734375\n",
      "Epoch [758], val_loss: 617.5197\n",
      "gradient norm: 207.37418365478516, minimum ratio: 0.75390625\n",
      "Epoch [759], val_loss: 620.3029\n",
      "gradient norm: 207.98565673828125, minimum ratio: 0.75390625\n",
      "Epoch [760], val_loss: 623.0952\n",
      "gradient norm: 208.60451889038086, minimum ratio: 0.7890625\n",
      "Epoch [761], val_loss: 625.8974\n",
      "gradient norm: 209.21910285949707, minimum ratio: 0.7578125\n",
      "Epoch [762], val_loss: 628.7089\n",
      "gradient norm: 209.83942985534668, minimum ratio: 0.75\n",
      "Epoch [763], val_loss: 631.5303\n",
      "gradient norm: 210.46383666992188, minimum ratio: 0.77734375\n",
      "Epoch [764], val_loss: 634.3610\n",
      "gradient norm: 211.08842086791992, minimum ratio: 0.7734375\n",
      "Epoch [765], val_loss: 637.2014\n",
      "gradient norm: 211.71555519104004, minimum ratio: 0.7421875\n",
      "Epoch [766], val_loss: 640.0513\n",
      "gradient norm: 212.3456802368164, minimum ratio: 0.7734375\n",
      "Epoch [767], val_loss: 642.9109\n",
      "gradient norm: 212.97288513183594, minimum ratio: 0.76953125\n",
      "Epoch [768], val_loss: 645.7802\n",
      "gradient norm: 213.60509490966797, minimum ratio: 0.7578125\n",
      "Epoch [769], val_loss: 648.6591\n",
      "gradient norm: 214.23051261901855, minimum ratio: 0.76171875\n",
      "Epoch [770], val_loss: 651.5478\n",
      "gradient norm: 214.86104774475098, minimum ratio: 0.76171875\n",
      "Epoch [771], val_loss: 654.4459\n",
      "gradient norm: 215.48808479309082, minimum ratio: 0.78515625\n",
      "Epoch [772], val_loss: 657.3538\n",
      "gradient norm: 216.12031745910645, minimum ratio: 0.77734375\n",
      "Epoch [773], val_loss: 660.2715\n",
      "gradient norm: 216.75568389892578, minimum ratio: 0.76953125\n",
      "Epoch [774], val_loss: 663.1991\n",
      "gradient norm: 217.38745880126953, minimum ratio: 0.765625\n",
      "Epoch [775], val_loss: 666.1365\n",
      "gradient norm: 218.0228214263916, minimum ratio: 0.75390625\n",
      "Epoch [776], val_loss: 669.0840\n",
      "gradient norm: 218.66382789611816, minimum ratio: 0.76953125\n",
      "Epoch [777], val_loss: 672.0414\n",
      "gradient norm: 219.30091667175293, minimum ratio: 0.77734375\n",
      "Epoch [778], val_loss: 675.0084\n",
      "gradient norm: 219.94188499450684, minimum ratio: 0.78125\n",
      "Epoch [779], val_loss: 677.9855\n",
      "gradient norm: 220.5858211517334, minimum ratio: 0.78125\n",
      "Epoch [780], val_loss: 680.9725\n",
      "gradient norm: 221.2314682006836, minimum ratio: 0.7734375\n",
      "Epoch [781], val_loss: 683.9694\n",
      "gradient norm: 221.87822723388672, minimum ratio: 0.76953125\n",
      "Epoch [782], val_loss: 686.9762\n",
      "gradient norm: 222.52209281921387, minimum ratio: 0.7578125\n",
      "Epoch [783], val_loss: 689.9930\n",
      "gradient norm: 223.1684741973877, minimum ratio: 0.765625\n",
      "Epoch [784], val_loss: 693.0198\n",
      "gradient norm: 223.81733894348145, minimum ratio: 0.7578125\n",
      "Epoch [785], val_loss: 696.0566\n",
      "gradient norm: 224.46654510498047, minimum ratio: 0.765625\n",
      "Epoch [786], val_loss: 699.1034\n",
      "gradient norm: 225.11706352233887, minimum ratio: 0.76171875\n",
      "Epoch [787], val_loss: 702.1602\n",
      "gradient norm: 225.76699256896973, minimum ratio: 0.7578125\n",
      "Epoch [788], val_loss: 705.2270\n",
      "gradient norm: 226.41657257080078, minimum ratio: 0.75390625\n",
      "Epoch [789], val_loss: 708.3038\n",
      "gradient norm: 227.07500648498535, minimum ratio: 0.78125\n",
      "Epoch [790], val_loss: 711.3908\n",
      "gradient norm: 227.724702835083, minimum ratio: 0.765625\n",
      "Epoch [791], val_loss: 714.4879\n",
      "gradient norm: 228.38405227661133, minimum ratio: 0.76953125\n",
      "Epoch [792], val_loss: 717.5950\n",
      "gradient norm: 229.03941535949707, minimum ratio: 0.79296875\n",
      "Epoch [793], val_loss: 720.7123\n",
      "gradient norm: 229.7028408050537, minimum ratio: 0.7578125\n",
      "Epoch [794], val_loss: 723.8397\n",
      "gradient norm: 230.36627769470215, minimum ratio: 0.7578125\n",
      "Epoch [795], val_loss: 726.9774\n",
      "gradient norm: 231.0244255065918, minimum ratio: 0.78515625\n",
      "Epoch [796], val_loss: 730.1252\n",
      "gradient norm: 231.6845474243164, minimum ratio: 0.7890625\n",
      "Epoch [797], val_loss: 733.2834\n",
      "gradient norm: 232.3515224456787, minimum ratio: 0.765625\n",
      "Epoch [798], val_loss: 736.4520\n",
      "gradient norm: 233.01478385925293, minimum ratio: 0.77734375\n",
      "Epoch [799], val_loss: 739.6307\n",
      "gradient norm: 233.68203163146973, minimum ratio: 0.7578125\n",
      "Epoch [800], val_loss: 742.8198\n",
      "gradient norm: 234.35345649719238, minimum ratio: 0.76171875\n",
      "Epoch [801], val_loss: 746.0194\n",
      "gradient norm: 235.0214786529541, minimum ratio: 0.77734375\n",
      "Epoch [802], val_loss: 749.2291\n",
      "gradient norm: 235.69007873535156, minimum ratio: 0.765625\n",
      "Epoch [803], val_loss: 752.4493\n",
      "gradient norm: 236.3570098876953, minimum ratio: 0.75\n",
      "Epoch [804], val_loss: 755.6798\n",
      "gradient norm: 237.03436851501465, minimum ratio: 0.7734375\n",
      "Epoch [805], val_loss: 758.9207\n",
      "gradient norm: 237.71148872375488, minimum ratio: 0.76953125\n",
      "Epoch [806], val_loss: 762.1721\n",
      "gradient norm: 238.38426971435547, minimum ratio: 0.76171875\n",
      "Epoch [807], val_loss: 765.4337\n",
      "gradient norm: 239.05909156799316, minimum ratio: 0.7734375\n",
      "Epoch [808], val_loss: 768.7061\n",
      "gradient norm: 239.73890686035156, minimum ratio: 0.77734375\n",
      "Epoch [809], val_loss: 771.9892\n",
      "gradient norm: 240.4150791168213, minimum ratio: 0.75390625\n",
      "Epoch [810], val_loss: 775.2827\n",
      "gradient norm: 241.09649276733398, minimum ratio: 0.74609375\n",
      "Epoch [811], val_loss: 778.5868\n",
      "gradient norm: 241.77899742126465, minimum ratio: 0.75390625\n",
      "Epoch [812], val_loss: 781.9014\n",
      "gradient norm: 242.4678020477295, minimum ratio: 0.7578125\n",
      "Epoch [813], val_loss: 785.2264\n",
      "gradient norm: 243.15731811523438, minimum ratio: 0.76953125\n",
      "Epoch [814], val_loss: 788.5622\n",
      "gradient norm: 243.84019088745117, minimum ratio: 0.76171875\n",
      "Epoch [815], val_loss: 791.9088\n",
      "gradient norm: 244.52678680419922, minimum ratio: 0.7578125\n",
      "Epoch [816], val_loss: 795.2659\n",
      "gradient norm: 245.2127456665039, minimum ratio: 0.76171875\n",
      "Epoch [817], val_loss: 798.6337\n",
      "gradient norm: 245.89757919311523, minimum ratio: 0.7578125\n",
      "Epoch [818], val_loss: 802.0124\n",
      "gradient norm: 246.58731269836426, minimum ratio: 0.74609375\n",
      "Epoch [819], val_loss: 805.4018\n",
      "gradient norm: 247.2816162109375, minimum ratio: 0.76171875\n",
      "Epoch [820], val_loss: 808.8020\n",
      "gradient norm: 247.96646308898926, minimum ratio: 0.78125\n",
      "Epoch [821], val_loss: 812.2126\n",
      "gradient norm: 248.66213607788086, minimum ratio: 0.73828125\n",
      "Epoch [822], val_loss: 815.6342\n",
      "gradient norm: 249.35950469970703, minimum ratio: 0.76953125\n",
      "Epoch [823], val_loss: 819.0666\n",
      "gradient norm: 250.06064414978027, minimum ratio: 0.75\n",
      "Epoch [824], val_loss: 822.5096\n",
      "gradient norm: 250.7611083984375, minimum ratio: 0.7421875\n",
      "Epoch [825], val_loss: 825.9636\n",
      "gradient norm: 251.46214866638184, minimum ratio: 0.75390625\n",
      "Epoch [826], val_loss: 829.4285\n",
      "gradient norm: 252.1625461578369, minimum ratio: 0.7421875\n",
      "Epoch [827], val_loss: 832.9042\n",
      "gradient norm: 252.86611938476562, minimum ratio: 0.76953125\n",
      "Epoch [828], val_loss: 836.3907\n",
      "gradient norm: 253.57407188415527, minimum ratio: 0.76171875\n",
      "Epoch [829], val_loss: 839.8882\n",
      "gradient norm: 254.278226852417, minimum ratio: 0.7734375\n",
      "Epoch [830], val_loss: 843.3964\n",
      "gradient norm: 254.97782135009766, minimum ratio: 0.75390625\n",
      "Epoch [831], val_loss: 846.9155\n",
      "gradient norm: 255.68466758728027, minimum ratio: 0.76171875\n",
      "Epoch [832], val_loss: 850.4456\n",
      "gradient norm: 256.39051818847656, minimum ratio: 0.7578125\n",
      "Epoch [833], val_loss: 853.9869\n",
      "gradient norm: 257.09543228149414, minimum ratio: 0.7421875\n",
      "Epoch [834], val_loss: 857.5389\n",
      "gradient norm: 257.8034496307373, minimum ratio: 0.76953125\n",
      "Epoch [835], val_loss: 861.1020\n",
      "gradient norm: 258.5137424468994, minimum ratio: 0.74609375\n",
      "Epoch [836], val_loss: 864.6760\n",
      "gradient norm: 259.22532081604004, minimum ratio: 0.77734375\n",
      "Epoch [837], val_loss: 868.2612\n",
      "gradient norm: 259.9466361999512, minimum ratio: 0.74609375\n",
      "Epoch [838], val_loss: 871.8575\n",
      "gradient norm: 260.66834449768066, minimum ratio: 0.78515625\n",
      "Epoch [839], val_loss: 875.4649\n",
      "gradient norm: 261.3855571746826, minimum ratio: 0.76171875\n",
      "Epoch [840], val_loss: 879.0836\n",
      "gradient norm: 262.1046142578125, minimum ratio: 0.7734375\n",
      "Epoch [841], val_loss: 882.7133\n",
      "gradient norm: 262.82494163513184, minimum ratio: 0.78515625\n",
      "Epoch [842], val_loss: 886.3539\n",
      "gradient norm: 263.5432643890381, minimum ratio: 0.77734375\n",
      "Epoch [843], val_loss: 890.0059\n",
      "gradient norm: 264.26733589172363, minimum ratio: 0.765625\n",
      "Epoch [844], val_loss: 893.6693\n",
      "gradient norm: 264.9888496398926, minimum ratio: 0.7734375\n",
      "Epoch [845], val_loss: 897.3439\n",
      "gradient norm: 265.71742820739746, minimum ratio: 0.76171875\n",
      "Epoch [846], val_loss: 901.0298\n",
      "gradient norm: 266.4279384613037, minimum ratio: 0.76953125\n",
      "Epoch [847], val_loss: 904.7272\n",
      "gradient norm: 267.16050720214844, minimum ratio: 0.7734375\n",
      "Epoch [848], val_loss: 908.4356\n",
      "gradient norm: 267.89269256591797, minimum ratio: 0.75390625\n",
      "Epoch [849], val_loss: 912.1552\n",
      "gradient norm: 268.6274299621582, minimum ratio: 0.74609375\n",
      "Epoch [850], val_loss: 915.8862\n",
      "gradient norm: 269.36461067199707, minimum ratio: 0.80078125\n",
      "Epoch [851], val_loss: 919.6290\n",
      "gradient norm: 270.09802055358887, minimum ratio: 0.7421875\n",
      "Epoch [852], val_loss: 923.3829\n",
      "gradient norm: 270.8290901184082, minimum ratio: 0.75\n",
      "Epoch [853], val_loss: 927.1483\n",
      "gradient norm: 271.5656337738037, minimum ratio: 0.78125\n",
      "Epoch [854], val_loss: 930.9250\n",
      "gradient norm: 272.2992115020752, minimum ratio: 0.76953125\n",
      "Epoch [855], val_loss: 934.7133\n",
      "gradient norm: 273.0392265319824, minimum ratio: 0.7578125\n",
      "Epoch [856], val_loss: 938.5130\n",
      "gradient norm: 273.7752742767334, minimum ratio: 0.76953125\n",
      "Epoch [857], val_loss: 942.3243\n",
      "gradient norm: 274.5110912322998, minimum ratio: 0.77734375\n",
      "Epoch [858], val_loss: 946.1475\n",
      "gradient norm: 275.25034523010254, minimum ratio: 0.7734375\n",
      "Epoch [859], val_loss: 949.9823\n",
      "gradient norm: 275.99643325805664, minimum ratio: 0.73828125\n",
      "Epoch [860], val_loss: 953.8284\n",
      "gradient norm: 276.7377338409424, minimum ratio: 0.80078125\n",
      "Epoch [861], val_loss: 957.6862\n",
      "gradient norm: 277.48395347595215, minimum ratio: 0.73828125\n",
      "Epoch [862], val_loss: 961.5554\n",
      "gradient norm: 278.23134422302246, minimum ratio: 0.7734375\n",
      "Epoch [863], val_loss: 965.4363\n",
      "gradient norm: 278.98570823669434, minimum ratio: 0.7421875\n",
      "Epoch [864], val_loss: 969.3290\n",
      "gradient norm: 279.7333755493164, minimum ratio: 0.74609375\n",
      "Epoch [865], val_loss: 973.2332\n",
      "gradient norm: 280.4849510192871, minimum ratio: 0.76171875\n",
      "Epoch [866], val_loss: 977.1492\n",
      "gradient norm: 281.2406425476074, minimum ratio: 0.74609375\n",
      "Epoch [867], val_loss: 981.0765\n",
      "gradient norm: 281.99514961242676, minimum ratio: 0.75\n",
      "Epoch [868], val_loss: 985.0162\n",
      "gradient norm: 282.74545097351074, minimum ratio: 0.765625\n",
      "Epoch [869], val_loss: 988.9672\n",
      "gradient norm: 283.50451278686523, minimum ratio: 0.7578125\n",
      "Epoch [870], val_loss: 992.9300\n",
      "gradient norm: 284.267578125, minimum ratio: 0.7578125\n",
      "Epoch [871], val_loss: 996.9046\n",
      "gradient norm: 285.0260257720947, minimum ratio: 0.765625\n",
      "Epoch [872], val_loss: 1000.8909\n",
      "gradient norm: 285.7902145385742, minimum ratio: 0.75\n",
      "Epoch [873], val_loss: 1004.8894\n",
      "gradient norm: 286.53192710876465, minimum ratio: 0.734375\n",
      "Epoch [874], val_loss: 1008.8995\n",
      "gradient norm: 287.29207611083984, minimum ratio: 0.74609375\n",
      "Epoch [875], val_loss: 1012.9219\n",
      "gradient norm: 288.0589199066162, minimum ratio: 0.77734375\n",
      "Epoch [876], val_loss: 1016.9559\n",
      "gradient norm: 288.8233108520508, minimum ratio: 0.75390625\n",
      "Epoch [877], val_loss: 1021.0021\n",
      "gradient norm: 289.59229850769043, minimum ratio: 0.765625\n",
      "Epoch [878], val_loss: 1025.0602\n",
      "gradient norm: 290.36114501953125, minimum ratio: 0.73828125\n",
      "Epoch [879], val_loss: 1029.1304\n",
      "gradient norm: 291.1253604888916, minimum ratio: 0.73828125\n",
      "Epoch [880], val_loss: 1033.2125\n",
      "gradient norm: 291.8956546783447, minimum ratio: 0.77734375\n",
      "Epoch [881], val_loss: 1037.3065\n",
      "gradient norm: 292.6682872772217, minimum ratio: 0.76953125\n",
      "Epoch [882], val_loss: 1041.4127\n",
      "gradient norm: 293.44787788391113, minimum ratio: 0.7734375\n",
      "Epoch [883], val_loss: 1045.5306\n",
      "gradient norm: 294.226261138916, minimum ratio: 0.7578125\n",
      "Epoch [884], val_loss: 1049.6608\n",
      "gradient norm: 295.00378036499023, minimum ratio: 0.7578125\n",
      "Epoch [885], val_loss: 1053.8029\n",
      "gradient norm: 295.7815341949463, minimum ratio: 0.76171875\n",
      "Epoch [886], val_loss: 1057.9570\n",
      "gradient norm: 296.56577491760254, minimum ratio: 0.7578125\n",
      "Epoch [887], val_loss: 1062.1232\n",
      "gradient norm: 297.33583068847656, minimum ratio: 0.75390625\n",
      "Epoch [888], val_loss: 1066.3016\n",
      "gradient norm: 298.11437225341797, minimum ratio: 0.75\n",
      "Epoch [889], val_loss: 1070.4926\n",
      "gradient norm: 298.8898468017578, minimum ratio: 0.7578125\n",
      "Epoch [890], val_loss: 1074.6956\n",
      "gradient norm: 299.66894149780273, minimum ratio: 0.765625\n",
      "Epoch [891], val_loss: 1078.9109\n",
      "gradient norm: 300.4530944824219, minimum ratio: 0.75390625\n",
      "Epoch [892], val_loss: 1083.1385\n",
      "gradient norm: 301.23950576782227, minimum ratio: 0.76171875\n",
      "Epoch [893], val_loss: 1087.3783\n",
      "gradient norm: 302.0262756347656, minimum ratio: 0.78125\n",
      "Epoch [894], val_loss: 1091.6305\n",
      "gradient norm: 302.813928604126, minimum ratio: 0.765625\n",
      "Epoch [895], val_loss: 1095.8948\n",
      "gradient norm: 303.6106262207031, minimum ratio: 0.7421875\n",
      "Epoch [896], val_loss: 1100.1719\n",
      "gradient norm: 304.4067077636719, minimum ratio: 0.7421875\n",
      "Epoch [897], val_loss: 1104.4611\n",
      "gradient norm: 305.2015151977539, minimum ratio: 0.78125\n",
      "Epoch [898], val_loss: 1108.7629\n",
      "gradient norm: 305.9933032989502, minimum ratio: 0.75390625\n",
      "Epoch [899], val_loss: 1113.0773\n",
      "gradient norm: 306.7838764190674, minimum ratio: 0.75390625\n",
      "Epoch [900], val_loss: 1117.4041\n",
      "gradient norm: 307.5868682861328, minimum ratio: 0.7578125\n",
      "Epoch [901], val_loss: 1121.7433\n",
      "gradient norm: 308.3759517669678, minimum ratio: 0.76953125\n",
      "Epoch [902], val_loss: 1126.0948\n",
      "gradient norm: 309.1775550842285, minimum ratio: 0.76171875\n",
      "Epoch [903], val_loss: 1130.4590\n",
      "gradient norm: 309.98422622680664, minimum ratio: 0.76953125\n",
      "Epoch [904], val_loss: 1134.8356\n",
      "gradient norm: 310.7842960357666, minimum ratio: 0.78125\n",
      "Epoch [905], val_loss: 1139.2247\n",
      "gradient norm: 311.59137535095215, minimum ratio: 0.765625\n",
      "Epoch [906], val_loss: 1143.6262\n",
      "gradient norm: 312.3973445892334, minimum ratio: 0.7578125\n",
      "Epoch [907], val_loss: 1148.0404\n",
      "gradient norm: 313.20823097229004, minimum ratio: 0.765625\n",
      "Epoch [908], val_loss: 1152.4672\n",
      "gradient norm: 314.0129871368408, minimum ratio: 0.78125\n",
      "Epoch [909], val_loss: 1156.9065\n",
      "gradient norm: 314.8171501159668, minimum ratio: 0.765625\n",
      "Epoch [910], val_loss: 1161.3585\n",
      "gradient norm: 315.63027000427246, minimum ratio: 0.765625\n",
      "Epoch [911], val_loss: 1165.8232\n",
      "gradient norm: 316.4431381225586, minimum ratio: 0.76171875\n",
      "Epoch [912], val_loss: 1170.3007\n",
      "gradient norm: 317.25100898742676, minimum ratio: 0.75\n",
      "Epoch [913], val_loss: 1174.7908\n",
      "gradient norm: 318.06618309020996, minimum ratio: 0.78515625\n",
      "Epoch [914], val_loss: 1179.2937\n",
      "gradient norm: 318.8810062408447, minimum ratio: 0.76953125\n",
      "Epoch [915], val_loss: 1183.8097\n",
      "gradient norm: 319.69456481933594, minimum ratio: 0.7578125\n",
      "Epoch [916], val_loss: 1188.3381\n",
      "gradient norm: 320.5044231414795, minimum ratio: 0.7578125\n",
      "Epoch [917], val_loss: 1192.8798\n",
      "gradient norm: 321.3220844268799, minimum ratio: 0.7578125\n",
      "Epoch [918], val_loss: 1197.4336\n",
      "gradient norm: 322.1464557647705, minimum ratio: 0.75\n",
      "Epoch [919], val_loss: 1202.0005\n",
      "gradient norm: 322.9686279296875, minimum ratio: 0.75390625\n",
      "Epoch [920], val_loss: 1206.5804\n",
      "gradient norm: 323.7877445220947, minimum ratio: 0.765625\n",
      "Epoch [921], val_loss: 1211.1731\n",
      "gradient norm: 324.6091251373291, minimum ratio: 0.76171875\n",
      "Epoch [922], val_loss: 1215.7794\n",
      "gradient norm: 325.43972969055176, minimum ratio: 0.76953125\n",
      "Epoch [923], val_loss: 1220.3987\n",
      "gradient norm: 326.26635360717773, minimum ratio: 0.77734375\n",
      "Epoch [924], val_loss: 1225.0303\n",
      "gradient norm: 327.0952682495117, minimum ratio: 0.765625\n",
      "Epoch [925], val_loss: 1229.6752\n",
      "gradient norm: 327.92623710632324, minimum ratio: 0.765625\n",
      "Epoch [926], val_loss: 1234.3324\n",
      "gradient norm: 328.75829124450684, minimum ratio: 0.77734375\n",
      "Epoch [927], val_loss: 1239.0028\n",
      "gradient norm: 329.5899257659912, minimum ratio: 0.77734375\n",
      "Epoch [928], val_loss: 1243.6864\n",
      "gradient norm: 330.43069648742676, minimum ratio: 0.765625\n",
      "Epoch [929], val_loss: 1248.3828\n",
      "gradient norm: 331.2692623138428, minimum ratio: 0.76953125\n",
      "Epoch [930], val_loss: 1253.0922\n",
      "gradient norm: 332.11121940612793, minimum ratio: 0.76953125\n",
      "Epoch [931], val_loss: 1257.8143\n",
      "gradient norm: 332.94214820861816, minimum ratio: 0.78125\n",
      "Epoch [932], val_loss: 1262.5497\n",
      "gradient norm: 333.7831344604492, minimum ratio: 0.7421875\n",
      "Epoch [933], val_loss: 1267.2982\n",
      "gradient norm: 334.61136627197266, minimum ratio: 0.75390625\n",
      "Epoch [934], val_loss: 1272.0604\n",
      "gradient norm: 335.4591751098633, minimum ratio: 0.734375\n",
      "Epoch [935], val_loss: 1276.8359\n",
      "gradient norm: 336.2994441986084, minimum ratio: 0.765625\n",
      "Epoch [936], val_loss: 1281.6243\n",
      "gradient norm: 337.1476879119873, minimum ratio: 0.74609375\n",
      "Epoch [937], val_loss: 1286.4258\n",
      "gradient norm: 337.99478340148926, minimum ratio: 0.765625\n",
      "Epoch [938], val_loss: 1291.2406\n",
      "gradient norm: 338.84391593933105, minimum ratio: 0.7890625\n",
      "Epoch [939], val_loss: 1296.0688\n",
      "gradient norm: 339.69272232055664, minimum ratio: 0.7734375\n",
      "Epoch [940], val_loss: 1300.9102\n",
      "gradient norm: 340.54455947875977, minimum ratio: 0.765625\n",
      "Epoch [941], val_loss: 1305.7651\n",
      "gradient norm: 341.394495010376, minimum ratio: 0.76171875\n",
      "Epoch [942], val_loss: 1310.6335\n",
      "gradient norm: 342.2481803894043, minimum ratio: 0.76953125\n",
      "Epoch [943], val_loss: 1315.5148\n",
      "gradient norm: 343.0970802307129, minimum ratio: 0.7578125\n",
      "Epoch [944], val_loss: 1320.4097\n",
      "gradient norm: 343.9504585266113, minimum ratio: 0.765625\n",
      "Epoch [945], val_loss: 1325.3179\n",
      "gradient norm: 344.8147602081299, minimum ratio: 0.76953125\n",
      "Epoch [946], val_loss: 1330.2397\n",
      "gradient norm: 345.6576385498047, minimum ratio: 0.76171875\n",
      "Epoch [947], val_loss: 1335.1749\n",
      "gradient norm: 346.51891899108887, minimum ratio: 0.76171875\n",
      "Epoch [948], val_loss: 1340.1237\n",
      "gradient norm: 347.37560844421387, minimum ratio: 0.76171875\n",
      "Epoch [949], val_loss: 1345.0858\n",
      "gradient norm: 348.2386703491211, minimum ratio: 0.74609375\n",
      "Epoch [950], val_loss: 1350.0618\n",
      "gradient norm: 349.1059875488281, minimum ratio: 0.75390625\n",
      "Epoch [951], val_loss: 1355.0510\n",
      "gradient norm: 349.97721672058105, minimum ratio: 0.76171875\n",
      "Epoch [952], val_loss: 1360.0543\n",
      "gradient norm: 350.84457206726074, minimum ratio: 0.75\n",
      "Epoch [953], val_loss: 1365.0712\n",
      "gradient norm: 351.7088928222656, minimum ratio: 0.75390625\n",
      "Epoch [954], val_loss: 1370.1013\n",
      "gradient norm: 352.5773754119873, minimum ratio: 0.7734375\n",
      "Epoch [955], val_loss: 1375.1454\n",
      "gradient norm: 353.4434585571289, minimum ratio: 0.765625\n",
      "Epoch [956], val_loss: 1380.2028\n",
      "gradient norm: 354.3186740875244, minimum ratio: 0.75\n",
      "Epoch [957], val_loss: 1385.2739\n",
      "gradient norm: 355.1893539428711, minimum ratio: 0.75390625\n",
      "Epoch [958], val_loss: 1390.3588\n",
      "gradient norm: 356.06624031066895, minimum ratio: 0.765625\n",
      "Epoch [959], val_loss: 1395.4572\n",
      "gradient norm: 356.94362449645996, minimum ratio: 0.7734375\n",
      "Epoch [960], val_loss: 1400.5697\n",
      "gradient norm: 357.82280349731445, minimum ratio: 0.73046875\n",
      "Epoch [961], val_loss: 1405.6954\n",
      "gradient norm: 358.69776916503906, minimum ratio: 0.75390625\n",
      "Epoch [962], val_loss: 1410.8356\n",
      "gradient norm: 359.5810203552246, minimum ratio: 0.76171875\n",
      "Epoch [963], val_loss: 1415.9896\n",
      "gradient norm: 360.45973205566406, minimum ratio: 0.75390625\n",
      "Epoch [964], val_loss: 1421.1573\n",
      "gradient norm: 361.344913482666, minimum ratio: 0.76171875\n",
      "Epoch [965], val_loss: 1426.3390\n",
      "gradient norm: 362.21774101257324, minimum ratio: 0.76953125\n",
      "Epoch [966], val_loss: 1431.5349\n",
      "gradient norm: 363.09547996520996, minimum ratio: 0.78125\n",
      "Epoch [967], val_loss: 1436.7450\n",
      "gradient norm: 363.98557472229004, minimum ratio: 0.75390625\n",
      "Epoch [968], val_loss: 1441.9686\n",
      "gradient norm: 364.87141036987305, minimum ratio: 0.75390625\n",
      "Epoch [969], val_loss: 1447.2063\n",
      "gradient norm: 365.76588439941406, minimum ratio: 0.74609375\n",
      "Epoch [970], val_loss: 1452.4584\n",
      "gradient norm: 366.65528869628906, minimum ratio: 0.76171875\n",
      "Epoch [971], val_loss: 1457.7242\n",
      "gradient norm: 367.5454502105713, minimum ratio: 0.77734375\n",
      "Epoch [972], val_loss: 1463.0042\n",
      "gradient norm: 368.4431037902832, minimum ratio: 0.7578125\n",
      "Epoch [973], val_loss: 1468.2982\n",
      "gradient norm: 369.3418769836426, minimum ratio: 0.7421875\n",
      "Epoch [974], val_loss: 1473.6066\n",
      "gradient norm: 370.2367477416992, minimum ratio: 0.74609375\n",
      "Epoch [975], val_loss: 1478.9286\n",
      "gradient norm: 371.12835121154785, minimum ratio: 0.765625\n",
      "Epoch [976], val_loss: 1484.2653\n",
      "gradient norm: 372.0234889984131, minimum ratio: 0.75\n",
      "Epoch [977], val_loss: 1489.6161\n",
      "gradient norm: 372.9272861480713, minimum ratio: 0.76171875\n",
      "Epoch [978], val_loss: 1494.9808\n",
      "gradient norm: 373.82835388183594, minimum ratio: 0.7421875\n",
      "Epoch [979], val_loss: 1500.3597\n",
      "gradient norm: 374.7385482788086, minimum ratio: 0.7734375\n",
      "Epoch [980], val_loss: 1505.7532\n",
      "gradient norm: 375.6396903991699, minimum ratio: 0.78125\n",
      "Epoch [981], val_loss: 1511.1606\n",
      "gradient norm: 376.5537967681885, minimum ratio: 0.75390625\n",
      "Epoch [982], val_loss: 1516.5824\n",
      "gradient norm: 377.46872329711914, minimum ratio: 0.7734375\n",
      "Epoch [983], val_loss: 1522.0181\n",
      "gradient norm: 378.3842086791992, minimum ratio: 0.75\n",
      "Epoch [984], val_loss: 1527.4681\n",
      "gradient norm: 379.2972717285156, minimum ratio: 0.765625\n",
      "Epoch [985], val_loss: 1532.9327\n",
      "gradient norm: 380.20423889160156, minimum ratio: 0.75390625\n",
      "Epoch [986], val_loss: 1538.4120\n",
      "gradient norm: 381.10893630981445, minimum ratio: 0.7578125\n",
      "Epoch [987], val_loss: 1543.9054\n",
      "gradient norm: 382.0242233276367, minimum ratio: 0.7734375\n",
      "Epoch [988], val_loss: 1549.4132\n",
      "gradient norm: 382.9353332519531, minimum ratio: 0.76171875\n",
      "Epoch [989], val_loss: 1554.9358\n",
      "gradient norm: 383.8512496948242, minimum ratio: 0.75\n",
      "Epoch [990], val_loss: 1560.4725\n",
      "gradient norm: 384.7624740600586, minimum ratio: 0.75390625\n",
      "Epoch [991], val_loss: 1566.0238\n",
      "gradient norm: 385.684757232666, minimum ratio: 0.76953125\n",
      "Epoch [992], val_loss: 1571.5898\n",
      "gradient norm: 386.60255432128906, minimum ratio: 0.76953125\n",
      "Epoch [993], val_loss: 1577.1702\n",
      "gradient norm: 387.50822830200195, minimum ratio: 0.75390625\n",
      "Epoch [994], val_loss: 1582.7654\n",
      "gradient norm: 388.4356880187988, minimum ratio: 0.7734375\n",
      "Epoch [995], val_loss: 1588.3754\n",
      "gradient norm: 389.3681640625, minimum ratio: 0.765625\n",
      "Epoch [996], val_loss: 1594.0000\n",
      "gradient norm: 390.2944984436035, minimum ratio: 0.7578125\n",
      "Epoch [997], val_loss: 1599.6388\n",
      "gradient norm: 391.22535705566406, minimum ratio: 0.7578125\n",
      "Epoch [998], val_loss: 1605.2922\n",
      "gradient norm: 392.16272735595703, minimum ratio: 0.78125\n",
      "Epoch [999], val_loss: 1610.9606\n",
      "gradient norm: 393.0940818786621, minimum ratio: 0.7578125\n",
      "Epoch [1000], val_loss: 1616.6436\n",
      "gradient norm: 394.02979278564453, minimum ratio: 0.7578125\n",
      "Epoch [1001], val_loss: 1622.3413\n",
      "gradient norm: 394.9657402038574, minimum ratio: 0.765625\n",
      "Epoch [1002], val_loss: 1628.0533\n",
      "gradient norm: 395.9004364013672, minimum ratio: 0.7578125\n",
      "Epoch [1003], val_loss: 1633.7803\n",
      "gradient norm: 396.84431076049805, minimum ratio: 0.765625\n",
      "Epoch [1004], val_loss: 1639.5219\n",
      "gradient norm: 397.7827377319336, minimum ratio: 0.7734375\n",
      "Epoch [1005], val_loss: 1645.2786\n",
      "gradient norm: 398.72266387939453, minimum ratio: 0.75\n",
      "Epoch [1006], val_loss: 1651.0503\n",
      "gradient norm: 399.6634712219238, minimum ratio: 0.75\n",
      "Epoch [1007], val_loss: 1656.8374\n",
      "gradient norm: 400.6019821166992, minimum ratio: 0.765625\n",
      "Epoch [1008], val_loss: 1662.6392\n",
      "gradient norm: 401.53148651123047, minimum ratio: 0.75390625\n",
      "Epoch [1009], val_loss: 1668.4563\n",
      "gradient norm: 402.4792251586914, minimum ratio: 0.75390625\n",
      "Epoch [1010], val_loss: 1674.2881\n",
      "gradient norm: 403.42223739624023, minimum ratio: 0.76171875\n",
      "Epoch [1011], val_loss: 1680.1346\n",
      "gradient norm: 404.3638038635254, minimum ratio: 0.76953125\n",
      "Epoch [1012], val_loss: 1685.9962\n",
      "gradient norm: 405.31141662597656, minimum ratio: 0.765625\n",
      "Epoch [1013], val_loss: 1691.8732\n",
      "gradient norm: 406.26639556884766, minimum ratio: 0.75\n",
      "Epoch [1014], val_loss: 1697.7650\n",
      "gradient norm: 407.21489334106445, minimum ratio: 0.74609375\n",
      "Epoch [1015], val_loss: 1703.6721\n",
      "gradient norm: 408.1712226867676, minimum ratio: 0.75390625\n",
      "Epoch [1016], val_loss: 1709.5941\n",
      "gradient norm: 409.135196685791, minimum ratio: 0.79296875\n",
      "Epoch [1017], val_loss: 1715.5310\n",
      "gradient norm: 410.0931282043457, minimum ratio: 0.765625\n",
      "Epoch [1018], val_loss: 1721.4832\n",
      "gradient norm: 411.0429878234863, minimum ratio: 0.765625\n",
      "Epoch [1019], val_loss: 1727.4507\n",
      "gradient norm: 412.0113067626953, minimum ratio: 0.7421875\n",
      "Epoch [1020], val_loss: 1733.4333\n",
      "gradient norm: 412.97107315063477, minimum ratio: 0.765625\n",
      "Epoch [1021], val_loss: 1739.4308\n",
      "gradient norm: 413.9421920776367, minimum ratio: 0.76953125\n",
      "Epoch [1022], val_loss: 1745.4437\n",
      "gradient norm: 414.903377532959, minimum ratio: 0.75390625\n",
      "Epoch [1023], val_loss: 1751.4719\n",
      "gradient norm: 415.86527252197266, minimum ratio: 0.76171875\n",
      "Epoch [1024], val_loss: 1757.5153\n",
      "gradient norm: 416.83517837524414, minimum ratio: 0.77734375\n",
      "Epoch [1025], val_loss: 1763.5746\n",
      "gradient norm: 417.7953300476074, minimum ratio: 0.75390625\n",
      "Epoch [1026], val_loss: 1769.6489\n",
      "gradient norm: 418.7535095214844, minimum ratio: 0.76171875\n",
      "Epoch [1027], val_loss: 1775.7388\n",
      "gradient norm: 419.7276611328125, minimum ratio: 0.76953125\n",
      "Epoch [1028], val_loss: 1781.8438\n",
      "gradient norm: 420.7036590576172, minimum ratio: 0.75\n",
      "Epoch [1029], val_loss: 1787.9641\n",
      "gradient norm: 421.67413330078125, minimum ratio: 0.76171875\n",
      "Epoch [1030], val_loss: 1794.1000\n",
      "gradient norm: 422.64570236206055, minimum ratio: 0.76953125\n",
      "Epoch [1031], val_loss: 1800.2511\n",
      "gradient norm: 423.62329483032227, minimum ratio: 0.76953125\n",
      "Epoch [1032], val_loss: 1806.4171\n",
      "gradient norm: 424.59928131103516, minimum ratio: 0.75390625\n",
      "Epoch [1033], val_loss: 1812.5988\n",
      "gradient norm: 425.58740997314453, minimum ratio: 0.76171875\n",
      "Epoch [1034], val_loss: 1818.7960\n",
      "gradient norm: 426.56848907470703, minimum ratio: 0.78125\n",
      "Epoch [1035], val_loss: 1825.0085\n",
      "gradient norm: 427.5470161437988, minimum ratio: 0.7578125\n",
      "Epoch [1036], val_loss: 1831.2368\n",
      "gradient norm: 428.53514862060547, minimum ratio: 0.7578125\n",
      "Epoch [1037], val_loss: 1837.4808\n",
      "gradient norm: 429.5189399719238, minimum ratio: 0.76953125\n",
      "Epoch [1038], val_loss: 1843.7402\n",
      "gradient norm: 430.5112953186035, minimum ratio: 0.74609375\n",
      "Epoch [1039], val_loss: 1850.0150\n",
      "gradient norm: 431.50562286376953, minimum ratio: 0.75\n",
      "Epoch [1040], val_loss: 1856.3057\n",
      "gradient norm: 432.50403213500977, minimum ratio: 0.76953125\n",
      "Epoch [1041], val_loss: 1862.6121\n",
      "gradient norm: 433.48333740234375, minimum ratio: 0.76171875\n",
      "Epoch [1042], val_loss: 1868.9337\n",
      "gradient norm: 434.47853088378906, minimum ratio: 0.76171875\n",
      "Epoch [1043], val_loss: 1875.2719\n",
      "gradient norm: 435.46305084228516, minimum ratio: 0.75390625\n",
      "Epoch [1044], val_loss: 1881.6254\n",
      "gradient norm: 436.4515609741211, minimum ratio: 0.75390625\n",
      "Epoch [1045], val_loss: 1887.9952\n",
      "gradient norm: 437.44744873046875, minimum ratio: 0.7578125\n",
      "Epoch [1046], val_loss: 1894.3806\n",
      "gradient norm: 438.45434188842773, minimum ratio: 0.7421875\n",
      "Epoch [1047], val_loss: 1900.7819\n",
      "gradient norm: 439.4514808654785, minimum ratio: 0.765625\n",
      "Epoch [1048], val_loss: 1907.1990\n",
      "gradient norm: 440.42763900756836, minimum ratio: 0.75\n",
      "Epoch [1049], val_loss: 1913.6324\n",
      "gradient norm: 441.43942642211914, minimum ratio: 0.75390625\n",
      "Epoch [1050], val_loss: 1920.0819\n",
      "gradient norm: 442.4433479309082, minimum ratio: 0.75390625\n",
      "Epoch [1051], val_loss: 1926.5471\n",
      "gradient norm: 443.45165252685547, minimum ratio: 0.73046875\n",
      "Epoch [1052], val_loss: 1933.0283\n",
      "gradient norm: 444.4530220031738, minimum ratio: 0.7578125\n",
      "Epoch [1053], val_loss: 1939.5259\n",
      "gradient norm: 445.46441650390625, minimum ratio: 0.75390625\n",
      "Epoch [1054], val_loss: 1946.0392\n",
      "gradient norm: 446.4787063598633, minimum ratio: 0.75390625\n",
      "Epoch [1055], val_loss: 1952.5690\n",
      "gradient norm: 447.48902130126953, minimum ratio: 0.7578125\n",
      "Epoch [1056], val_loss: 1959.1144\n",
      "gradient norm: 448.5038757324219, minimum ratio: 0.765625\n",
      "Epoch [1057], val_loss: 1965.6760\n",
      "gradient norm: 449.5276527404785, minimum ratio: 0.74609375\n",
      "Epoch [1058], val_loss: 1972.2538\n",
      "gradient norm: 450.5367088317871, minimum ratio: 0.73828125\n",
      "Epoch [1059], val_loss: 1978.8470\n",
      "gradient norm: 451.5633964538574, minimum ratio: 0.7578125\n",
      "Epoch [1060], val_loss: 1985.4569\n",
      "gradient norm: 452.5736274719238, minimum ratio: 0.7578125\n",
      "Epoch [1061], val_loss: 1992.0820\n",
      "gradient norm: 453.59431076049805, minimum ratio: 0.76953125\n",
      "Epoch [1062], val_loss: 1998.7235\n",
      "gradient norm: 454.6180305480957, minimum ratio: 0.75\n",
      "Epoch [1063], val_loss: 2005.3810\n",
      "gradient norm: 455.6427230834961, minimum ratio: 0.75390625\n",
      "Epoch [1064], val_loss: 2012.0551\n",
      "gradient norm: 456.66716384887695, minimum ratio: 0.75390625\n",
      "Epoch [1065], val_loss: 2018.7448\n",
      "gradient norm: 457.68775939941406, minimum ratio: 0.765625\n",
      "Epoch [1066], val_loss: 2025.4508\n",
      "gradient norm: 458.7113800048828, minimum ratio: 0.75390625\n",
      "Epoch [1067], val_loss: 2032.1737\n",
      "gradient norm: 459.7340545654297, minimum ratio: 0.76171875\n",
      "Epoch [1068], val_loss: 2038.9122\n",
      "gradient norm: 460.75958251953125, minimum ratio: 0.7578125\n",
      "Epoch [1069], val_loss: 2045.6677\n",
      "gradient norm: 461.7893371582031, minimum ratio: 0.77734375\n",
      "Epoch [1070], val_loss: 2052.4392\n",
      "gradient norm: 462.8172950744629, minimum ratio: 0.7578125\n",
      "Epoch [1071], val_loss: 2059.2273\n",
      "gradient norm: 463.8543128967285, minimum ratio: 0.76953125\n",
      "Epoch [1072], val_loss: 2066.0320\n",
      "gradient norm: 464.8854522705078, minimum ratio: 0.7578125\n",
      "Epoch [1073], val_loss: 2072.8533\n",
      "gradient norm: 465.92736434936523, minimum ratio: 0.75\n",
      "Epoch [1074], val_loss: 2079.6909\n",
      "gradient norm: 466.9696960449219, minimum ratio: 0.7421875\n",
      "Epoch [1075], val_loss: 2086.5444\n",
      "gradient norm: 468.01958084106445, minimum ratio: 0.73828125\n",
      "Epoch [1076], val_loss: 2093.4146\n",
      "gradient norm: 469.0545768737793, minimum ratio: 0.75390625\n",
      "Epoch [1077], val_loss: 2100.3015\n",
      "gradient norm: 470.09322357177734, minimum ratio: 0.76953125\n",
      "Epoch [1078], val_loss: 2107.2051\n",
      "gradient norm: 471.14452362060547, minimum ratio: 0.7734375\n",
      "Epoch [1079], val_loss: 2114.1248\n",
      "gradient norm: 472.17900466918945, minimum ratio: 0.76171875\n",
      "Epoch [1080], val_loss: 2121.0613\n",
      "gradient norm: 473.23040771484375, minimum ratio: 0.7578125\n",
      "Epoch [1081], val_loss: 2128.0142\n",
      "gradient norm: 474.2889862060547, minimum ratio: 0.7578125\n",
      "Epoch [1082], val_loss: 2134.9834\n",
      "gradient norm: 475.3491554260254, minimum ratio: 0.74609375\n",
      "Epoch [1083], val_loss: 2141.9695\n",
      "gradient norm: 476.38479232788086, minimum ratio: 0.74609375\n",
      "Epoch [1084], val_loss: 2148.9727\n",
      "gradient norm: 477.43187713623047, minimum ratio: 0.765625\n",
      "Epoch [1085], val_loss: 2155.9924\n",
      "gradient norm: 478.4900894165039, minimum ratio: 0.76171875\n",
      "Epoch [1086], val_loss: 2163.0291\n",
      "gradient norm: 479.54089736938477, minimum ratio: 0.78125\n",
      "Epoch [1087], val_loss: 2170.0830\n",
      "gradient norm: 480.5933609008789, minimum ratio: 0.75390625\n",
      "Epoch [1088], val_loss: 2177.1533\n",
      "gradient norm: 481.6561164855957, minimum ratio: 0.7421875\n",
      "Epoch [1089], val_loss: 2184.2402\n",
      "gradient norm: 482.72156143188477, minimum ratio: 0.7578125\n",
      "Epoch [1090], val_loss: 2191.3440\n",
      "gradient norm: 483.777099609375, minimum ratio: 0.7890625\n",
      "Epoch [1091], val_loss: 2198.4646\n",
      "gradient norm: 484.8504829406738, minimum ratio: 0.75390625\n",
      "Epoch [1092], val_loss: 2205.6016\n",
      "gradient norm: 485.9205017089844, minimum ratio: 0.73828125\n",
      "Epoch [1093], val_loss: 2212.7559\n",
      "gradient norm: 486.98512268066406, minimum ratio: 0.765625\n",
      "Epoch [1094], val_loss: 2219.9268\n",
      "gradient norm: 488.04821395874023, minimum ratio: 0.73046875\n",
      "Epoch [1095], val_loss: 2227.1152\n",
      "gradient norm: 489.1202812194824, minimum ratio: 0.75\n",
      "Epoch [1096], val_loss: 2234.3198\n",
      "gradient norm: 490.19152450561523, minimum ratio: 0.76953125\n",
      "Epoch [1097], val_loss: 2241.5417\n",
      "gradient norm: 491.2710380554199, minimum ratio: 0.7578125\n",
      "Epoch [1098], val_loss: 2248.7805\n",
      "gradient norm: 492.34033203125, minimum ratio: 0.76953125\n",
      "Epoch [1099], val_loss: 2256.0369\n",
      "gradient norm: 493.40513610839844, minimum ratio: 0.75\n",
      "Epoch [1100], val_loss: 2263.3103\n",
      "gradient norm: 494.4902992248535, minimum ratio: 0.73828125\n",
      "Epoch [1101], val_loss: 2270.6003\n",
      "gradient norm: 495.5751724243164, minimum ratio: 0.765625\n",
      "Epoch [1102], val_loss: 2277.9077\n",
      "gradient norm: 496.65538024902344, minimum ratio: 0.7734375\n",
      "Epoch [1103], val_loss: 2285.2319\n",
      "gradient norm: 497.72674560546875, minimum ratio: 0.77734375\n",
      "Epoch [1104], val_loss: 2292.5740\n",
      "gradient norm: 498.81101989746094, minimum ratio: 0.74609375\n",
      "Epoch [1105], val_loss: 2299.9333\n",
      "gradient norm: 499.89855194091797, minimum ratio: 0.7578125\n",
      "Epoch [1106], val_loss: 2307.3096\n",
      "gradient norm: 500.9589042663574, minimum ratio: 0.75\n",
      "Epoch [1107], val_loss: 2314.7034\n",
      "gradient norm: 502.0443000793457, minimum ratio: 0.75\n",
      "Epoch [1108], val_loss: 2322.1143\n",
      "gradient norm: 503.1378402709961, minimum ratio: 0.76171875\n",
      "Epoch [1109], val_loss: 2329.5422\n",
      "gradient norm: 504.2344512939453, minimum ratio: 0.75\n",
      "Epoch [1110], val_loss: 2336.9873\n",
      "gradient norm: 505.32727432250977, minimum ratio: 0.75\n",
      "Epoch [1111], val_loss: 2344.4497\n",
      "gradient norm: 506.4196968078613, minimum ratio: 0.75390625\n",
      "Epoch [1112], val_loss: 2351.9307\n",
      "gradient norm: 507.4893684387207, minimum ratio: 0.765625\n",
      "Epoch [1113], val_loss: 2359.4285\n",
      "gradient norm: 508.58252716064453, minimum ratio: 0.7578125\n",
      "Epoch [1114], val_loss: 2366.9438\n",
      "gradient norm: 509.69026947021484, minimum ratio: 0.76171875\n",
      "Epoch [1115], val_loss: 2374.4766\n",
      "gradient norm: 510.78967666625977, minimum ratio: 0.76953125\n",
      "Epoch [1116], val_loss: 2382.0273\n",
      "gradient norm: 511.88038635253906, minimum ratio: 0.765625\n",
      "Epoch [1117], val_loss: 2389.5955\n",
      "gradient norm: 512.9908866882324, minimum ratio: 0.7578125\n",
      "Epoch [1118], val_loss: 2397.1807\n",
      "gradient norm: 514.0901222229004, minimum ratio: 0.76171875\n",
      "Epoch [1119], val_loss: 2404.7834\n",
      "gradient norm: 515.1968307495117, minimum ratio: 0.734375\n",
      "Epoch [1120], val_loss: 2412.4045\n",
      "gradient norm: 516.3060722351074, minimum ratio: 0.75\n",
      "Epoch [1121], val_loss: 2420.0425\n",
      "gradient norm: 517.419979095459, minimum ratio: 0.74609375\n",
      "Epoch [1122], val_loss: 2427.6982\n",
      "gradient norm: 518.5233001708984, minimum ratio: 0.734375\n",
      "Epoch [1123], val_loss: 2435.3718\n",
      "gradient norm: 519.6388053894043, minimum ratio: 0.765625\n",
      "Epoch [1124], val_loss: 2443.0630\n",
      "gradient norm: 520.7601165771484, minimum ratio: 0.73828125\n",
      "Epoch [1125], val_loss: 2450.7705\n",
      "gradient norm: 521.8634262084961, minimum ratio: 0.75390625\n",
      "Epoch [1126], val_loss: 2458.4968\n",
      "gradient norm: 522.979866027832, minimum ratio: 0.75\n",
      "Epoch [1127], val_loss: 2466.2410\n",
      "gradient norm: 524.0880241394043, minimum ratio: 0.76171875\n",
      "Epoch [1128], val_loss: 2474.0029\n",
      "gradient norm: 525.2140159606934, minimum ratio: 0.74609375\n",
      "Epoch [1129], val_loss: 2481.7827\n",
      "gradient norm: 526.3257713317871, minimum ratio: 0.75390625\n",
      "Epoch [1130], val_loss: 2489.5801\n",
      "gradient norm: 527.4457168579102, minimum ratio: 0.74609375\n",
      "Epoch [1131], val_loss: 2497.3953\n",
      "gradient norm: 528.5702476501465, minimum ratio: 0.75\n",
      "Epoch [1132], val_loss: 2505.2275\n",
      "gradient norm: 529.7049751281738, minimum ratio: 0.73828125\n",
      "Epoch [1133], val_loss: 2513.0779\n",
      "gradient norm: 530.8393020629883, minimum ratio: 0.75390625\n",
      "Epoch [1134], val_loss: 2520.9465\n",
      "gradient norm: 531.9770812988281, minimum ratio: 0.76953125\n",
      "Epoch [1135], val_loss: 2528.8323\n",
      "gradient norm: 533.084415435791, minimum ratio: 0.7578125\n",
      "Epoch [1136], val_loss: 2536.7368\n",
      "gradient norm: 534.2165412902832, minimum ratio: 0.76953125\n",
      "Epoch [1137], val_loss: 2544.6592\n",
      "gradient norm: 535.3496170043945, minimum ratio: 0.75390625\n",
      "Epoch [1138], val_loss: 2552.5996\n",
      "gradient norm: 536.4784049987793, minimum ratio: 0.765625\n",
      "Epoch [1139], val_loss: 2560.5581\n",
      "gradient norm: 537.5996398925781, minimum ratio: 0.74609375\n",
      "Epoch [1140], val_loss: 2568.5349\n",
      "gradient norm: 538.7334823608398, minimum ratio: 0.76171875\n",
      "Epoch [1141], val_loss: 2576.5298\n",
      "gradient norm: 539.8706970214844, minimum ratio: 0.7578125\n",
      "Epoch [1142], val_loss: 2584.5420\n",
      "gradient norm: 541.0204887390137, minimum ratio: 0.74609375\n",
      "Epoch [1143], val_loss: 2592.5732\n",
      "gradient norm: 542.1718444824219, minimum ratio: 0.76171875\n",
      "Epoch [1144], val_loss: 2600.6216\n",
      "gradient norm: 543.3187255859375, minimum ratio: 0.74609375\n",
      "Epoch [1145], val_loss: 2608.6885\n",
      "gradient norm: 544.4535751342773, minimum ratio: 0.734375\n",
      "Epoch [1146], val_loss: 2616.7732\n",
      "gradient norm: 545.5954208374023, minimum ratio: 0.75390625\n",
      "Epoch [1147], val_loss: 2624.8772\n",
      "gradient norm: 546.7307205200195, minimum ratio: 0.765625\n",
      "Epoch [1148], val_loss: 2632.9980\n",
      "gradient norm: 547.8381195068359, minimum ratio: 0.7734375\n",
      "Epoch [1149], val_loss: 2641.1387\n",
      "gradient norm: 548.992862701416, minimum ratio: 0.765625\n",
      "Epoch [1150], val_loss: 2649.2976\n",
      "gradient norm: 550.1502685546875, minimum ratio: 0.74609375\n",
      "Epoch [1151], val_loss: 2657.4741\n",
      "gradient norm: 551.3124504089355, minimum ratio: 0.77734375\n",
      "Epoch [1152], val_loss: 2665.6692\n",
      "gradient norm: 552.4732437133789, minimum ratio: 0.76171875\n",
      "Epoch [1153], val_loss: 2673.8823\n",
      "gradient norm: 553.6355018615723, minimum ratio: 0.765625\n",
      "Epoch [1154], val_loss: 2682.1143\n",
      "gradient norm: 554.7936820983887, minimum ratio: 0.76953125\n",
      "Epoch [1155], val_loss: 2690.3645\n",
      "gradient norm: 555.9573402404785, minimum ratio: 0.7421875\n",
      "Epoch [1156], val_loss: 2698.6335\n",
      "gradient norm: 557.1164932250977, minimum ratio: 0.75390625\n",
      "Epoch [1157], val_loss: 2706.9211\n",
      "gradient norm: 558.287784576416, minimum ratio: 0.75390625\n",
      "Epoch [1158], val_loss: 2715.2271\n",
      "gradient norm: 559.439826965332, minimum ratio: 0.74609375\n",
      "Epoch [1159], val_loss: 2723.5520\n",
      "gradient norm: 560.615909576416, minimum ratio: 0.76171875\n",
      "Epoch [1160], val_loss: 2731.8943\n",
      "gradient norm: 561.7871208190918, minimum ratio: 0.76171875\n",
      "Epoch [1161], val_loss: 2740.2561\n",
      "gradient norm: 562.9344444274902, minimum ratio: 0.73046875\n",
      "Epoch [1162], val_loss: 2748.6367\n",
      "gradient norm: 564.1089286804199, minimum ratio: 0.75\n",
      "Epoch [1163], val_loss: 2757.0352\n",
      "gradient norm: 565.2836837768555, minimum ratio: 0.74609375\n",
      "Epoch [1164], val_loss: 2765.4524\n",
      "gradient norm: 566.4532241821289, minimum ratio: 0.76171875\n",
      "Epoch [1165], val_loss: 2773.8887\n",
      "gradient norm: 567.6153984069824, minimum ratio: 0.78515625\n",
      "Epoch [1166], val_loss: 2782.3435\n",
      "gradient norm: 568.8000526428223, minimum ratio: 0.76953125\n",
      "Epoch [1167], val_loss: 2790.8162\n",
      "gradient norm: 569.9556121826172, minimum ratio: 0.76171875\n",
      "Epoch [1168], val_loss: 2799.3086\n",
      "gradient norm: 571.1223487854004, minimum ratio: 0.75\n",
      "Epoch [1169], val_loss: 2807.8201\n",
      "gradient norm: 572.2908058166504, minimum ratio: 0.765625\n",
      "Epoch [1170], val_loss: 2816.3501\n",
      "gradient norm: 573.4753112792969, minimum ratio: 0.75390625\n",
      "Epoch [1171], val_loss: 2824.8992\n",
      "gradient norm: 574.6631660461426, minimum ratio: 0.7890625\n",
      "Epoch [1172], val_loss: 2833.4668\n",
      "gradient norm: 575.8591194152832, minimum ratio: 0.765625\n",
      "Epoch [1173], val_loss: 2842.0532\n",
      "gradient norm: 577.0475273132324, minimum ratio: 0.76171875\n",
      "Epoch [1174], val_loss: 2850.6582\n",
      "gradient norm: 578.239616394043, minimum ratio: 0.7734375\n",
      "Epoch [1175], val_loss: 2859.2825\n",
      "gradient norm: 579.4293899536133, minimum ratio: 0.765625\n",
      "Epoch [1176], val_loss: 2867.9258\n",
      "gradient norm: 580.6314506530762, minimum ratio: 0.7421875\n",
      "Epoch [1177], val_loss: 2876.5879\n",
      "gradient norm: 581.8352928161621, minimum ratio: 0.75390625\n",
      "Epoch [1178], val_loss: 2885.2693\n",
      "gradient norm: 583.0253257751465, minimum ratio: 0.75390625\n",
      "Epoch [1179], val_loss: 2893.9688\n",
      "gradient norm: 584.2252998352051, minimum ratio: 0.7734375\n",
      "Epoch [1180], val_loss: 2902.6882\n",
      "gradient norm: 585.4336624145508, minimum ratio: 0.75\n",
      "Epoch [1181], val_loss: 2911.4263\n",
      "gradient norm: 586.6250610351562, minimum ratio: 0.7734375\n",
      "Epoch [1182], val_loss: 2920.1829\n",
      "gradient norm: 587.7890739440918, minimum ratio: 0.7734375\n",
      "Epoch [1183], val_loss: 2928.9597\n",
      "gradient norm: 588.9861145019531, minimum ratio: 0.76953125\n",
      "Epoch [1184], val_loss: 2937.7554\n",
      "gradient norm: 590.186637878418, minimum ratio: 0.75\n",
      "Epoch [1185], val_loss: 2946.5703\n",
      "gradient norm: 591.3939247131348, minimum ratio: 0.7734375\n",
      "Epoch [1186], val_loss: 2955.4041\n",
      "gradient norm: 592.5858154296875, minimum ratio: 0.7578125\n",
      "Epoch [1187], val_loss: 2964.2578\n",
      "gradient norm: 593.7934303283691, minimum ratio: 0.7890625\n",
      "Epoch [1188], val_loss: 2973.1304\n",
      "gradient norm: 595.0034141540527, minimum ratio: 0.76171875\n",
      "Epoch [1189], val_loss: 2982.0220\n",
      "gradient norm: 596.2178268432617, minimum ratio: 0.75390625\n",
      "Epoch [1190], val_loss: 2990.9338\n",
      "gradient norm: 597.4091682434082, minimum ratio: 0.76953125\n",
      "Epoch [1191], val_loss: 2999.8650\n",
      "gradient norm: 598.6203727722168, minimum ratio: 0.74609375\n",
      "Epoch [1192], val_loss: 3008.8152\n",
      "gradient norm: 599.8451881408691, minimum ratio: 0.75390625\n",
      "Epoch [1193], val_loss: 3017.7849\n",
      "gradient norm: 601.0603675842285, minimum ratio: 0.75390625\n",
      "Epoch [1194], val_loss: 3026.7737\n",
      "gradient norm: 602.2848587036133, minimum ratio: 0.74609375\n",
      "Epoch [1195], val_loss: 3035.7817\n",
      "gradient norm: 603.5024909973145, minimum ratio: 0.75\n",
      "Epoch [1196], val_loss: 3044.8091\n",
      "gradient norm: 604.7351531982422, minimum ratio: 0.76171875\n",
      "Epoch [1197], val_loss: 3053.8560\n",
      "gradient norm: 605.9624519348145, minimum ratio: 0.765625\n",
      "Epoch [1198], val_loss: 3062.9226\n",
      "gradient norm: 607.188117980957, minimum ratio: 0.76171875\n",
      "Epoch [1199], val_loss: 3072.0083\n",
      "gradient norm: 608.4206085205078, minimum ratio: 0.77734375\n",
      "Epoch [1200], val_loss: 3081.1140\n",
      "gradient norm: 609.6200485229492, minimum ratio: 0.78515625\n",
      "Epoch [1201], val_loss: 3090.2393\n",
      "gradient norm: 610.8493881225586, minimum ratio: 0.765625\n",
      "Epoch [1202], val_loss: 3099.3843\n",
      "gradient norm: 612.0899429321289, minimum ratio: 0.75390625\n",
      "Epoch [1203], val_loss: 3108.5481\n",
      "gradient norm: 613.3339080810547, minimum ratio: 0.75390625\n",
      "Epoch [1204], val_loss: 3117.7322\n",
      "gradient norm: 614.559455871582, minimum ratio: 0.76953125\n",
      "Epoch [1205], val_loss: 3126.9353\n",
      "gradient norm: 615.8067588806152, minimum ratio: 0.765625\n",
      "Epoch [1206], val_loss: 3136.1587\n",
      "gradient norm: 617.0169143676758, minimum ratio: 0.765625\n",
      "Epoch [1207], val_loss: 3145.4026\n",
      "gradient norm: 618.2568130493164, minimum ratio: 0.7578125\n",
      "Epoch [1208], val_loss: 3154.6658\n",
      "gradient norm: 619.4751014709473, minimum ratio: 0.76171875\n",
      "Epoch [1209], val_loss: 3163.9490\n",
      "gradient norm: 620.7286376953125, minimum ratio: 0.75390625\n",
      "Epoch [1210], val_loss: 3173.2515\n",
      "gradient norm: 621.9746589660645, minimum ratio: 0.75\n",
      "Epoch [1211], val_loss: 3182.5745\n",
      "gradient norm: 623.2314338684082, minimum ratio: 0.76171875\n",
      "Epoch [1212], val_loss: 3191.9170\n",
      "gradient norm: 624.4444923400879, minimum ratio: 0.76171875\n",
      "Epoch [1213], val_loss: 3201.2795\n",
      "gradient norm: 625.6961631774902, minimum ratio: 0.75\n",
      "Epoch [1214], val_loss: 3210.6621\n",
      "gradient norm: 626.9488754272461, minimum ratio: 0.73828125\n",
      "Epoch [1215], val_loss: 3220.0647\n",
      "gradient norm: 628.205005645752, minimum ratio: 0.76171875\n",
      "Epoch [1216], val_loss: 3229.4871\n",
      "gradient norm: 629.4527740478516, minimum ratio: 0.7421875\n",
      "Epoch [1217], val_loss: 3238.9292\n",
      "gradient norm: 630.718807220459, minimum ratio: 0.7421875\n",
      "Epoch [1218], val_loss: 3248.3914\n",
      "gradient norm: 631.9747352600098, minimum ratio: 0.76171875\n",
      "Epoch [1219], val_loss: 3257.8735\n",
      "gradient norm: 633.2276153564453, minimum ratio: 0.734375\n",
      "Epoch [1220], val_loss: 3267.3757\n",
      "gradient norm: 634.4983291625977, minimum ratio: 0.7578125\n",
      "Epoch [1221], val_loss: 3276.8975\n",
      "gradient norm: 635.7504768371582, minimum ratio: 0.7578125\n",
      "Epoch [1222], val_loss: 3286.4399\n",
      "gradient norm: 637.0061187744141, minimum ratio: 0.7421875\n",
      "Epoch [1223], val_loss: 3296.0022\n",
      "gradient norm: 638.2815284729004, minimum ratio: 0.765625\n",
      "Epoch [1224], val_loss: 3305.5833\n",
      "gradient norm: 639.5479698181152, minimum ratio: 0.75390625\n",
      "Epoch [1225], val_loss: 3315.1853\n",
      "gradient norm: 640.8093681335449, minimum ratio: 0.75\n",
      "Epoch [1226], val_loss: 3324.8074\n",
      "gradient norm: 642.0662307739258, minimum ratio: 0.74609375\n",
      "Epoch [1227], val_loss: 3334.4495\n",
      "gradient norm: 643.322940826416, minimum ratio: 0.74609375\n",
      "Epoch [1228], val_loss: 3344.1116\n",
      "gradient norm: 644.5915985107422, minimum ratio: 0.75390625\n",
      "Epoch [1229], val_loss: 3353.7942\n",
      "gradient norm: 645.8636856079102, minimum ratio: 0.75390625\n",
      "Epoch [1230], val_loss: 3363.4978\n",
      "gradient norm: 647.1252899169922, minimum ratio: 0.734375\n",
      "Epoch [1231], val_loss: 3373.2212\n",
      "gradient norm: 648.3878593444824, minimum ratio: 0.76953125\n",
      "Epoch [1232], val_loss: 3382.9656\n",
      "gradient norm: 649.6692123413086, minimum ratio: 0.75\n",
      "Epoch [1233], val_loss: 3392.7305\n",
      "gradient norm: 650.9401893615723, minimum ratio: 0.76953125\n",
      "Epoch [1234], val_loss: 3402.5159\n",
      "gradient norm: 652.2296333312988, minimum ratio: 0.7421875\n",
      "Epoch [1235], val_loss: 3412.3215\n",
      "gradient norm: 653.5037002563477, minimum ratio: 0.7578125\n",
      "Epoch [1236], val_loss: 3422.1479\n",
      "gradient norm: 654.7818450927734, minimum ratio: 0.73828125\n",
      "Epoch [1237], val_loss: 3431.9951\n",
      "gradient norm: 656.0678939819336, minimum ratio: 0.74609375\n",
      "Epoch [1238], val_loss: 3441.8628\n",
      "gradient norm: 657.336597442627, minimum ratio: 0.76171875\n",
      "Epoch [1239], val_loss: 3451.7507\n",
      "gradient norm: 658.6372489929199, minimum ratio: 0.75390625\n",
      "Epoch [1240], val_loss: 3461.6592\n",
      "gradient norm: 659.9230766296387, minimum ratio: 0.7578125\n",
      "Epoch [1241], val_loss: 3471.5886\n",
      "gradient norm: 661.2255096435547, minimum ratio: 0.73828125\n",
      "Epoch [1242], val_loss: 3481.5378\n",
      "gradient norm: 662.5308609008789, minimum ratio: 0.734375\n",
      "Epoch [1243], val_loss: 3491.5081\n",
      "gradient norm: 663.8058776855469, minimum ratio: 0.75\n",
      "Epoch [1244], val_loss: 3501.4995\n",
      "gradient norm: 665.1034278869629, minimum ratio: 0.75\n",
      "Epoch [1245], val_loss: 3511.5112\n",
      "gradient norm: 666.4020614624023, minimum ratio: 0.734375\n",
      "Epoch [1246], val_loss: 3521.5432\n",
      "gradient norm: 667.6958694458008, minimum ratio: 0.75390625\n",
      "Epoch [1247], val_loss: 3531.5967\n",
      "gradient norm: 669.0029106140137, minimum ratio: 0.734375\n",
      "Epoch [1248], val_loss: 3541.6709\n",
      "gradient norm: 670.2934341430664, minimum ratio: 0.75390625\n",
      "Epoch [1249], val_loss: 3551.7649\n",
      "gradient norm: 671.5959205627441, minimum ratio: 0.7421875\n",
      "Epoch [1250], val_loss: 3561.8804\n",
      "gradient norm: 672.8962631225586, minimum ratio: 0.75\n",
      "Epoch [1251], val_loss: 3572.0183\n",
      "gradient norm: 674.2160987854004, minimum ratio: 0.75390625\n",
      "Epoch [1252], val_loss: 3582.1750\n",
      "gradient norm: 675.5284194946289, minimum ratio: 0.7578125\n",
      "Epoch [1253], val_loss: 3592.3533\n",
      "gradient norm: 676.8326568603516, minimum ratio: 0.76171875\n",
      "Epoch [1254], val_loss: 3602.5525\n",
      "gradient norm: 678.1429557800293, minimum ratio: 0.7578125\n",
      "Epoch [1255], val_loss: 3612.7725\n",
      "gradient norm: 679.4582939147949, minimum ratio: 0.7578125\n",
      "Epoch [1256], val_loss: 3623.0137\n",
      "gradient norm: 680.7748832702637, minimum ratio: 0.78125\n",
      "Epoch [1257], val_loss: 3633.2759\n",
      "gradient norm: 682.0734062194824, minimum ratio: 0.75\n",
      "Epoch [1258], val_loss: 3643.5591\n",
      "gradient norm: 683.3816871643066, minimum ratio: 0.7578125\n",
      "Epoch [1259], val_loss: 3653.8638\n",
      "gradient norm: 684.7038307189941, minimum ratio: 0.7421875\n",
      "Epoch [1260], val_loss: 3664.1887\n",
      "gradient norm: 686.037899017334, minimum ratio: 0.76171875\n",
      "Epoch [1261], val_loss: 3674.5349\n",
      "gradient norm: 687.3641052246094, minimum ratio: 0.7421875\n",
      "Epoch [1262], val_loss: 3684.9021\n",
      "gradient norm: 688.6954116821289, minimum ratio: 0.7578125\n",
      "Epoch [1263], val_loss: 3695.2905\n",
      "gradient norm: 690.0306930541992, minimum ratio: 0.7578125\n",
      "Epoch [1264], val_loss: 3705.7004\n",
      "gradient norm: 691.3392486572266, minimum ratio: 0.75\n",
      "Epoch [1265], val_loss: 3716.1316\n",
      "gradient norm: 692.6446113586426, minimum ratio: 0.76953125\n",
      "Epoch [1266], val_loss: 3726.5837\n",
      "gradient norm: 693.9668769836426, minimum ratio: 0.7734375\n",
      "Epoch [1267], val_loss: 3737.0576\n",
      "gradient norm: 695.2866058349609, minimum ratio: 0.76953125\n",
      "Epoch [1268], val_loss: 3747.5532\n",
      "gradient norm: 696.6268081665039, minimum ratio: 0.7578125\n",
      "Epoch [1269], val_loss: 3758.0701\n",
      "gradient norm: 697.9669418334961, minimum ratio: 0.76171875\n",
      "Epoch [1270], val_loss: 3768.6084\n",
      "gradient norm: 699.3109817504883, minimum ratio: 0.765625\n",
      "Epoch [1271], val_loss: 3779.1675\n",
      "gradient norm: 700.652099609375, minimum ratio: 0.74609375\n",
      "Epoch [1272], val_loss: 3789.7483\n",
      "gradient norm: 701.9825325012207, minimum ratio: 0.75390625\n",
      "Epoch [1273], val_loss: 3800.3503\n",
      "gradient norm: 703.3141288757324, minimum ratio: 0.7578125\n",
      "Epoch [1274], val_loss: 3810.9741\n",
      "gradient norm: 704.6491394042969, minimum ratio: 0.7734375\n",
      "Epoch [1275], val_loss: 3821.6191\n",
      "gradient norm: 706.00732421875, minimum ratio: 0.75\n",
      "Epoch [1276], val_loss: 3832.2859\n",
      "gradient norm: 707.3395042419434, minimum ratio: 0.75\n",
      "Epoch [1277], val_loss: 3842.9734\n",
      "gradient norm: 708.6768074035645, minimum ratio: 0.75390625\n",
      "Epoch [1278], val_loss: 3853.6833\n",
      "gradient norm: 710.0220985412598, minimum ratio: 0.75\n",
      "Epoch [1279], val_loss: 3864.4155\n",
      "gradient norm: 711.380500793457, minimum ratio: 0.7421875\n",
      "Epoch [1280], val_loss: 3875.1680\n",
      "gradient norm: 712.7466659545898, minimum ratio: 0.765625\n",
      "Epoch [1281], val_loss: 3885.9421\n",
      "gradient norm: 714.0923919677734, minimum ratio: 0.73828125\n",
      "Epoch [1282], val_loss: 3896.7383\n",
      "gradient norm: 715.4463348388672, minimum ratio: 0.7421875\n",
      "Epoch [1283], val_loss: 3907.5566\n",
      "gradient norm: 716.798095703125, minimum ratio: 0.75\n",
      "Epoch [1284], val_loss: 3918.3958\n",
      "gradient norm: 718.1668891906738, minimum ratio: 0.73828125\n",
      "Epoch [1285], val_loss: 3929.2559\n",
      "gradient norm: 719.5410614013672, minimum ratio: 0.74609375\n",
      "Epoch [1286], val_loss: 3940.1384\n",
      "gradient norm: 720.8944816589355, minimum ratio: 0.75390625\n",
      "Epoch [1287], val_loss: 3951.0420\n",
      "gradient norm: 722.2314949035645, minimum ratio: 0.7578125\n",
      "Epoch [1288], val_loss: 3961.9680\n",
      "gradient norm: 723.5918388366699, minimum ratio: 0.74609375\n",
      "Epoch [1289], val_loss: 3972.9158\n",
      "gradient norm: 724.9722290039062, minimum ratio: 0.73828125\n",
      "Epoch [1290], val_loss: 3983.8845\n",
      "gradient norm: 726.3362770080566, minimum ratio: 0.75390625\n",
      "Epoch [1291], val_loss: 3994.8762\n",
      "gradient norm: 727.7051963806152, minimum ratio: 0.73046875\n",
      "Epoch [1292], val_loss: 4005.8884\n",
      "gradient norm: 729.0720138549805, minimum ratio: 0.76171875\n",
      "Epoch [1293], val_loss: 4016.9238\n",
      "gradient norm: 730.4587287902832, minimum ratio: 0.76171875\n",
      "Epoch [1294], val_loss: 4027.9800\n",
      "gradient norm: 731.8299865722656, minimum ratio: 0.75390625\n",
      "Epoch [1295], val_loss: 4039.0588\n",
      "gradient norm: 733.1991996765137, minimum ratio: 0.76953125\n",
      "Epoch [1296], val_loss: 4050.1599\n",
      "gradient norm: 734.5827713012695, minimum ratio: 0.76953125\n",
      "Epoch [1297], val_loss: 4061.2834\n",
      "gradient norm: 735.9761772155762, minimum ratio: 0.7578125\n",
      "Epoch [1298], val_loss: 4072.4280\n",
      "gradient norm: 737.3317527770996, minimum ratio: 0.75\n",
      "Epoch [1299], val_loss: 4083.5950\n",
      "gradient norm: 738.7084121704102, minimum ratio: 0.76953125\n",
      "Epoch [1300], val_loss: 4094.7837\n",
      "gradient norm: 740.0931053161621, minimum ratio: 0.7421875\n",
      "Epoch [1301], val_loss: 4105.9946\n",
      "gradient norm: 741.4658355712891, minimum ratio: 0.73046875\n",
      "Epoch [1302], val_loss: 4117.2285\n",
      "gradient norm: 742.8440246582031, minimum ratio: 0.7578125\n",
      "Epoch [1303], val_loss: 4128.4844\n",
      "gradient norm: 744.2290954589844, minimum ratio: 0.765625\n",
      "Epoch [1304], val_loss: 4139.7627\n",
      "gradient norm: 745.6149597167969, minimum ratio: 0.76953125\n",
      "Epoch [1305], val_loss: 4151.0630\n",
      "gradient norm: 747.0091781616211, minimum ratio: 0.734375\n",
      "Epoch [1306], val_loss: 4162.3848\n",
      "gradient norm: 748.3942108154297, minimum ratio: 0.7578125\n",
      "Epoch [1307], val_loss: 4173.7300\n",
      "gradient norm: 749.7828674316406, minimum ratio: 0.75390625\n",
      "Epoch [1308], val_loss: 4185.0972\n",
      "gradient norm: 751.181640625, minimum ratio: 0.765625\n",
      "Epoch [1309], val_loss: 4196.4868\n",
      "gradient norm: 752.5864105224609, minimum ratio: 0.734375\n",
      "Epoch [1310], val_loss: 4207.8975\n",
      "gradient norm: 753.9637145996094, minimum ratio: 0.75\n",
      "Epoch [1311], val_loss: 4219.3320\n",
      "gradient norm: 755.3682479858398, minimum ratio: 0.75390625\n",
      "Epoch [1312], val_loss: 4230.7886\n",
      "gradient norm: 756.7698974609375, minimum ratio: 0.7578125\n",
      "Epoch [1313], val_loss: 4242.2671\n",
      "gradient norm: 758.1890029907227, minimum ratio: 0.74609375\n",
      "Epoch [1314], val_loss: 4253.7686\n",
      "gradient norm: 759.6098022460938, minimum ratio: 0.78125\n",
      "Epoch [1315], val_loss: 4265.2915\n",
      "gradient norm: 761.0321807861328, minimum ratio: 0.75390625\n",
      "Epoch [1316], val_loss: 4276.8384\n",
      "gradient norm: 762.4237670898438, minimum ratio: 0.76171875\n",
      "Epoch [1317], val_loss: 4288.4067\n",
      "gradient norm: 763.8432540893555, minimum ratio: 0.75\n",
      "Epoch [1318], val_loss: 4299.9990\n",
      "gradient norm: 765.2293319702148, minimum ratio: 0.7421875\n",
      "Epoch [1319], val_loss: 4311.6123\n",
      "gradient norm: 766.6479034423828, minimum ratio: 0.7578125\n",
      "Epoch [1320], val_loss: 4323.2490\n",
      "gradient norm: 768.068359375, minimum ratio: 0.77734375\n",
      "Epoch [1321], val_loss: 4334.9082\n",
      "gradient norm: 769.4868240356445, minimum ratio: 0.7578125\n",
      "Epoch [1322], val_loss: 4346.5903\n",
      "gradient norm: 770.9053268432617, minimum ratio: 0.76953125\n",
      "Epoch [1323], val_loss: 4358.2959\n",
      "gradient norm: 772.3078155517578, minimum ratio: 0.7421875\n",
      "Epoch [1324], val_loss: 4370.0239\n",
      "gradient norm: 773.7451477050781, minimum ratio: 0.734375\n",
      "Epoch [1325], val_loss: 4381.7739\n",
      "gradient norm: 775.1776351928711, minimum ratio: 0.74609375\n",
      "Epoch [1326], val_loss: 4393.5474\n",
      "gradient norm: 776.5950622558594, minimum ratio: 0.75\n",
      "Epoch [1327], val_loss: 4405.3423\n",
      "gradient norm: 778.0256958007812, minimum ratio: 0.75\n",
      "Epoch [1328], val_loss: 4417.1626\n",
      "gradient norm: 779.4589767456055, minimum ratio: 0.765625\n",
      "Epoch [1329], val_loss: 4429.0039\n",
      "gradient norm: 780.8719863891602, minimum ratio: 0.765625\n",
      "Epoch [1330], val_loss: 4440.8691\n",
      "gradient norm: 782.314826965332, minimum ratio: 0.74609375\n",
      "Epoch [1331], val_loss: 4452.7559\n",
      "gradient norm: 783.7446441650391, minimum ratio: 0.75390625\n",
      "Epoch [1332], val_loss: 4464.6670\n",
      "gradient norm: 785.1629333496094, minimum ratio: 0.7578125\n",
      "Epoch [1333], val_loss: 4476.6006\n",
      "gradient norm: 786.5866241455078, minimum ratio: 0.74609375\n",
      "Epoch [1334], val_loss: 4488.5566\n",
      "gradient norm: 788.0289306640625, minimum ratio: 0.73828125\n",
      "Epoch [1335], val_loss: 4500.5356\n",
      "gradient norm: 789.471061706543, minimum ratio: 0.7265625\n",
      "Epoch [1336], val_loss: 4512.5381\n",
      "gradient norm: 790.8947982788086, minimum ratio: 0.734375\n",
      "Epoch [1337], val_loss: 4524.5635\n",
      "gradient norm: 792.3474960327148, minimum ratio: 0.796875\n",
      "Epoch [1338], val_loss: 4536.6118\n",
      "gradient norm: 793.7722244262695, minimum ratio: 0.7578125\n",
      "Epoch [1339], val_loss: 4548.6841\n",
      "gradient norm: 795.1984405517578, minimum ratio: 0.75\n",
      "Epoch [1340], val_loss: 4560.7793\n",
      "gradient norm: 796.6550598144531, minimum ratio: 0.75390625\n",
      "Epoch [1341], val_loss: 4572.8979\n",
      "gradient norm: 798.1202621459961, minimum ratio: 0.7578125\n",
      "Epoch [1342], val_loss: 4585.0396\n",
      "gradient norm: 799.5640563964844, minimum ratio: 0.75390625\n",
      "Epoch [1343], val_loss: 4597.2041\n",
      "gradient norm: 800.9927139282227, minimum ratio: 0.76171875\n",
      "Epoch [1344], val_loss: 4609.3936\n",
      "gradient norm: 802.4628829956055, minimum ratio: 0.7421875\n",
      "Epoch [1345], val_loss: 4621.6040\n",
      "gradient norm: 803.9228973388672, minimum ratio: 0.77734375\n",
      "Epoch [1346], val_loss: 4633.8403\n",
      "gradient norm: 805.3885040283203, minimum ratio: 0.765625\n",
      "Epoch [1347], val_loss: 4646.0981\n",
      "gradient norm: 806.8350372314453, minimum ratio: 0.7578125\n",
      "Epoch [1348], val_loss: 4658.3809\n",
      "gradient norm: 808.2998199462891, minimum ratio: 0.73828125\n",
      "Epoch [1349], val_loss: 4670.6855\n",
      "gradient norm: 809.7759475708008, minimum ratio: 0.7578125\n",
      "Epoch [1350], val_loss: 4683.0137\n",
      "gradient norm: 811.2164077758789, minimum ratio: 0.75390625\n",
      "Epoch [1351], val_loss: 4695.3657\n",
      "gradient norm: 812.6980972290039, minimum ratio: 0.7578125\n",
      "Epoch [1352], val_loss: 4707.7407\n",
      "gradient norm: 814.1374588012695, minimum ratio: 0.75390625\n",
      "Epoch [1353], val_loss: 4720.1406\n",
      "gradient norm: 815.5977554321289, minimum ratio: 0.7578125\n",
      "Epoch [1354], val_loss: 4732.5625\n",
      "gradient norm: 817.072639465332, minimum ratio: 0.74609375\n",
      "Epoch [1355], val_loss: 4745.0093\n",
      "gradient norm: 818.5608673095703, minimum ratio: 0.76953125\n",
      "Epoch [1356], val_loss: 4757.4785\n",
      "gradient norm: 820.0404357910156, minimum ratio: 0.76953125\n",
      "Epoch [1357], val_loss: 4769.9712\n",
      "gradient norm: 821.5025787353516, minimum ratio: 0.75\n",
      "Epoch [1358], val_loss: 4782.4883\n",
      "gradient norm: 822.9958267211914, minimum ratio: 0.75390625\n",
      "Epoch [1359], val_loss: 4795.0278\n",
      "gradient norm: 824.4905776977539, minimum ratio: 0.73046875\n",
      "Epoch [1360], val_loss: 4807.5923\n",
      "gradient norm: 825.9745330810547, minimum ratio: 0.76171875\n",
      "Epoch [1361], val_loss: 4820.1802\n",
      "gradient norm: 827.4523773193359, minimum ratio: 0.7578125\n",
      "Epoch [1362], val_loss: 4832.7920\n",
      "gradient norm: 828.9074630737305, minimum ratio: 0.76171875\n",
      "Epoch [1363], val_loss: 4845.4268\n",
      "gradient norm: 830.3895950317383, minimum ratio: 0.74609375\n",
      "Epoch [1364], val_loss: 4858.0854\n",
      "gradient norm: 831.8761444091797, minimum ratio: 0.7578125\n",
      "Epoch [1365], val_loss: 4870.7690\n",
      "gradient norm: 833.380973815918, minimum ratio: 0.76171875\n",
      "Epoch [1366], val_loss: 4883.4746\n",
      "gradient norm: 834.8872833251953, minimum ratio: 0.7578125\n",
      "Epoch [1367], val_loss: 4896.2056\n",
      "gradient norm: 836.3715438842773, minimum ratio: 0.76953125\n",
      "Epoch [1368], val_loss: 4908.9590\n",
      "gradient norm: 837.8327026367188, minimum ratio: 0.74609375\n",
      "Epoch [1369], val_loss: 4921.7383\n",
      "gradient norm: 839.3154296875, minimum ratio: 0.7578125\n",
      "Epoch [1370], val_loss: 4934.5410\n",
      "gradient norm: 840.8231887817383, minimum ratio: 0.75\n",
      "Epoch [1371], val_loss: 4947.3667\n",
      "gradient norm: 842.3204193115234, minimum ratio: 0.76953125\n",
      "Epoch [1372], val_loss: 4960.2183\n",
      "gradient norm: 843.8058090209961, minimum ratio: 0.7578125\n",
      "Epoch [1373], val_loss: 4973.0923\n",
      "gradient norm: 845.2922058105469, minimum ratio: 0.75390625\n",
      "Epoch [1374], val_loss: 4985.9917\n",
      "gradient norm: 846.8023147583008, minimum ratio: 0.7734375\n",
      "Epoch [1375], val_loss: 4998.9141\n",
      "gradient norm: 848.2867126464844, minimum ratio: 0.73828125\n",
      "Epoch [1376], val_loss: 5011.8623\n",
      "gradient norm: 849.7862930297852, minimum ratio: 0.76171875\n",
      "Epoch [1377], val_loss: 5024.8320\n",
      "gradient norm: 851.3076095581055, minimum ratio: 0.765625\n",
      "Epoch [1378], val_loss: 5037.8267\n",
      "gradient norm: 852.8169937133789, minimum ratio: 0.74609375\n",
      "Epoch [1379], val_loss: 5050.8462\n",
      "gradient norm: 854.3314208984375, minimum ratio: 0.73828125\n",
      "Epoch [1380], val_loss: 5063.8887\n",
      "gradient norm: 855.8484954833984, minimum ratio: 0.75390625\n",
      "Epoch [1381], val_loss: 5076.9561\n",
      "gradient norm: 857.3552093505859, minimum ratio: 0.765625\n",
      "Epoch [1382], val_loss: 5090.0469\n",
      "gradient norm: 858.8749465942383, minimum ratio: 0.75390625\n",
      "Epoch [1383], val_loss: 5103.1636\n",
      "gradient norm: 860.3693313598633, minimum ratio: 0.74609375\n",
      "Epoch [1384], val_loss: 5116.3042\n",
      "gradient norm: 861.8993453979492, minimum ratio: 0.7578125\n",
      "Epoch [1385], val_loss: 5129.4683\n",
      "gradient norm: 863.4123764038086, minimum ratio: 0.75390625\n",
      "Epoch [1386], val_loss: 5142.6577\n",
      "gradient norm: 864.931999206543, minimum ratio: 0.73046875\n",
      "Epoch [1387], val_loss: 5155.8716\n",
      "gradient norm: 866.469352722168, minimum ratio: 0.734375\n",
      "Epoch [1388], val_loss: 5169.1108\n",
      "gradient norm: 868.0102615356445, minimum ratio: 0.75\n",
      "Epoch [1389], val_loss: 5182.3730\n",
      "gradient norm: 869.5055465698242, minimum ratio: 0.73828125\n",
      "Epoch [1390], val_loss: 5195.6611\n",
      "gradient norm: 871.0519638061523, minimum ratio: 0.76171875\n",
      "Epoch [1391], val_loss: 5208.9727\n",
      "gradient norm: 872.5941162109375, minimum ratio: 0.75390625\n",
      "Epoch [1392], val_loss: 5222.3081\n",
      "gradient norm: 874.1438598632812, minimum ratio: 0.7578125\n",
      "Epoch [1393], val_loss: 5235.6694\n",
      "gradient norm: 875.6587829589844, minimum ratio: 0.7578125\n",
      "Epoch [1394], val_loss: 5249.0552\n",
      "gradient norm: 877.1608047485352, minimum ratio: 0.76953125\n",
      "Epoch [1395], val_loss: 5262.4663\n",
      "gradient norm: 878.7071914672852, minimum ratio: 0.74609375\n",
      "Epoch [1396], val_loss: 5275.8999\n",
      "gradient norm: 880.2561187744141, minimum ratio: 0.75390625\n",
      "Epoch [1397], val_loss: 5289.3599\n",
      "gradient norm: 881.7782440185547, minimum ratio: 0.76953125\n",
      "Epoch [1398], val_loss: 5302.8442\n",
      "gradient norm: 883.3136291503906, minimum ratio: 0.75390625\n",
      "Epoch [1399], val_loss: 5316.3525\n",
      "gradient norm: 884.8750381469727, minimum ratio: 0.74609375\n",
      "Epoch [1400], val_loss: 5329.8872\n",
      "gradient norm: 886.4314956665039, minimum ratio: 0.73046875\n",
      "Epoch [1401], val_loss: 5343.4443\n",
      "gradient norm: 887.9803161621094, minimum ratio: 0.7578125\n",
      "Epoch [1402], val_loss: 5357.0288\n",
      "gradient norm: 889.5334548950195, minimum ratio: 0.76171875\n",
      "Epoch [1403], val_loss: 5370.6362\n",
      "gradient norm: 891.1018142700195, minimum ratio: 0.76171875\n",
      "Epoch [1404], val_loss: 5384.2705\n",
      "gradient norm: 892.6477508544922, minimum ratio: 0.7421875\n",
      "Epoch [1405], val_loss: 5397.9282\n",
      "gradient norm: 894.1420669555664, minimum ratio: 0.75\n",
      "Epoch [1406], val_loss: 5411.6108\n",
      "gradient norm: 895.6803970336914, minimum ratio: 0.7578125\n",
      "Epoch [1407], val_loss: 5425.3203\n",
      "gradient norm: 897.2234878540039, minimum ratio: 0.76171875\n",
      "Epoch [1408], val_loss: 5439.0542\n",
      "gradient norm: 898.7958145141602, minimum ratio: 0.7734375\n",
      "Epoch [1409], val_loss: 5452.8135\n",
      "gradient norm: 900.3602905273438, minimum ratio: 0.75\n",
      "Epoch [1410], val_loss: 5466.5967\n",
      "gradient norm: 901.9316177368164, minimum ratio: 0.765625\n",
      "Epoch [1411], val_loss: 5480.4048\n",
      "gradient norm: 903.4847183227539, minimum ratio: 0.734375\n",
      "Epoch [1412], val_loss: 5494.2383\n",
      "gradient norm: 905.0649642944336, minimum ratio: 0.76171875\n",
      "Epoch [1413], val_loss: 5508.0972\n",
      "gradient norm: 906.6499557495117, minimum ratio: 0.765625\n",
      "Epoch [1414], val_loss: 5521.9805\n",
      "gradient norm: 908.2368392944336, minimum ratio: 0.765625\n",
      "Epoch [1415], val_loss: 5535.8877\n",
      "gradient norm: 909.8147659301758, minimum ratio: 0.73828125\n",
      "Epoch [1416], val_loss: 5549.8223\n",
      "gradient norm: 911.3501586914062, minimum ratio: 0.75\n",
      "Epoch [1417], val_loss: 5563.7798\n",
      "gradient norm: 912.9168853759766, minimum ratio: 0.73828125\n",
      "Epoch [1418], val_loss: 5577.7646\n",
      "gradient norm: 914.5104293823242, minimum ratio: 0.765625\n",
      "Epoch [1419], val_loss: 5591.7739\n",
      "gradient norm: 916.0629348754883, minimum ratio: 0.7578125\n",
      "Epoch [1420], val_loss: 5605.8081\n",
      "gradient norm: 917.6362380981445, minimum ratio: 0.76171875\n",
      "Epoch [1421], val_loss: 5619.8691\n",
      "gradient norm: 919.1897506713867, minimum ratio: 0.7734375\n",
      "Epoch [1422], val_loss: 5633.9556\n",
      "gradient norm: 920.7901382446289, minimum ratio: 0.7578125\n",
      "Epoch [1423], val_loss: 5648.0669\n",
      "gradient norm: 922.381721496582, minimum ratio: 0.75390625\n",
      "Epoch [1424], val_loss: 5662.2031\n",
      "gradient norm: 923.9553756713867, minimum ratio: 0.7421875\n",
      "Epoch [1425], val_loss: 5676.3662\n",
      "gradient norm: 925.5387344360352, minimum ratio: 0.7578125\n",
      "Epoch [1426], val_loss: 5690.5532\n",
      "gradient norm: 927.1387176513672, minimum ratio: 0.7578125\n",
      "Epoch [1427], val_loss: 5704.7656\n",
      "gradient norm: 928.7179718017578, minimum ratio: 0.76171875\n",
      "Epoch [1428], val_loss: 5719.0049\n",
      "gradient norm: 930.3046264648438, minimum ratio: 0.765625\n",
      "Epoch [1429], val_loss: 5733.2686\n",
      "gradient norm: 931.8799133300781, minimum ratio: 0.74609375\n",
      "Epoch [1430], val_loss: 5747.5601\n",
      "gradient norm: 933.475471496582, minimum ratio: 0.7890625\n",
      "Epoch [1431], val_loss: 5761.8750\n",
      "gradient norm: 935.0704040527344, minimum ratio: 0.74609375\n",
      "Epoch [1432], val_loss: 5776.2178\n",
      "gradient norm: 936.6666717529297, minimum ratio: 0.765625\n",
      "Epoch [1433], val_loss: 5790.5840\n",
      "gradient norm: 938.2621536254883, minimum ratio: 0.75390625\n",
      "Epoch [1434], val_loss: 5804.9766\n",
      "gradient norm: 939.8684844970703, minimum ratio: 0.76171875\n",
      "Epoch [1435], val_loss: 5819.3965\n",
      "gradient norm: 941.4814529418945, minimum ratio: 0.74609375\n",
      "Epoch [1436], val_loss: 5833.8394\n",
      "gradient norm: 943.0825881958008, minimum ratio: 0.76171875\n",
      "Epoch [1437], val_loss: 5848.3101\n",
      "gradient norm: 944.7084808349609, minimum ratio: 0.7578125\n",
      "Epoch [1438], val_loss: 5862.8052\n",
      "gradient norm: 946.2941741943359, minimum ratio: 0.73828125\n",
      "Epoch [1439], val_loss: 5877.3281\n",
      "gradient norm: 947.9235229492188, minimum ratio: 0.7421875\n",
      "Epoch [1440], val_loss: 5891.8745\n",
      "gradient norm: 949.5251617431641, minimum ratio: 0.75390625\n",
      "Epoch [1441], val_loss: 5906.4473\n",
      "gradient norm: 951.1413497924805, minimum ratio: 0.7578125\n",
      "Epoch [1442], val_loss: 5921.0469\n",
      "gradient norm: 952.7609710693359, minimum ratio: 0.78125\n",
      "Epoch [1443], val_loss: 5935.6714\n",
      "gradient norm: 954.3854904174805, minimum ratio: 0.76953125\n",
      "Epoch [1444], val_loss: 5950.3228\n",
      "gradient norm: 956.0077590942383, minimum ratio: 0.734375\n",
      "Epoch [1445], val_loss: 5964.9985\n",
      "gradient norm: 957.6147003173828, minimum ratio: 0.77734375\n",
      "Epoch [1446], val_loss: 5979.7021\n",
      "gradient norm: 959.2403106689453, minimum ratio: 0.7578125\n",
      "Epoch [1447], val_loss: 5994.4302\n",
      "gradient norm: 960.8600921630859, minimum ratio: 0.7421875\n",
      "Epoch [1448], val_loss: 6009.1846\n",
      "gradient norm: 962.4903411865234, minimum ratio: 0.76171875\n",
      "Epoch [1449], val_loss: 6023.9648\n",
      "gradient norm: 964.1215362548828, minimum ratio: 0.765625\n",
      "Epoch [1450], val_loss: 6038.7715\n",
      "gradient norm: 965.7693099975586, minimum ratio: 0.76953125\n",
      "Epoch [1451], val_loss: 6053.6045\n",
      "gradient norm: 967.3685150146484, minimum ratio: 0.76171875\n",
      "Epoch [1452], val_loss: 6068.4629\n",
      "gradient norm: 968.9886016845703, minimum ratio: 0.75\n",
      "Epoch [1453], val_loss: 6083.3486\n",
      "gradient norm: 970.6267852783203, minimum ratio: 0.75\n",
      "Epoch [1454], val_loss: 6098.2603\n",
      "gradient norm: 972.233772277832, minimum ratio: 0.76953125\n",
      "Epoch [1455], val_loss: 6113.1978\n",
      "gradient norm: 973.8871154785156, minimum ratio: 0.76953125\n",
      "Epoch [1456], val_loss: 6128.1636\n",
      "gradient norm: 975.4728317260742, minimum ratio: 0.75390625\n",
      "Epoch [1457], val_loss: 6143.1543\n",
      "gradient norm: 977.1004943847656, minimum ratio: 0.75390625\n",
      "Epoch [1458], val_loss: 6158.1724\n",
      "gradient norm: 978.7621688842773, minimum ratio: 0.76953125\n",
      "Epoch [1459], val_loss: 6173.2168\n",
      "gradient norm: 980.4109420776367, minimum ratio: 0.75\n",
      "Epoch [1460], val_loss: 6188.2871\n",
      "gradient norm: 982.0423889160156, minimum ratio: 0.78125\n",
      "Epoch [1461], val_loss: 6203.3843\n",
      "gradient norm: 983.6887130737305, minimum ratio: 0.75390625\n",
      "Epoch [1462], val_loss: 6218.5073\n",
      "gradient norm: 985.3270874023438, minimum ratio: 0.7421875\n",
      "Epoch [1463], val_loss: 6233.6577\n",
      "gradient norm: 986.9974060058594, minimum ratio: 0.75\n",
      "Epoch [1464], val_loss: 6248.8340\n",
      "gradient norm: 988.6578979492188, minimum ratio: 0.76171875\n",
      "Epoch [1465], val_loss: 6264.0376\n",
      "gradient norm: 990.315559387207, minimum ratio: 0.73828125\n",
      "Epoch [1466], val_loss: 6279.2656\n",
      "gradient norm: 991.9668197631836, minimum ratio: 0.75390625\n",
      "Epoch [1467], val_loss: 6294.5215\n",
      "gradient norm: 993.643928527832, minimum ratio: 0.75390625\n",
      "Epoch [1468], val_loss: 6309.8032\n",
      "gradient norm: 995.3075714111328, minimum ratio: 0.76171875\n",
      "Epoch [1469], val_loss: 6325.1123\n",
      "gradient norm: 996.9750213623047, minimum ratio: 0.765625\n",
      "Epoch [1470], val_loss: 6340.4473\n",
      "gradient norm: 998.648551940918, minimum ratio: 0.7578125\n",
      "Epoch [1471], val_loss: 6355.8081\n",
      "gradient norm: 1000.3324966430664, minimum ratio: 0.75390625\n",
      "Epoch [1472], val_loss: 6371.1968\n",
      "gradient norm: 1001.9460906982422, minimum ratio: 0.76171875\n",
      "Epoch [1473], val_loss: 6386.6118\n",
      "gradient norm: 1003.6201248168945, minimum ratio: 0.7734375\n",
      "Epoch [1474], val_loss: 6402.0552\n",
      "gradient norm: 1005.2870101928711, minimum ratio: 0.7421875\n",
      "Epoch [1475], val_loss: 6417.5239\n",
      "gradient norm: 1006.9153442382812, minimum ratio: 0.77734375\n",
      "Epoch [1476], val_loss: 6433.0205\n",
      "gradient norm: 1008.5843276977539, minimum ratio: 0.7734375\n",
      "Epoch [1477], val_loss: 6448.5435\n",
      "gradient norm: 1010.263542175293, minimum ratio: 0.75390625\n",
      "Epoch [1478], val_loss: 6464.0933\n",
      "gradient norm: 1011.9424057006836, minimum ratio: 0.7578125\n",
      "Epoch [1479], val_loss: 6479.6719\n",
      "gradient norm: 1013.622184753418, minimum ratio: 0.765625\n",
      "Epoch [1480], val_loss: 6495.2749\n",
      "gradient norm: 1015.3189239501953, minimum ratio: 0.765625\n",
      "Epoch [1481], val_loss: 6510.9067\n",
      "gradient norm: 1016.9888610839844, minimum ratio: 0.7421875\n",
      "Epoch [1482], val_loss: 6526.5645\n",
      "gradient norm: 1018.6917114257812, minimum ratio: 0.76953125\n",
      "Epoch [1483], val_loss: 6542.2495\n",
      "gradient norm: 1020.3073959350586, minimum ratio: 0.77734375\n",
      "Epoch [1484], val_loss: 6557.9614\n",
      "gradient norm: 1021.9791259765625, minimum ratio: 0.76171875\n",
      "Epoch [1485], val_loss: 6573.7002\n",
      "gradient norm: 1023.6714248657227, minimum ratio: 0.75\n",
      "Epoch [1486], val_loss: 6589.4678\n",
      "gradient norm: 1025.3480911254883, minimum ratio: 0.7734375\n",
      "Epoch [1487], val_loss: 6605.2607\n",
      "gradient norm: 1027.0579528808594, minimum ratio: 0.73828125\n",
      "Epoch [1488], val_loss: 6621.0830\n",
      "gradient norm: 1028.7713317871094, minimum ratio: 0.734375\n",
      "Epoch [1489], val_loss: 6636.9302\n",
      "gradient norm: 1030.486343383789, minimum ratio: 0.78515625\n",
      "Epoch [1490], val_loss: 6652.8057\n",
      "gradient norm: 1032.1744689941406, minimum ratio: 0.7734375\n",
      "Epoch [1491], val_loss: 6668.7085\n",
      "gradient norm: 1033.855697631836, minimum ratio: 0.78125\n",
      "Epoch [1492], val_loss: 6684.6377\n",
      "gradient norm: 1035.5329055786133, minimum ratio: 0.77734375\n",
      "Epoch [1493], val_loss: 6700.5962\n",
      "gradient norm: 1037.2550582885742, minimum ratio: 0.75\n",
      "Epoch [1494], val_loss: 6716.5811\n",
      "gradient norm: 1038.9648361206055, minimum ratio: 0.7578125\n",
      "Epoch [1495], val_loss: 6732.5942\n",
      "gradient norm: 1040.6690368652344, minimum ratio: 0.76171875\n",
      "Epoch [1496], val_loss: 6748.6333\n",
      "gradient norm: 1042.3963775634766, minimum ratio: 0.80078125\n",
      "Epoch [1497], val_loss: 6764.7007\n",
      "gradient norm: 1044.1185302734375, minimum ratio: 0.76171875\n",
      "Epoch [1498], val_loss: 6780.7949\n",
      "gradient norm: 1045.8255920410156, minimum ratio: 0.78515625\n",
      "Epoch [1499], val_loss: 6796.9165\n",
      "gradient norm: 1047.5293502807617, minimum ratio: 0.7421875\n",
      "Epoch [1500], val_loss: 6813.0669\n",
      "gradient norm: 1049.219711303711, minimum ratio: 0.74609375\n",
      "Epoch [1501], val_loss: 6829.2427\n",
      "gradient norm: 1050.9267349243164, minimum ratio: 0.76953125\n",
      "Epoch [1502], val_loss: 6845.4487\n",
      "gradient norm: 1052.662223815918, minimum ratio: 0.76171875\n",
      "Epoch [1503], val_loss: 6861.6802\n",
      "gradient norm: 1054.3803100585938, minimum ratio: 0.75\n",
      "Epoch [1504], val_loss: 6877.9414\n",
      "gradient norm: 1056.0880889892578, minimum ratio: 0.75\n",
      "Epoch [1505], val_loss: 6894.2280\n",
      "gradient norm: 1057.8310241699219, minimum ratio: 0.75\n",
      "Epoch [1506], val_loss: 6910.5425\n",
      "gradient norm: 1059.5279998779297, minimum ratio: 0.75390625\n",
      "Epoch [1507], val_loss: 6926.8857\n",
      "gradient norm: 1061.2480087280273, minimum ratio: 0.7421875\n",
      "Epoch [1508], val_loss: 6943.2544\n",
      "gradient norm: 1062.9790954589844, minimum ratio: 0.7578125\n",
      "Epoch [1509], val_loss: 6959.6538\n",
      "gradient norm: 1064.6702880859375, minimum ratio: 0.765625\n",
      "Epoch [1510], val_loss: 6976.0791\n",
      "gradient norm: 1066.3706359863281, minimum ratio: 0.75\n",
      "Epoch [1511], val_loss: 6992.5342\n",
      "gradient norm: 1068.1184997558594, minimum ratio: 0.7578125\n",
      "Epoch [1512], val_loss: 7009.0156\n",
      "gradient norm: 1069.839500427246, minimum ratio: 0.73828125\n",
      "Epoch [1513], val_loss: 7025.5259\n",
      "gradient norm: 1071.5964660644531, minimum ratio: 0.75\n",
      "Epoch [1514], val_loss: 7042.0640\n",
      "gradient norm: 1073.328353881836, minimum ratio: 0.75390625\n",
      "Epoch [1515], val_loss: 7058.6309\n",
      "gradient norm: 1075.0590286254883, minimum ratio: 0.76171875\n",
      "Epoch [1516], val_loss: 7075.2261\n",
      "gradient norm: 1076.8163528442383, minimum ratio: 0.76953125\n",
      "Epoch [1517], val_loss: 7091.8477\n",
      "gradient norm: 1078.572494506836, minimum ratio: 0.7421875\n",
      "Epoch [1518], val_loss: 7108.4990\n",
      "gradient norm: 1080.3141479492188, minimum ratio: 0.77734375\n",
      "Epoch [1519], val_loss: 7125.1768\n",
      "gradient norm: 1082.0815811157227, minimum ratio: 0.75\n",
      "Epoch [1520], val_loss: 7141.8833\n",
      "gradient norm: 1083.793556213379, minimum ratio: 0.73828125\n",
      "Epoch [1521], val_loss: 7158.6191\n",
      "gradient norm: 1085.554916381836, minimum ratio: 0.74609375\n",
      "Epoch [1522], val_loss: 7175.3818\n",
      "gradient norm: 1087.3026962280273, minimum ratio: 0.7421875\n",
      "Epoch [1523], val_loss: 7192.1743\n",
      "gradient norm: 1089.053352355957, minimum ratio: 0.74609375\n",
      "Epoch [1524], val_loss: 7208.9932\n",
      "gradient norm: 1090.8128662109375, minimum ratio: 0.76171875\n",
      "Epoch [1525], val_loss: 7225.8418\n",
      "gradient norm: 1092.5617294311523, minimum ratio: 0.765625\n",
      "Epoch [1526], val_loss: 7242.7168\n",
      "gradient norm: 1094.3115005493164, minimum ratio: 0.76171875\n",
      "Epoch [1527], val_loss: 7259.6201\n",
      "gradient norm: 1096.0522994995117, minimum ratio: 0.765625\n",
      "Epoch [1528], val_loss: 7276.5532\n",
      "gradient norm: 1097.829963684082, minimum ratio: 0.7578125\n",
      "Epoch [1529], val_loss: 7293.5132\n",
      "gradient norm: 1099.5755233764648, minimum ratio: 0.74609375\n",
      "Epoch [1530], val_loss: 7310.5034\n",
      "gradient norm: 1101.3456649780273, minimum ratio: 0.734375\n",
      "Epoch [1531], val_loss: 7327.5210\n",
      "gradient norm: 1103.0691757202148, minimum ratio: 0.76953125\n",
      "Epoch [1532], val_loss: 7344.5674\n",
      "gradient norm: 1104.859359741211, minimum ratio: 0.75\n",
      "Epoch [1533], val_loss: 7361.6416\n",
      "gradient norm: 1106.6342391967773, minimum ratio: 0.76953125\n",
      "Epoch [1534], val_loss: 7378.7432\n",
      "gradient norm: 1108.373146057129, minimum ratio: 0.75390625\n",
      "Epoch [1535], val_loss: 7395.8750\n",
      "gradient norm: 1110.1687316894531, minimum ratio: 0.7578125\n",
      "Epoch [1536], val_loss: 7413.0352\n",
      "gradient norm: 1111.9658813476562, minimum ratio: 0.7734375\n",
      "Epoch [1537], val_loss: 7430.2241\n",
      "gradient norm: 1113.7214126586914, minimum ratio: 0.7578125\n",
      "Epoch [1538], val_loss: 7447.4399\n",
      "gradient norm: 1115.4807052612305, minimum ratio: 0.76171875\n",
      "Epoch [1539], val_loss: 7464.6865\n",
      "gradient norm: 1117.2438125610352, minimum ratio: 0.75390625\n",
      "Epoch [1540], val_loss: 7481.9600\n",
      "gradient norm: 1119.0387420654297, minimum ratio: 0.76171875\n",
      "Epoch [1541], val_loss: 7499.2632\n",
      "gradient norm: 1120.8447647094727, minimum ratio: 0.74609375\n",
      "Epoch [1542], val_loss: 7516.5957\n",
      "gradient norm: 1122.6334838867188, minimum ratio: 0.76171875\n",
      "Epoch [1543], val_loss: 7533.9565\n",
      "gradient norm: 1124.4403991699219, minimum ratio: 0.75390625\n",
      "Epoch [1544], val_loss: 7551.3467\n",
      "gradient norm: 1126.2374572753906, minimum ratio: 0.76171875\n",
      "Epoch [1545], val_loss: 7568.7651\n",
      "gradient norm: 1128.0351867675781, minimum ratio: 0.765625\n",
      "Epoch [1546], val_loss: 7586.2134\n",
      "gradient norm: 1129.8077011108398, minimum ratio: 0.7421875\n",
      "Epoch [1547], val_loss: 7603.6899\n",
      "gradient norm: 1131.6002655029297, minimum ratio: 0.75\n",
      "Epoch [1548], val_loss: 7621.1948\n",
      "gradient norm: 1133.418960571289, minimum ratio: 0.75390625\n",
      "Epoch [1549], val_loss: 7638.7300\n",
      "gradient norm: 1135.187385559082, minimum ratio: 0.76171875\n",
      "Epoch [1550], val_loss: 7656.2925\n",
      "gradient norm: 1136.9786987304688, minimum ratio: 0.7578125\n",
      "Epoch [1551], val_loss: 7673.8857\n",
      "gradient norm: 1138.8027954101562, minimum ratio: 0.7578125\n",
      "Epoch [1552], val_loss: 7691.5068\n",
      "gradient norm: 1140.5712890625, minimum ratio: 0.75\n",
      "Epoch [1553], val_loss: 7709.1567\n",
      "gradient norm: 1142.399040222168, minimum ratio: 0.75\n",
      "Epoch [1554], val_loss: 7726.8359\n",
      "gradient norm: 1144.2283172607422, minimum ratio: 0.76953125\n",
      "Epoch [1555], val_loss: 7744.5439\n",
      "gradient norm: 1145.981575012207, minimum ratio: 0.74609375\n",
      "Epoch [1556], val_loss: 7762.2827\n",
      "gradient norm: 1147.7920303344727, minimum ratio: 0.765625\n",
      "Epoch [1557], val_loss: 7780.0493\n",
      "gradient norm: 1149.6267776489258, minimum ratio: 0.76953125\n",
      "Epoch [1558], val_loss: 7797.8452\n",
      "gradient norm: 1151.4611282348633, minimum ratio: 0.75\n",
      "Epoch [1559], val_loss: 7815.6689\n",
      "gradient norm: 1153.2725067138672, minimum ratio: 0.765625\n",
      "Epoch [1560], val_loss: 7833.5234\n",
      "gradient norm: 1155.1124267578125, minimum ratio: 0.75\n",
      "Epoch [1561], val_loss: 7851.4058\n",
      "gradient norm: 1156.914665222168, minimum ratio: 0.74609375\n",
      "Epoch [1562], val_loss: 7869.3174\n",
      "gradient norm: 1158.7583770751953, minimum ratio: 0.7421875\n",
      "Epoch [1563], val_loss: 7887.2593\n",
      "gradient norm: 1160.570701599121, minimum ratio: 0.734375\n",
      "Epoch [1564], val_loss: 7905.2300\n",
      "gradient norm: 1162.3601684570312, minimum ratio: 0.74609375\n",
      "Epoch [1565], val_loss: 7923.2310\n",
      "gradient norm: 1164.151252746582, minimum ratio: 0.76171875\n",
      "Epoch [1566], val_loss: 7941.2598\n",
      "gradient norm: 1165.9122848510742, minimum ratio: 0.7578125\n",
      "Epoch [1567], val_loss: 7959.3218\n",
      "gradient norm: 1167.7266311645508, minimum ratio: 0.76171875\n",
      "Epoch [1568], val_loss: 7977.4102\n",
      "gradient norm: 1169.551139831543, minimum ratio: 0.7578125\n",
      "Epoch [1569], val_loss: 7995.5293\n",
      "gradient norm: 1171.3922882080078, minimum ratio: 0.7734375\n",
      "Epoch [1570], val_loss: 8013.6782\n",
      "gradient norm: 1173.2500534057617, minimum ratio: 0.77734375\n",
      "Epoch [1571], val_loss: 8031.8564\n",
      "gradient norm: 1175.065299987793, minimum ratio: 0.75\n",
      "Epoch [1572], val_loss: 8050.0649\n",
      "gradient norm: 1176.9033660888672, minimum ratio: 0.76171875\n",
      "Epoch [1573], val_loss: 8068.3027\n",
      "gradient norm: 1178.7651062011719, minimum ratio: 0.7578125\n",
      "Epoch [1574], val_loss: 8086.5708\n",
      "gradient norm: 1180.6162338256836, minimum ratio: 0.7421875\n",
      "Epoch [1575], val_loss: 8104.8677\n",
      "gradient norm: 1182.4604263305664, minimum ratio: 0.7578125\n",
      "Epoch [1576], val_loss: 8123.1943\n",
      "gradient norm: 1184.2962799072266, minimum ratio: 0.77734375\n",
      "Epoch [1577], val_loss: 8141.5498\n",
      "gradient norm: 1186.1453475952148, minimum ratio: 0.75\n",
      "Epoch [1578], val_loss: 8159.9365\n",
      "gradient norm: 1188.0033493041992, minimum ratio: 0.7734375\n",
      "Epoch [1579], val_loss: 8178.3525\n",
      "gradient norm: 1189.8358764648438, minimum ratio: 0.7578125\n",
      "Epoch [1580], val_loss: 8196.7979\n",
      "gradient norm: 1191.6559219360352, minimum ratio: 0.734375\n",
      "Epoch [1581], val_loss: 8215.2744\n",
      "gradient norm: 1193.5334014892578, minimum ratio: 0.77734375\n",
      "Epoch [1582], val_loss: 8233.7783\n",
      "gradient norm: 1195.412483215332, minimum ratio: 0.7578125\n",
      "Epoch [1583], val_loss: 8252.3135\n",
      "gradient norm: 1197.2937774658203, minimum ratio: 0.7421875\n",
      "Epoch [1584], val_loss: 8270.8789\n",
      "gradient norm: 1199.1703262329102, minimum ratio: 0.76171875\n",
      "Epoch [1585], val_loss: 8289.4707\n",
      "gradient norm: 1201.0301666259766, minimum ratio: 0.7578125\n",
      "Epoch [1586], val_loss: 8308.0967\n",
      "gradient norm: 1202.906349182129, minimum ratio: 0.76953125\n",
      "Epoch [1587], val_loss: 8326.7510\n",
      "gradient norm: 1204.7612915039062, minimum ratio: 0.78515625\n",
      "Epoch [1588], val_loss: 8345.4355\n",
      "gradient norm: 1206.6408615112305, minimum ratio: 0.7578125\n",
      "Epoch [1589], val_loss: 8364.1504\n",
      "gradient norm: 1208.5172119140625, minimum ratio: 0.76953125\n",
      "Epoch [1590], val_loss: 8382.8945\n",
      "gradient norm: 1210.410774230957, minimum ratio: 0.75390625\n",
      "Epoch [1591], val_loss: 8401.6680\n",
      "gradient norm: 1212.2749099731445, minimum ratio: 0.7734375\n",
      "Epoch [1592], val_loss: 8420.4727\n",
      "gradient norm: 1214.1499404907227, minimum ratio: 0.73046875\n",
      "Epoch [1593], val_loss: 8439.3076\n",
      "gradient norm: 1216.0131912231445, minimum ratio: 0.75390625\n",
      "Epoch [1594], val_loss: 8458.1719\n",
      "gradient norm: 1217.8036804199219, minimum ratio: 0.74609375\n",
      "Epoch [1595], val_loss: 8477.0693\n",
      "gradient norm: 1219.6635437011719, minimum ratio: 0.75390625\n",
      "Epoch [1596], val_loss: 8495.9951\n",
      "gradient norm: 1221.5177993774414, minimum ratio: 0.76171875\n",
      "Epoch [1597], val_loss: 8514.9521\n",
      "gradient norm: 1223.4240188598633, minimum ratio: 0.76953125\n",
      "Epoch [1598], val_loss: 8533.9395\n",
      "gradient norm: 1225.2881164550781, minimum ratio: 0.75\n",
      "Epoch [1599], val_loss: 8552.9570\n",
      "gradient norm: 1227.1979064941406, minimum ratio: 0.76171875\n",
      "Epoch [1600], val_loss: 8572.0049\n",
      "gradient norm: 1229.0910110473633, minimum ratio: 0.76171875\n",
      "Epoch [1601], val_loss: 8591.0840\n",
      "gradient norm: 1230.9637451171875, minimum ratio: 0.7421875\n",
      "Epoch [1602], val_loss: 8610.1934\n",
      "gradient norm: 1232.8367614746094, minimum ratio: 0.7578125\n",
      "Epoch [1603], val_loss: 8629.3320\n",
      "gradient norm: 1234.6971969604492, minimum ratio: 0.7421875\n",
      "Epoch [1604], val_loss: 8648.5029\n",
      "gradient norm: 1236.6009521484375, minimum ratio: 0.75\n",
      "Epoch [1605], val_loss: 8667.7041\n",
      "gradient norm: 1238.4825057983398, minimum ratio: 0.75\n",
      "Epoch [1606], val_loss: 8686.9346\n",
      "gradient norm: 1240.4049072265625, minimum ratio: 0.7265625\n",
      "Epoch [1607], val_loss: 8706.1963\n",
      "gradient norm: 1242.3293380737305, minimum ratio: 0.7421875\n",
      "Epoch [1608], val_loss: 8725.4883\n",
      "gradient norm: 1244.2356338500977, minimum ratio: 0.75390625\n",
      "Epoch [1609], val_loss: 8744.8105\n",
      "gradient norm: 1246.1418838500977, minimum ratio: 0.7265625\n",
      "Epoch [1610], val_loss: 8764.1631\n",
      "gradient norm: 1248.0436477661133, minimum ratio: 0.74609375\n",
      "Epoch [1611], val_loss: 8783.5479\n",
      "gradient norm: 1249.9527206420898, minimum ratio: 0.76953125\n",
      "Epoch [1612], val_loss: 8802.9619\n",
      "gradient norm: 1251.8380279541016, minimum ratio: 0.75\n",
      "Epoch [1613], val_loss: 8822.4062\n",
      "gradient norm: 1253.7729873657227, minimum ratio: 0.75\n",
      "Epoch [1614], val_loss: 8841.8838\n",
      "gradient norm: 1255.7098388671875, minimum ratio: 0.76171875\n",
      "Epoch [1615], val_loss: 8861.3877\n",
      "gradient norm: 1257.6226272583008, minimum ratio: 0.7265625\n",
      "Epoch [1616], val_loss: 8880.9258\n",
      "gradient norm: 1259.548324584961, minimum ratio: 0.76171875\n",
      "Epoch [1617], val_loss: 8900.4932\n",
      "gradient norm: 1261.4322128295898, minimum ratio: 0.7578125\n",
      "Epoch [1618], val_loss: 8920.0928\n",
      "gradient norm: 1263.3760299682617, minimum ratio: 0.74609375\n",
      "Epoch [1619], val_loss: 8939.7207\n",
      "gradient norm: 1265.3217697143555, minimum ratio: 0.7265625\n",
      "Epoch [1620], val_loss: 8959.3799\n",
      "gradient norm: 1267.2465286254883, minimum ratio: 0.7578125\n",
      "Epoch [1621], val_loss: 8979.0713\n",
      "gradient norm: 1269.1926193237305, minimum ratio: 0.734375\n",
      "Epoch [1622], val_loss: 8998.7930\n",
      "gradient norm: 1271.0957870483398, minimum ratio: 0.7578125\n",
      "Epoch [1623], val_loss: 9018.5459\n",
      "gradient norm: 1272.9692916870117, minimum ratio: 0.73828125\n",
      "Epoch [1624], val_loss: 9038.3311\n",
      "gradient norm: 1274.8689575195312, minimum ratio: 0.7421875\n",
      "Epoch [1625], val_loss: 9058.1475\n",
      "gradient norm: 1276.8054428100586, minimum ratio: 0.76171875\n",
      "Epoch [1626], val_loss: 9077.9932\n",
      "gradient norm: 1278.7233810424805, minimum ratio: 0.74609375\n",
      "Epoch [1627], val_loss: 9097.8701\n",
      "gradient norm: 1280.6254806518555, minimum ratio: 0.75390625\n",
      "Epoch [1628], val_loss: 9117.7803\n",
      "gradient norm: 1282.5207061767578, minimum ratio: 0.75390625\n",
      "Epoch [1629], val_loss: 9137.7207\n",
      "gradient norm: 1284.4843978881836, minimum ratio: 0.75\n",
      "Epoch [1630], val_loss: 9157.6934\n",
      "gradient norm: 1286.4139404296875, minimum ratio: 0.7578125\n",
      "Epoch [1631], val_loss: 9177.6953\n",
      "gradient norm: 1288.3085479736328, minimum ratio: 0.7421875\n",
      "Epoch [1632], val_loss: 9197.7305\n",
      "gradient norm: 1290.2599487304688, minimum ratio: 0.75390625\n",
      "Epoch [1633], val_loss: 9217.7959\n",
      "gradient norm: 1292.2308807373047, minimum ratio: 0.7421875\n",
      "Epoch [1634], val_loss: 9237.8926\n",
      "gradient norm: 1294.2035827636719, minimum ratio: 0.74609375\n",
      "Epoch [1635], val_loss: 9258.0215\n",
      "gradient norm: 1296.1782760620117, minimum ratio: 0.75\n",
      "Epoch [1636], val_loss: 9278.1797\n",
      "gradient norm: 1298.1420669555664, minimum ratio: 0.765625\n",
      "Epoch [1637], val_loss: 9298.3711\n",
      "gradient norm: 1300.0847930908203, minimum ratio: 0.73046875\n",
      "Epoch [1638], val_loss: 9318.5928\n",
      "gradient norm: 1302.0640487670898, minimum ratio: 0.75\n",
      "Epoch [1639], val_loss: 9338.8457\n",
      "gradient norm: 1304.0460891723633, minimum ratio: 0.77734375\n",
      "Epoch [1640], val_loss: 9359.1309\n",
      "gradient norm: 1306.0298461914062, minimum ratio: 0.75390625\n",
      "Epoch [1641], val_loss: 9379.4453\n",
      "gradient norm: 1307.9995956420898, minimum ratio: 0.734375\n",
      "Epoch [1642], val_loss: 9399.7939\n",
      "gradient norm: 1309.943244934082, minimum ratio: 0.76171875\n",
      "Epoch [1643], val_loss: 9420.1729\n",
      "gradient norm: 1311.90673828125, minimum ratio: 0.75\n",
      "Epoch [1644], val_loss: 9440.5859\n",
      "gradient norm: 1313.877555847168, minimum ratio: 0.75390625\n",
      "Epoch [1645], val_loss: 9461.0273\n",
      "gradient norm: 1315.8486709594727, minimum ratio: 0.765625\n",
      "Epoch [1646], val_loss: 9481.5020\n",
      "gradient norm: 1317.7926940917969, minimum ratio: 0.77734375\n",
      "Epoch [1647], val_loss: 9502.0078\n",
      "gradient norm: 1319.7785720825195, minimum ratio: 0.7421875\n",
      "Epoch [1648], val_loss: 9522.5430\n",
      "gradient norm: 1321.7223052978516, minimum ratio: 0.76171875\n",
      "Epoch [1649], val_loss: 9543.1133\n",
      "gradient norm: 1323.7224960327148, minimum ratio: 0.75\n",
      "Epoch [1650], val_loss: 9563.7119\n",
      "gradient norm: 1325.701416015625, minimum ratio: 0.73828125\n",
      "Epoch [1651], val_loss: 9584.3447\n",
      "gradient norm: 1327.6875305175781, minimum ratio: 0.74609375\n",
      "Epoch [1652], val_loss: 9605.0088\n",
      "gradient norm: 1329.6440505981445, minimum ratio: 0.7421875\n",
      "Epoch [1653], val_loss: 9625.7061\n",
      "gradient norm: 1331.5741882324219, minimum ratio: 0.75\n",
      "Epoch [1654], val_loss: 9646.4316\n",
      "gradient norm: 1333.5582962036133, minimum ratio: 0.75390625\n",
      "Epoch [1655], val_loss: 9667.1914\n",
      "gradient norm: 1335.4994659423828, minimum ratio: 0.734375\n",
      "Epoch [1656], val_loss: 9687.9844\n",
      "gradient norm: 1337.469825744629, minimum ratio: 0.75\n",
      "Epoch [1657], val_loss: 9708.8086\n",
      "gradient norm: 1339.4586563110352, minimum ratio: 0.765625\n",
      "Epoch [1658], val_loss: 9729.6670\n",
      "gradient norm: 1341.3767471313477, minimum ratio: 0.76171875\n",
      "Epoch [1659], val_loss: 9750.5537\n",
      "gradient norm: 1343.3546524047852, minimum ratio: 0.75390625\n",
      "Epoch [1660], val_loss: 9771.4746\n",
      "gradient norm: 1345.36083984375, minimum ratio: 0.7734375\n",
      "Epoch [1661], val_loss: 9792.4277\n",
      "gradient norm: 1347.368797302246, minimum ratio: 0.75\n",
      "Epoch [1662], val_loss: 9813.4121\n",
      "gradient norm: 1349.3925704956055, minimum ratio: 0.7578125\n",
      "Epoch [1663], val_loss: 9834.4297\n",
      "gradient norm: 1351.4016876220703, minimum ratio: 0.7578125\n",
      "Epoch [1664], val_loss: 9855.4785\n",
      "gradient norm: 1353.4271545410156, minimum ratio: 0.76171875\n",
      "Epoch [1665], val_loss: 9876.5596\n",
      "gradient norm: 1355.4332809448242, minimum ratio: 0.76171875\n",
      "Epoch [1666], val_loss: 9897.6729\n",
      "gradient norm: 1357.4408874511719, minimum ratio: 0.7734375\n",
      "Epoch [1667], val_loss: 9918.8193\n",
      "gradient norm: 1359.4566345214844, minimum ratio: 0.75\n",
      "Epoch [1668], val_loss: 9939.9971\n",
      "gradient norm: 1361.4752731323242, minimum ratio: 0.7578125\n",
      "Epoch [1669], val_loss: 9961.2070\n",
      "gradient norm: 1363.4882736206055, minimum ratio: 0.76171875\n",
      "Epoch [1670], val_loss: 9982.4492\n",
      "gradient norm: 1365.526840209961, minimum ratio: 0.77734375\n",
      "Epoch [1671], val_loss: 10003.7236\n",
      "gradient norm: 1367.5284271240234, minimum ratio: 0.765625\n",
      "Epoch [1672], val_loss: 10025.0303\n",
      "gradient norm: 1369.570442199707, minimum ratio: 0.765625\n",
      "Epoch [1673], val_loss: 10046.3672\n",
      "gradient norm: 1371.5922470092773, minimum ratio: 0.76171875\n",
      "Epoch [1674], val_loss: 10067.7383\n",
      "gradient norm: 1373.6121292114258, minimum ratio: 0.74609375\n",
      "Epoch [1675], val_loss: 10089.1416\n",
      "gradient norm: 1375.6243515014648, minimum ratio: 0.78515625\n",
      "Epoch [1676], val_loss: 10110.5762\n",
      "gradient norm: 1377.6533889770508, minimum ratio: 0.74609375\n",
      "Epoch [1677], val_loss: 10132.0449\n",
      "gradient norm: 1379.6605834960938, minimum ratio: 0.73828125\n",
      "Epoch [1678], val_loss: 10153.5449\n",
      "gradient norm: 1381.7134399414062, minimum ratio: 0.765625\n",
      "Epoch [1679], val_loss: 10175.0781\n",
      "gradient norm: 1383.7683334350586, minimum ratio: 0.76171875\n",
      "Epoch [1680], val_loss: 10196.6426\n",
      "gradient norm: 1385.781753540039, minimum ratio: 0.7578125\n",
      "Epoch [1681], val_loss: 10218.2422\n",
      "gradient norm: 1387.745002746582, minimum ratio: 0.75\n",
      "Epoch [1682], val_loss: 10239.8721\n",
      "gradient norm: 1389.8050918579102, minimum ratio: 0.7421875\n",
      "Epoch [1683], val_loss: 10261.5361\n",
      "gradient norm: 1391.8385620117188, minimum ratio: 0.76171875\n",
      "Epoch [1684], val_loss: 10283.2334\n",
      "gradient norm: 1393.8465270996094, minimum ratio: 0.7421875\n",
      "Epoch [1685], val_loss: 10304.9609\n",
      "gradient norm: 1395.8503723144531, minimum ratio: 0.75\n",
      "Epoch [1686], val_loss: 10326.7246\n",
      "gradient norm: 1397.8904876708984, minimum ratio: 0.76953125\n",
      "Epoch [1687], val_loss: 10348.5195\n",
      "gradient norm: 1399.8638381958008, minimum ratio: 0.75390625\n",
      "Epoch [1688], val_loss: 10370.3496\n",
      "gradient norm: 1401.9118194580078, minimum ratio: 0.75390625\n",
      "Epoch [1689], val_loss: 10392.2109\n",
      "gradient norm: 1403.9439086914062, minimum ratio: 0.74609375\n",
      "Epoch [1690], val_loss: 10414.1055\n",
      "gradient norm: 1405.9429092407227, minimum ratio: 0.76171875\n",
      "Epoch [1691], val_loss: 10436.0342\n",
      "gradient norm: 1408.0197143554688, minimum ratio: 0.73828125\n",
      "Epoch [1692], val_loss: 10457.9941\n",
      "gradient norm: 1410.098243713379, minimum ratio: 0.77734375\n",
      "Epoch [1693], val_loss: 10479.9883\n",
      "gradient norm: 1412.1514739990234, minimum ratio: 0.7421875\n",
      "Epoch [1694], val_loss: 10502.0146\n",
      "gradient norm: 1414.1979675292969, minimum ratio: 0.75390625\n",
      "Epoch [1695], val_loss: 10524.0771\n",
      "gradient norm: 1416.282211303711, minimum ratio: 0.75390625\n",
      "Epoch [1696], val_loss: 10546.1680\n",
      "gradient norm: 1418.368064880371, minimum ratio: 0.7578125\n",
      "Epoch [1697], val_loss: 10568.2930\n",
      "gradient norm: 1420.4306640625, minimum ratio: 0.765625\n",
      "Epoch [1698], val_loss: 10590.4521\n",
      "gradient norm: 1422.4990158081055, minimum ratio: 0.7421875\n",
      "Epoch [1699], val_loss: 10612.6436\n",
      "gradient norm: 1424.5418090820312, minimum ratio: 0.74609375\n",
      "Epoch [1700], val_loss: 10634.8701\n",
      "gradient norm: 1426.6353149414062, minimum ratio: 0.7578125\n",
      "Epoch [1701], val_loss: 10657.1279\n",
      "gradient norm: 1428.7305908203125, minimum ratio: 0.74609375\n",
      "Epoch [1702], val_loss: 10679.4180\n",
      "gradient norm: 1430.827766418457, minimum ratio: 0.765625\n",
      "Epoch [1703], val_loss: 10701.7422\n",
      "gradient norm: 1432.889259338379, minimum ratio: 0.73828125\n",
      "Epoch [1704], val_loss: 10724.0977\n",
      "gradient norm: 1434.990005493164, minimum ratio: 0.75390625\n",
      "Epoch [1705], val_loss: 10746.4883\n",
      "gradient norm: 1437.0927200317383, minimum ratio: 0.76953125\n",
      "Epoch [1706], val_loss: 10768.9102\n",
      "gradient norm: 1439.133529663086, minimum ratio: 0.7578125\n",
      "Epoch [1707], val_loss: 10791.3682\n",
      "gradient norm: 1441.1780624389648, minimum ratio: 0.7734375\n",
      "Epoch [1708], val_loss: 10813.8584\n",
      "gradient norm: 1443.258071899414, minimum ratio: 0.7421875\n",
      "Epoch [1709], val_loss: 10836.3818\n",
      "gradient norm: 1445.3369216918945, minimum ratio: 0.75390625\n",
      "Epoch [1710], val_loss: 10858.9385\n",
      "gradient norm: 1447.4489135742188, minimum ratio: 0.76171875\n",
      "Epoch [1711], val_loss: 10881.5283\n",
      "gradient norm: 1449.4678497314453, minimum ratio: 0.75390625\n",
      "Epoch [1712], val_loss: 10904.1533\n",
      "gradient norm: 1451.5425186157227, minimum ratio: 0.74609375\n",
      "Epoch [1713], val_loss: 10926.8096\n",
      "gradient norm: 1453.6203994750977, minimum ratio: 0.75390625\n",
      "Epoch [1714], val_loss: 10949.5029\n",
      "gradient norm: 1455.7097091674805, minimum ratio: 0.734375\n",
      "Epoch [1715], val_loss: 10972.2256\n",
      "gradient norm: 1457.7856063842773, minimum ratio: 0.734375\n",
      "Epoch [1716], val_loss: 10994.9854\n",
      "gradient norm: 1459.7841110229492, minimum ratio: 0.7734375\n",
      "Epoch [1717], val_loss: 11017.7773\n",
      "gradient norm: 1461.9085540771484, minimum ratio: 0.76171875\n",
      "Epoch [1718], val_loss: 11040.6055\n",
      "gradient norm: 1464.0127792358398, minimum ratio: 0.73828125\n",
      "Epoch [1719], val_loss: 11063.4648\n",
      "gradient norm: 1466.111473083496, minimum ratio: 0.765625\n",
      "Epoch [1720], val_loss: 11086.3574\n",
      "gradient norm: 1468.2228469848633, minimum ratio: 0.734375\n",
      "Epoch [1721], val_loss: 11109.2871\n",
      "gradient norm: 1470.3350067138672, minimum ratio: 0.734375\n",
      "Epoch [1722], val_loss: 11132.2471\n",
      "gradient norm: 1472.4185638427734, minimum ratio: 0.75\n",
      "Epoch [1723], val_loss: 11155.2451\n",
      "gradient norm: 1474.552749633789, minimum ratio: 0.75390625\n",
      "Epoch [1724], val_loss: 11178.2725\n",
      "gradient norm: 1476.6608123779297, minimum ratio: 0.7421875\n",
      "Epoch [1725], val_loss: 11201.3369\n",
      "gradient norm: 1478.7824172973633, minimum ratio: 0.74609375\n",
      "Epoch [1726], val_loss: 11224.4336\n",
      "gradient norm: 1480.9237442016602, minimum ratio: 0.75\n",
      "Epoch [1727], val_loss: 11247.5625\n",
      "gradient norm: 1483.0411758422852, minimum ratio: 0.75\n",
      "Epoch [1728], val_loss: 11270.7285\n",
      "gradient norm: 1485.161117553711, minimum ratio: 0.75\n",
      "Epoch [1729], val_loss: 11293.9258\n",
      "gradient norm: 1487.3079147338867, minimum ratio: 0.73046875\n",
      "Epoch [1730], val_loss: 11317.1572\n",
      "gradient norm: 1489.4567947387695, minimum ratio: 0.76171875\n",
      "Epoch [1731], val_loss: 11340.4229\n",
      "gradient norm: 1491.5677642822266, minimum ratio: 0.76171875\n",
      "Epoch [1732], val_loss: 11363.7246\n",
      "gradient norm: 1493.720329284668, minimum ratio: 0.75\n",
      "Epoch [1733], val_loss: 11387.0566\n",
      "gradient norm: 1495.7705612182617, minimum ratio: 0.73828125\n",
      "Epoch [1734], val_loss: 11410.4238\n",
      "gradient norm: 1497.9163436889648, minimum ratio: 0.73828125\n",
      "Epoch [1735], val_loss: 11433.8271\n",
      "gradient norm: 1500.056884765625, minimum ratio: 0.76171875\n",
      "Epoch [1736], val_loss: 11457.2637\n",
      "gradient norm: 1502.1902923583984, minimum ratio: 0.75390625\n",
      "Epoch [1737], val_loss: 11480.7334\n",
      "gradient norm: 1504.3511657714844, minimum ratio: 0.75\n",
      "Epoch [1738], val_loss: 11504.2373\n",
      "gradient norm: 1506.514633178711, minimum ratio: 0.74609375\n",
      "Epoch [1739], val_loss: 11527.7764\n",
      "gradient norm: 1508.6625061035156, minimum ratio: 0.7578125\n",
      "Epoch [1740], val_loss: 11551.3496\n",
      "gradient norm: 1510.7866897583008, minimum ratio: 0.734375\n",
      "Epoch [1741], val_loss: 11574.9561\n",
      "gradient norm: 1512.8979873657227, minimum ratio: 0.76171875\n",
      "Epoch [1742], val_loss: 11598.5986\n",
      "gradient norm: 1514.9857330322266, minimum ratio: 0.77734375\n",
      "Epoch [1743], val_loss: 11622.2734\n",
      "gradient norm: 1517.1380615234375, minimum ratio: 0.7421875\n",
      "Epoch [1744], val_loss: 11645.9854\n",
      "gradient norm: 1519.2658615112305, minimum ratio: 0.74609375\n",
      "Epoch [1745], val_loss: 11669.7295\n",
      "gradient norm: 1521.373390197754, minimum ratio: 0.7578125\n",
      "Epoch [1746], val_loss: 11693.5117\n",
      "gradient norm: 1523.4664916992188, minimum ratio: 0.75390625\n",
      "Epoch [1747], val_loss: 11717.3262\n",
      "gradient norm: 1525.620460510254, minimum ratio: 0.73828125\n",
      "Epoch [1748], val_loss: 11741.1748\n",
      "gradient norm: 1527.7436599731445, minimum ratio: 0.75\n",
      "Epoch [1749], val_loss: 11765.0615\n",
      "gradient norm: 1529.8814849853516, minimum ratio: 0.75\n",
      "Epoch [1750], val_loss: 11788.9795\n",
      "gradient norm: 1532.0166778564453, minimum ratio: 0.7421875\n",
      "Epoch [1751], val_loss: 11812.9346\n",
      "gradient norm: 1534.2041549682617, minimum ratio: 0.76953125\n",
      "Epoch [1752], val_loss: 11836.9219\n",
      "gradient norm: 1536.3621139526367, minimum ratio: 0.7578125\n",
      "Epoch [1753], val_loss: 11860.9463\n",
      "gradient norm: 1538.5166549682617, minimum ratio: 0.74609375\n",
      "Epoch [1754], val_loss: 11885.0029\n",
      "gradient norm: 1540.7099151611328, minimum ratio: 0.78125\n",
      "Epoch [1755], val_loss: 11909.0947\n",
      "gradient norm: 1542.8818054199219, minimum ratio: 0.75390625\n",
      "Epoch [1756], val_loss: 11933.2227\n",
      "gradient norm: 1545.0497436523438, minimum ratio: 0.77734375\n",
      "Epoch [1757], val_loss: 11957.3838\n",
      "gradient norm: 1547.2214965820312, minimum ratio: 0.7578125\n",
      "Epoch [1758], val_loss: 11981.5811\n",
      "gradient norm: 1549.422119140625, minimum ratio: 0.74609375\n",
      "Epoch [1759], val_loss: 12005.8105\n",
      "gradient norm: 1551.6248474121094, minimum ratio: 0.76953125\n",
      "Epoch [1760], val_loss: 12030.0771\n",
      "gradient norm: 1553.755859375, minimum ratio: 0.76171875\n",
      "Epoch [1761], val_loss: 12054.3779\n",
      "gradient norm: 1555.9194641113281, minimum ratio: 0.734375\n",
      "Epoch [1762], val_loss: 12078.7129\n",
      "gradient norm: 1558.1275787353516, minimum ratio: 0.7578125\n",
      "Epoch [1763], val_loss: 12103.0840\n",
      "gradient norm: 1560.3103790283203, minimum ratio: 0.75\n",
      "Epoch [1764], val_loss: 12127.4883\n",
      "gradient norm: 1562.4950256347656, minimum ratio: 0.7578125\n",
      "Epoch [1765], val_loss: 12151.9287\n",
      "gradient norm: 1564.7088928222656, minimum ratio: 0.74609375\n",
      "Epoch [1766], val_loss: 12176.4033\n",
      "gradient norm: 1566.9051208496094, minimum ratio: 0.734375\n",
      "Epoch [1767], val_loss: 12200.9141\n",
      "gradient norm: 1569.1226654052734, minimum ratio: 0.7421875\n",
      "Epoch [1768], val_loss: 12225.4570\n",
      "gradient norm: 1571.3421020507812, minimum ratio: 0.7578125\n",
      "Epoch [1769], val_loss: 12250.0371\n",
      "gradient norm: 1573.5176849365234, minimum ratio: 0.73828125\n",
      "Epoch [1770], val_loss: 12274.6514\n",
      "gradient norm: 1575.7148132324219, minimum ratio: 0.7421875\n",
      "Epoch [1771], val_loss: 12299.2998\n",
      "gradient norm: 1577.8885498046875, minimum ratio: 0.7578125\n",
      "Epoch [1772], val_loss: 12323.9854\n",
      "gradient norm: 1580.115478515625, minimum ratio: 0.7265625\n",
      "Epoch [1773], val_loss: 12348.7041\n",
      "gradient norm: 1582.3199462890625, minimum ratio: 0.76171875\n",
      "Epoch [1774], val_loss: 12373.4600\n",
      "gradient norm: 1584.5001983642578, minimum ratio: 0.74609375\n",
      "Epoch [1775], val_loss: 12398.2510\n",
      "gradient norm: 1586.6752166748047, minimum ratio: 0.7578125\n",
      "Epoch [1776], val_loss: 12423.0771\n",
      "gradient norm: 1588.8082427978516, minimum ratio: 0.74609375\n",
      "Epoch [1777], val_loss: 12447.9395\n",
      "gradient norm: 1590.9654998779297, minimum ratio: 0.7734375\n",
      "Epoch [1778], val_loss: 12472.8369\n",
      "gradient norm: 1593.1697082519531, minimum ratio: 0.76953125\n",
      "Epoch [1779], val_loss: 12497.7705\n",
      "gradient norm: 1595.3822937011719, minimum ratio: 0.75\n",
      "Epoch [1780], val_loss: 12522.7383\n",
      "gradient norm: 1597.6239929199219, minimum ratio: 0.7421875\n",
      "Epoch [1781], val_loss: 12547.7432\n",
      "gradient norm: 1599.7835998535156, minimum ratio: 0.72265625\n",
      "Epoch [1782], val_loss: 12572.7812\n",
      "gradient norm: 1602.0291137695312, minimum ratio: 0.7421875\n",
      "Epoch [1783], val_loss: 12597.8564\n",
      "gradient norm: 1604.2763214111328, minimum ratio: 0.73828125\n",
      "Epoch [1784], val_loss: 12622.9668\n",
      "gradient norm: 1606.5257110595703, minimum ratio: 0.76953125\n",
      "Epoch [1785], val_loss: 12648.1123\n",
      "gradient norm: 1608.7767333984375, minimum ratio: 0.76171875\n",
      "Epoch [1786], val_loss: 12673.2939\n",
      "gradient norm: 1610.9962921142578, minimum ratio: 0.7734375\n",
      "Epoch [1787], val_loss: 12698.5088\n",
      "gradient norm: 1613.1924743652344, minimum ratio: 0.734375\n",
      "Epoch [1788], val_loss: 12723.7617\n",
      "gradient norm: 1615.413803100586, minimum ratio: 0.734375\n",
      "Epoch [1789], val_loss: 12749.0498\n",
      "gradient norm: 1617.5352172851562, minimum ratio: 0.76171875\n",
      "Epoch [1790], val_loss: 12774.3750\n",
      "gradient norm: 1619.7159881591797, minimum ratio: 0.734375\n",
      "Epoch [1791], val_loss: 12799.7344\n",
      "gradient norm: 1621.978271484375, minimum ratio: 0.75390625\n",
      "Epoch [1792], val_loss: 12825.1289\n",
      "gradient norm: 1624.2142486572266, minimum ratio: 0.74609375\n",
      "Epoch [1793], val_loss: 12850.5625\n",
      "gradient norm: 1626.4592590332031, minimum ratio: 0.7578125\n",
      "Epoch [1794], val_loss: 12876.0293\n",
      "gradient norm: 1628.699478149414, minimum ratio: 0.734375\n",
      "Epoch [1795], val_loss: 12901.5332\n",
      "gradient norm: 1630.927001953125, minimum ratio: 0.76953125\n",
      "Epoch [1796], val_loss: 12927.0723\n",
      "gradient norm: 1633.1986389160156, minimum ratio: 0.74609375\n",
      "Epoch [1797], val_loss: 12952.6465\n",
      "gradient norm: 1635.4501647949219, minimum ratio: 0.76171875\n",
      "Epoch [1798], val_loss: 12978.2588\n",
      "gradient norm: 1637.7256774902344, minimum ratio: 0.75\n",
      "Epoch [1799], val_loss: 13003.9053\n",
      "gradient norm: 1639.9256744384766, minimum ratio: 0.75\n",
      "Epoch [1800], val_loss: 13029.5889\n",
      "gradient norm: 1642.1813201904297, minimum ratio: 0.7578125\n",
      "Epoch [1801], val_loss: 13055.3076\n",
      "gradient norm: 1644.4325866699219, minimum ratio: 0.765625\n",
      "Epoch [1802], val_loss: 13081.0635\n",
      "gradient norm: 1646.6858825683594, minimum ratio: 0.7421875\n",
      "Epoch [1803], val_loss: 13106.8535\n",
      "gradient norm: 1648.9473876953125, minimum ratio: 0.75390625\n",
      "Epoch [1804], val_loss: 13132.6797\n",
      "gradient norm: 1651.208267211914, minimum ratio: 0.75390625\n",
      "Epoch [1805], val_loss: 13158.5449\n",
      "gradient norm: 1653.4460754394531, minimum ratio: 0.74609375\n",
      "Epoch [1806], val_loss: 13184.4453\n",
      "gradient norm: 1655.7078857421875, minimum ratio: 0.74609375\n",
      "Epoch [1807], val_loss: 13210.3809\n",
      "gradient norm: 1657.951187133789, minimum ratio: 0.7578125\n",
      "Epoch [1808], val_loss: 13236.3535\n",
      "gradient norm: 1660.2454223632812, minimum ratio: 0.75\n",
      "Epoch [1809], val_loss: 13262.3613\n",
      "gradient norm: 1662.5116729736328, minimum ratio: 0.75390625\n",
      "Epoch [1810], val_loss: 13288.4062\n",
      "gradient norm: 1664.7715454101562, minimum ratio: 0.73828125\n",
      "Epoch [1811], val_loss: 13314.4863\n",
      "gradient norm: 1667.0010528564453, minimum ratio: 0.75\n",
      "Epoch [1812], val_loss: 13340.6055\n",
      "gradient norm: 1669.3033142089844, minimum ratio: 0.76953125\n",
      "Epoch [1813], val_loss: 13366.7588\n",
      "gradient norm: 1671.5735321044922, minimum ratio: 0.7421875\n",
      "Epoch [1814], val_loss: 13392.9492\n",
      "gradient norm: 1673.87939453125, minimum ratio: 0.75\n",
      "Epoch [1815], val_loss: 13419.1748\n",
      "gradient norm: 1676.1435546875, minimum ratio: 0.7578125\n",
      "Epoch [1816], val_loss: 13445.4404\n",
      "gradient norm: 1678.4532318115234, minimum ratio: 0.75\n",
      "Epoch [1817], val_loss: 13471.7363\n",
      "gradient norm: 1680.7644958496094, minimum ratio: 0.73828125\n",
      "Epoch [1818], val_loss: 13498.0713\n",
      "gradient norm: 1682.9879913330078, minimum ratio: 0.7578125\n",
      "Epoch [1819], val_loss: 13524.4443\n",
      "gradient norm: 1685.2562103271484, minimum ratio: 0.7421875\n",
      "Epoch [1820], val_loss: 13550.8535\n",
      "gradient norm: 1687.5145568847656, minimum ratio: 0.76171875\n",
      "Epoch [1821], val_loss: 13577.3008\n",
      "gradient norm: 1689.7747650146484, minimum ratio: 0.765625\n",
      "Epoch [1822], val_loss: 13603.7822\n",
      "gradient norm: 1692.0956115722656, minimum ratio: 0.7734375\n",
      "Epoch [1823], val_loss: 13630.3027\n",
      "gradient norm: 1694.3890533447266, minimum ratio: 0.75390625\n",
      "Epoch [1824], val_loss: 13656.8564\n",
      "gradient norm: 1696.7135925292969, minimum ratio: 0.74609375\n",
      "Epoch [1825], val_loss: 13683.4502\n",
      "gradient norm: 1699.0066223144531, minimum ratio: 0.765625\n",
      "Epoch [1826], val_loss: 13710.0801\n",
      "gradient norm: 1701.305892944336, minimum ratio: 0.7578125\n",
      "Epoch [1827], val_loss: 13736.7451\n",
      "gradient norm: 1703.6361999511719, minimum ratio: 0.73828125\n",
      "Epoch [1828], val_loss: 13763.4473\n",
      "gradient norm: 1705.8777465820312, minimum ratio: 0.74609375\n",
      "Epoch [1829], val_loss: 13790.1855\n",
      "gradient norm: 1708.1348266601562, minimum ratio: 0.74609375\n",
      "Epoch [1830], val_loss: 13816.9629\n",
      "gradient norm: 1710.4709777832031, minimum ratio: 0.75390625\n",
      "Epoch [1831], val_loss: 13843.7754\n",
      "gradient norm: 1712.8086547851562, minimum ratio: 0.7578125\n",
      "Epoch [1832], val_loss: 13870.6240\n",
      "gradient norm: 1715.0268859863281, minimum ratio: 0.7734375\n",
      "Epoch [1833], val_loss: 13897.5127\n",
      "gradient norm: 1717.3683319091797, minimum ratio: 0.74609375\n",
      "Epoch [1834], val_loss: 13924.4355\n",
      "gradient norm: 1719.7118377685547, minimum ratio: 0.76953125\n",
      "Epoch [1835], val_loss: 13951.3975\n",
      "gradient norm: 1722.0025482177734, minimum ratio: 0.765625\n",
      "Epoch [1836], val_loss: 13978.3955\n",
      "gradient norm: 1724.341552734375, minimum ratio: 0.76171875\n",
      "Epoch [1837], val_loss: 14005.4336\n",
      "gradient norm: 1726.619140625, minimum ratio: 0.7578125\n",
      "Epoch [1838], val_loss: 14032.5029\n",
      "gradient norm: 1728.8899230957031, minimum ratio: 0.76953125\n",
      "Epoch [1839], val_loss: 14059.6152\n",
      "gradient norm: 1731.1703033447266, minimum ratio: 0.73828125\n",
      "Epoch [1840], val_loss: 14086.7637\n",
      "gradient norm: 1733.525146484375, minimum ratio: 0.7578125\n",
      "Epoch [1841], val_loss: 14113.9463\n",
      "gradient norm: 1735.882064819336, minimum ratio: 0.734375\n",
      "Epoch [1842], val_loss: 14141.1680\n",
      "gradient norm: 1738.2081756591797, minimum ratio: 0.7421875\n",
      "Epoch [1843], val_loss: 14168.4297\n",
      "gradient norm: 1740.5690307617188, minimum ratio: 0.76171875\n",
      "Epoch [1844], val_loss: 14195.7246\n",
      "gradient norm: 1742.9068908691406, minimum ratio: 0.75\n",
      "Epoch [1845], val_loss: 14223.0586\n",
      "gradient norm: 1745.1715240478516, minimum ratio: 0.77734375\n",
      "Epoch [1846], val_loss: 14250.4297\n",
      "gradient norm: 1747.5061492919922, minimum ratio: 0.76171875\n",
      "Epoch [1847], val_loss: 14277.8369\n",
      "gradient norm: 1749.7979125976562, minimum ratio: 0.7578125\n",
      "Epoch [1848], val_loss: 14305.2852\n",
      "gradient norm: 1752.1383972167969, minimum ratio: 0.7421875\n",
      "Epoch [1849], val_loss: 14332.7695\n",
      "gradient norm: 1754.4525146484375, minimum ratio: 0.76171875\n",
      "Epoch [1850], val_loss: 14360.2920\n",
      "gradient norm: 1756.7306671142578, minimum ratio: 0.7421875\n",
      "Epoch [1851], val_loss: 14387.8516\n",
      "gradient norm: 1759.1067352294922, minimum ratio: 0.76171875\n",
      "Epoch [1852], val_loss: 14415.4482\n",
      "gradient norm: 1761.4842987060547, minimum ratio: 0.76953125\n",
      "Epoch [1853], val_loss: 14443.0830\n",
      "gradient norm: 1763.8641052246094, minimum ratio: 0.7734375\n",
      "Epoch [1854], val_loss: 14470.7529\n",
      "gradient norm: 1766.2455596923828, minimum ratio: 0.7734375\n",
      "Epoch [1855], val_loss: 14498.4629\n",
      "gradient norm: 1768.6290588378906, minimum ratio: 0.765625\n",
      "Epoch [1856], val_loss: 14526.2100\n",
      "gradient norm: 1770.9819793701172, minimum ratio: 0.734375\n",
      "Epoch [1857], val_loss: 14553.9912\n",
      "gradient norm: 1773.330795288086, minimum ratio: 0.75390625\n",
      "Epoch [1858], val_loss: 14581.8135\n",
      "gradient norm: 1775.6923828125, minimum ratio: 0.75390625\n",
      "Epoch [1859], val_loss: 14609.6699\n",
      "gradient norm: 1778.0635833740234, minimum ratio: 0.75390625\n",
      "Epoch [1860], val_loss: 14637.5703\n",
      "gradient norm: 1780.39501953125, minimum ratio: 0.75\n",
      "Epoch [1861], val_loss: 14665.5049\n",
      "gradient norm: 1782.7267456054688, minimum ratio: 0.76171875\n",
      "Epoch [1862], val_loss: 14693.4766\n",
      "gradient norm: 1785.0921936035156, minimum ratio: 0.734375\n",
      "Epoch [1863], val_loss: 14721.4863\n",
      "gradient norm: 1787.4910888671875, minimum ratio: 0.75390625\n",
      "Epoch [1864], val_loss: 14749.5371\n",
      "gradient norm: 1789.8628540039062, minimum ratio: 0.76171875\n",
      "Epoch [1865], val_loss: 14777.6221\n",
      "gradient norm: 1792.2655944824219, minimum ratio: 0.76171875\n",
      "Epoch [1866], val_loss: 14805.7471\n",
      "gradient norm: 1794.6701202392578, minimum ratio: 0.76171875\n",
      "Epoch [1867], val_loss: 14833.9102\n",
      "gradient norm: 1797.0138854980469, minimum ratio: 0.75\n",
      "Epoch [1868], val_loss: 14862.1084\n",
      "gradient norm: 1799.3930969238281, minimum ratio: 0.765625\n",
      "Epoch [1869], val_loss: 14890.3447\n",
      "gradient norm: 1801.7716979980469, minimum ratio: 0.73828125\n",
      "Epoch [1870], val_loss: 14918.6221\n",
      "gradient norm: 1804.1105041503906, minimum ratio: 0.76953125\n",
      "Epoch [1871], val_loss: 14946.9336\n",
      "gradient norm: 1806.4066009521484, minimum ratio: 0.76171875\n",
      "Epoch [1872], val_loss: 14975.2871\n",
      "gradient norm: 1808.7889556884766, minimum ratio: 0.75390625\n",
      "Epoch [1873], val_loss: 15003.6787\n",
      "gradient norm: 1811.2068634033203, minimum ratio: 0.73828125\n",
      "Epoch [1874], val_loss: 15032.1064\n",
      "gradient norm: 1813.5874481201172, minimum ratio: 0.765625\n",
      "Epoch [1875], val_loss: 15060.5732\n",
      "gradient norm: 1815.9197998046875, minimum ratio: 0.74609375\n",
      "Epoch [1876], val_loss: 15089.0801\n",
      "gradient norm: 1818.3085174560547, minimum ratio: 0.75\n",
      "Epoch [1877], val_loss: 15117.6250\n",
      "gradient norm: 1820.661392211914, minimum ratio: 0.74609375\n",
      "Epoch [1878], val_loss: 15146.2070\n",
      "gradient norm: 1823.0430145263672, minimum ratio: 0.7578125\n",
      "Epoch [1879], val_loss: 15174.8281\n",
      "gradient norm: 1825.4723358154297, minimum ratio: 0.75\n",
      "Epoch [1880], val_loss: 15203.4883\n",
      "gradient norm: 1827.8623962402344, minimum ratio: 0.765625\n",
      "Epoch [1881], val_loss: 15232.1885\n",
      "gradient norm: 1830.1640167236328, minimum ratio: 0.7578125\n",
      "Epoch [1882], val_loss: 15260.9248\n",
      "gradient norm: 1832.592300415039, minimum ratio: 0.74609375\n",
      "Epoch [1883], val_loss: 15289.7002\n",
      "gradient norm: 1835.0294494628906, minimum ratio: 0.76171875\n",
      "Epoch [1884], val_loss: 15318.5137\n",
      "gradient norm: 1837.4290161132812, minimum ratio: 0.7578125\n",
      "Epoch [1885], val_loss: 15347.3682\n",
      "gradient norm: 1839.8215942382812, minimum ratio: 0.74609375\n",
      "Epoch [1886], val_loss: 15376.2598\n",
      "gradient norm: 1842.2645416259766, minimum ratio: 0.75\n",
      "Epoch [1887], val_loss: 15405.1904\n",
      "gradient norm: 1844.6737518310547, minimum ratio: 0.75\n",
      "Epoch [1888], val_loss: 15434.1582\n",
      "gradient norm: 1847.1204681396484, minimum ratio: 0.75390625\n",
      "Epoch [1889], val_loss: 15463.1631\n",
      "gradient norm: 1849.539566040039, minimum ratio: 0.75\n",
      "Epoch [1890], val_loss: 15492.2100\n",
      "gradient norm: 1851.9900817871094, minimum ratio: 0.7578125\n",
      "Epoch [1891], val_loss: 15521.2900\n",
      "gradient norm: 1854.3563385009766, minimum ratio: 0.7421875\n",
      "Epoch [1892], val_loss: 15550.4150\n",
      "gradient norm: 1856.8108978271484, minimum ratio: 0.765625\n",
      "Epoch [1893], val_loss: 15579.5752\n",
      "gradient norm: 1859.193832397461, minimum ratio: 0.7578125\n",
      "Epoch [1894], val_loss: 15608.7754\n",
      "gradient norm: 1861.6521911621094, minimum ratio: 0.75\n",
      "Epoch [1895], val_loss: 15638.0146\n",
      "gradient norm: 1864.0811309814453, minimum ratio: 0.734375\n",
      "Epoch [1896], val_loss: 15667.2900\n",
      "gradient norm: 1866.5204467773438, minimum ratio: 0.765625\n",
      "Epoch [1897], val_loss: 15696.6064\n",
      "gradient norm: 1868.966796875, minimum ratio: 0.7265625\n",
      "Epoch [1898], val_loss: 15725.9619\n",
      "gradient norm: 1871.3941955566406, minimum ratio: 0.75390625\n",
      "Epoch [1899], val_loss: 15755.3564\n",
      "gradient norm: 1873.7943115234375, minimum ratio: 0.77734375\n",
      "Epoch [1900], val_loss: 15784.7900\n",
      "gradient norm: 1876.2642669677734, minimum ratio: 0.74609375\n",
      "Epoch [1901], val_loss: 15814.2598\n",
      "gradient norm: 1878.7359771728516, minimum ratio: 0.7421875\n",
      "Epoch [1902], val_loss: 15843.7715\n",
      "gradient norm: 1881.1878509521484, minimum ratio: 0.76171875\n",
      "Epoch [1903], val_loss: 15873.3203\n",
      "gradient norm: 1883.6579132080078, minimum ratio: 0.74609375\n",
      "Epoch [1904], val_loss: 15902.9082\n",
      "gradient norm: 1886.0999145507812, minimum ratio: 0.7578125\n",
      "Epoch [1905], val_loss: 15932.5371\n",
      "gradient norm: 1888.4583892822266, minimum ratio: 0.73828125\n",
      "Epoch [1906], val_loss: 15962.2021\n",
      "gradient norm: 1890.9142150878906, minimum ratio: 0.7265625\n",
      "Epoch [1907], val_loss: 15991.9102\n",
      "gradient norm: 1893.3179473876953, minimum ratio: 0.765625\n",
      "Epoch [1908], val_loss: 16021.6562\n",
      "gradient norm: 1895.7747192382812, minimum ratio: 0.74609375\n",
      "Epoch [1909], val_loss: 16051.4414\n",
      "gradient norm: 1898.2133331298828, minimum ratio: 0.73046875\n",
      "Epoch [1910], val_loss: 16081.2666\n",
      "gradient norm: 1900.6346893310547, minimum ratio: 0.765625\n",
      "Epoch [1911], val_loss: 16111.1299\n",
      "gradient norm: 1903.0902404785156, minimum ratio: 0.74609375\n",
      "Epoch [1912], val_loss: 16141.0312\n",
      "gradient norm: 1905.5832061767578, minimum ratio: 0.7578125\n",
      "Epoch [1913], val_loss: 16170.9766\n",
      "gradient norm: 1908.0401458740234, minimum ratio: 0.75390625\n",
      "Epoch [1914], val_loss: 16200.9600\n",
      "gradient norm: 1910.444091796875, minimum ratio: 0.74609375\n",
      "Epoch [1915], val_loss: 16230.9805\n",
      "gradient norm: 1912.8635559082031, minimum ratio: 0.75\n",
      "Epoch [1916], val_loss: 16261.0420\n",
      "gradient norm: 1915.3477783203125, minimum ratio: 0.73828125\n",
      "Epoch [1917], val_loss: 16291.1445\n",
      "gradient norm: 1917.8045196533203, minimum ratio: 0.76171875\n",
      "Epoch [1918], val_loss: 16321.2832\n",
      "gradient norm: 1920.2746276855469, minimum ratio: 0.75\n",
      "Epoch [1919], val_loss: 16351.4648\n",
      "gradient norm: 1922.7492980957031, minimum ratio: 0.75390625\n",
      "Epoch [1920], val_loss: 16381.6846\n",
      "gradient norm: 1925.2247467041016, minimum ratio: 0.7265625\n",
      "Epoch [1921], val_loss: 16411.9434\n",
      "gradient norm: 1927.6414794921875, minimum ratio: 0.7265625\n",
      "Epoch [1922], val_loss: 16442.2441\n",
      "gradient norm: 1930.122573852539, minimum ratio: 0.7421875\n",
      "Epoch [1923], val_loss: 16472.5820\n",
      "gradient norm: 1932.5393981933594, minimum ratio: 0.73828125\n",
      "Epoch [1924], val_loss: 16502.9609\n",
      "gradient norm: 1935.0217895507812, minimum ratio: 0.734375\n",
      "Epoch [1925], val_loss: 16533.3809\n",
      "gradient norm: 1937.5400848388672, minimum ratio: 0.75390625\n",
      "Epoch [1926], val_loss: 16563.8379\n",
      "gradient norm: 1940.026626586914, minimum ratio: 0.734375\n",
      "Epoch [1927], val_loss: 16594.3398\n",
      "gradient norm: 1942.5106048583984, minimum ratio: 0.7734375\n",
      "Epoch [1928], val_loss: 16624.8789\n",
      "gradient norm: 1945.0347290039062, minimum ratio: 0.76171875\n",
      "Epoch [1929], val_loss: 16655.4551\n",
      "gradient norm: 1947.5609130859375, minimum ratio: 0.76953125\n",
      "Epoch [1930], val_loss: 16686.0723\n",
      "gradient norm: 1950.0889434814453, minimum ratio: 0.74609375\n",
      "Epoch [1931], val_loss: 16716.7305\n",
      "gradient norm: 1952.5745697021484, minimum ratio: 0.74609375\n",
      "Epoch [1932], val_loss: 16747.4258\n",
      "gradient norm: 1955.0718841552734, minimum ratio: 0.734375\n",
      "Epoch [1933], val_loss: 16778.1641\n",
      "gradient norm: 1957.5905456542969, minimum ratio: 0.76171875\n",
      "Epoch [1934], val_loss: 16808.9375\n",
      "gradient norm: 1960.0945739746094, minimum ratio: 0.734375\n",
      "Epoch [1935], val_loss: 16839.7559\n",
      "gradient norm: 1962.5908508300781, minimum ratio: 0.75390625\n",
      "Epoch [1936], val_loss: 16870.6133\n",
      "gradient norm: 1965.130615234375, minimum ratio: 0.7578125\n",
      "Epoch [1937], val_loss: 16901.5078\n",
      "gradient norm: 1967.6463928222656, minimum ratio: 0.7578125\n",
      "Epoch [1938], val_loss: 16932.4453\n",
      "gradient norm: 1970.0340423583984, minimum ratio: 0.75390625\n",
      "Epoch [1939], val_loss: 16963.4219\n",
      "gradient norm: 1972.5190124511719, minimum ratio: 0.74609375\n",
      "Epoch [1940], val_loss: 16994.4395\n",
      "gradient norm: 1975.0314331054688, minimum ratio: 0.73828125\n",
      "Epoch [1941], val_loss: 17025.4980\n",
      "gradient norm: 1977.580825805664, minimum ratio: 0.74609375\n",
      "Epoch [1942], val_loss: 17056.5977\n",
      "gradient norm: 1980.1229400634766, minimum ratio: 0.73046875\n",
      "Epoch [1943], val_loss: 17087.7363\n",
      "gradient norm: 1982.6762390136719, minimum ratio: 0.75\n",
      "Epoch [1944], val_loss: 17118.9160\n",
      "gradient norm: 1985.2315979003906, minimum ratio: 0.76171875\n",
      "Epoch [1945], val_loss: 17150.1367\n",
      "gradient norm: 1987.7890014648438, minimum ratio: 0.7421875\n",
      "Epoch [1946], val_loss: 17181.3926\n",
      "gradient norm: 1990.2086029052734, minimum ratio: 0.75390625\n",
      "Epoch [1947], val_loss: 17212.6934\n",
      "gradient norm: 1992.7696228027344, minimum ratio: 0.76171875\n",
      "Epoch [1948], val_loss: 17244.0312\n",
      "gradient norm: 1995.2517547607422, minimum ratio: 0.73046875\n",
      "Epoch [1949], val_loss: 17275.4121\n",
      "gradient norm: 1997.8164672851562, minimum ratio: 0.74609375\n",
      "Epoch [1950], val_loss: 17306.8359\n",
      "gradient norm: 2000.3839416503906, minimum ratio: 0.76953125\n",
      "Epoch [1951], val_loss: 17338.2949\n",
      "gradient norm: 2002.8574676513672, minimum ratio: 0.74609375\n",
      "Epoch [1952], val_loss: 17369.7949\n",
      "gradient norm: 2005.396255493164, minimum ratio: 0.76171875\n",
      "Epoch [1953], val_loss: 17401.3359\n",
      "gradient norm: 2007.9019927978516, minimum ratio: 0.75390625\n",
      "Epoch [1954], val_loss: 17432.9219\n",
      "gradient norm: 2010.4768829345703, minimum ratio: 0.73046875\n",
      "Epoch [1955], val_loss: 17464.5469\n",
      "gradient norm: 2012.9020538330078, minimum ratio: 0.76171875\n",
      "Epoch [1956], val_loss: 17496.2129\n",
      "gradient norm: 2015.4169311523438, minimum ratio: 0.75390625\n",
      "Epoch [1957], val_loss: 17527.9180\n",
      "gradient norm: 2017.9615173339844, minimum ratio: 0.7734375\n",
      "Epoch [1958], val_loss: 17559.6660\n",
      "gradient norm: 2020.480239868164, minimum ratio: 0.76171875\n",
      "Epoch [1959], val_loss: 17591.4531\n",
      "gradient norm: 2023.0105743408203, minimum ratio: 0.7421875\n",
      "Epoch [1960], val_loss: 17623.2832\n",
      "gradient norm: 2025.5495910644531, minimum ratio: 0.7421875\n",
      "Epoch [1961], val_loss: 17655.1543\n",
      "gradient norm: 2028.138198852539, minimum ratio: 0.76171875\n",
      "Epoch [1962], val_loss: 17687.0625\n",
      "gradient norm: 2030.7286682128906, minimum ratio: 0.7421875\n",
      "Epoch [1963], val_loss: 17719.0137\n",
      "gradient norm: 2033.288803100586, minimum ratio: 0.734375\n",
      "Epoch [1964], val_loss: 17751.0078\n",
      "gradient norm: 2035.8832397460938, minimum ratio: 0.73828125\n",
      "Epoch [1965], val_loss: 17783.0410\n",
      "gradient norm: 2038.4075164794922, minimum ratio: 0.78515625\n",
      "Epoch [1966], val_loss: 17815.1133\n",
      "gradient norm: 2041.0059967041016, minimum ratio: 0.75\n",
      "Epoch [1967], val_loss: 17847.2285\n",
      "gradient norm: 2043.5687408447266, minimum ratio: 0.76171875\n",
      "Epoch [1968], val_loss: 17879.3828\n",
      "gradient norm: 2046.0609283447266, minimum ratio: 0.75\n",
      "Epoch [1969], val_loss: 17911.5840\n",
      "gradient norm: 2048.6182556152344, minimum ratio: 0.75390625\n",
      "Epoch [1970], val_loss: 17943.8223\n",
      "gradient norm: 2051.173126220703, minimum ratio: 0.76171875\n",
      "Epoch [1971], val_loss: 17976.1016\n",
      "gradient norm: 2053.7813415527344, minimum ratio: 0.76953125\n",
      "Epoch [1972], val_loss: 18008.4219\n",
      "gradient norm: 2056.386749267578, minimum ratio: 0.765625\n",
      "Epoch [1973], val_loss: 18040.7891\n",
      "gradient norm: 2058.957992553711, minimum ratio: 0.765625\n",
      "Epoch [1974], val_loss: 18073.1875\n",
      "gradient norm: 2061.4276123046875, minimum ratio: 0.76171875\n",
      "Epoch [1975], val_loss: 18105.6348\n",
      "gradient norm: 2064.043746948242, minimum ratio: 0.75390625\n",
      "Epoch [1976], val_loss: 18138.1191\n",
      "gradient norm: 2066.6239013671875, minimum ratio: 0.74609375\n",
      "Epoch [1977], val_loss: 18170.6465\n",
      "gradient norm: 2069.243423461914, minimum ratio: 0.75390625\n",
      "Epoch [1978], val_loss: 18203.2188\n",
      "gradient norm: 2071.8655548095703, minimum ratio: 0.75\n",
      "Epoch [1979], val_loss: 18235.8262\n",
      "gradient norm: 2074.489517211914, minimum ratio: 0.7578125\n",
      "Epoch [1980], val_loss: 18268.4785\n",
      "gradient norm: 2077.0094451904297, minimum ratio: 0.73828125\n",
      "Epoch [1981], val_loss: 18301.1699\n",
      "gradient norm: 2079.637451171875, minimum ratio: 0.73046875\n",
      "Epoch [1982], val_loss: 18333.9043\n",
      "gradient norm: 2082.2242889404297, minimum ratio: 0.73828125\n",
      "Epoch [1983], val_loss: 18366.6816\n",
      "gradient norm: 2084.819793701172, minimum ratio: 0.75\n",
      "Epoch [1984], val_loss: 18399.5000\n",
      "gradient norm: 2087.366653442383, minimum ratio: 0.765625\n",
      "Epoch [1985], val_loss: 18432.3574\n",
      "gradient norm: 2089.966079711914, minimum ratio: 0.75\n",
      "Epoch [1986], val_loss: 18465.2578\n",
      "gradient norm: 2092.6038665771484, minimum ratio: 0.74609375\n",
      "Epoch [1987], val_loss: 18498.2031\n",
      "gradient norm: 2095.243911743164, minimum ratio: 0.765625\n",
      "Epoch [1988], val_loss: 18531.1875\n",
      "gradient norm: 2097.8351135253906, minimum ratio: 0.7734375\n",
      "Epoch [1989], val_loss: 18564.2129\n",
      "gradient norm: 2100.4310913085938, minimum ratio: 0.765625\n",
      "Epoch [1990], val_loss: 18597.2793\n",
      "gradient norm: 2102.8900756835938, minimum ratio: 0.75\n",
      "Epoch [1991], val_loss: 18630.3887\n",
      "gradient norm: 2105.5048828125, minimum ratio: 0.74609375\n",
      "Epoch [1992], val_loss: 18663.5449\n",
      "gradient norm: 2108.1546020507812, minimum ratio: 0.7578125\n",
      "Epoch [1993], val_loss: 18696.7324\n",
      "gradient norm: 2110.806167602539, minimum ratio: 0.73828125\n",
      "Epoch [1994], val_loss: 18729.9707\n",
      "gradient norm: 2113.406967163086, minimum ratio: 0.7421875\n",
      "Epoch [1995], val_loss: 18763.2441\n",
      "gradient norm: 2116.0283203125, minimum ratio: 0.7265625\n",
      "Epoch [1996], val_loss: 18796.5625\n",
      "gradient norm: 2118.6859741210938, minimum ratio: 0.75\n",
      "Epoch [1997], val_loss: 18829.9258\n",
      "gradient norm: 2121.2483825683594, minimum ratio: 0.7578125\n",
      "Epoch [1998], val_loss: 18863.3242\n",
      "gradient norm: 2123.8836059570312, minimum ratio: 0.75390625\n",
      "Epoch [1999], val_loss: 18896.7676\n"
     ]
    }
   ],
   "source": [
    "history_1,grad_norm_1,model_1  = fit(num_epochs, lr, model_1, data_loader, criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9ba1f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'grad_norm': 2.383659452199936, 'ratio': 0.703125},\n",
       " 1: {'grad_norm': 2.450784344226122, 'ratio': 0.6953125},\n",
       " 2: {'grad_norm': 2.5180913619697094, 'ratio': 0.72265625},\n",
       " 3: {'grad_norm': 2.5854485370218754, 'ratio': 0.71484375},\n",
       " 4: {'grad_norm': 2.652941904962063, 'ratio': 0.7421875},\n",
       " 5: {'grad_norm': 2.720516160130501, 'ratio': 0.69921875},\n",
       " 6: {'grad_norm': 2.7881786674261093, 'ratio': 0.734375},\n",
       " 7: {'grad_norm': 2.8559238128364086, 'ratio': 0.71875},\n",
       " 8: {'grad_norm': 2.9237753264606, 'ratio': 0.73828125},\n",
       " 9: {'grad_norm': 2.991733245551586, 'ratio': 0.73046875},\n",
       " 10: {'grad_norm': 3.059774350374937, 'ratio': 0.75390625},\n",
       " 11: {'grad_norm': 3.127897422760725, 'ratio': 0.74609375},\n",
       " 12: {'grad_norm': 3.19608923047781, 'ratio': 0.71875},\n",
       " 13: {'grad_norm': 3.2643523551523685, 'ratio': 0.7265625},\n",
       " 14: {'grad_norm': 3.3327057994902134, 'ratio': 0.73046875},\n",
       " 15: {'grad_norm': 3.4011525362730026, 'ratio': 0.71875},\n",
       " 16: {'grad_norm': 3.469749730080366, 'ratio': 0.72265625},\n",
       " 17: {'grad_norm': 3.5383895710110664, 'ratio': 0.73046875},\n",
       " 18: {'grad_norm': 3.6071263402700424, 'ratio': 0.7421875},\n",
       " 19: {'grad_norm': 3.675946056842804, 'ratio': 0.75390625},\n",
       " 20: {'grad_norm': 3.7448699697852135, 'ratio': 0.7265625},\n",
       " 21: {'grad_norm': 3.8138678669929504, 'ratio': 0.73046875},\n",
       " 22: {'grad_norm': 3.883002392947674, 'ratio': 0.7421875},\n",
       " 23: {'grad_norm': 3.952174298465252, 'ratio': 0.7265625},\n",
       " 24: {'grad_norm': 4.0215198546648026, 'ratio': 0.71875},\n",
       " 25: {'grad_norm': 4.090937778353691, 'ratio': 0.7265625},\n",
       " 26: {'grad_norm': 4.16046417504549, 'ratio': 0.7265625},\n",
       " 27: {'grad_norm': 4.230110287666321, 'ratio': 0.72265625},\n",
       " 28: {'grad_norm': 4.299836881458759, 'ratio': 0.7265625},\n",
       " 29: {'grad_norm': 4.369661584496498, 'ratio': 0.71875},\n",
       " 30: {'grad_norm': 4.439679600298405, 'ratio': 0.74609375},\n",
       " 31: {'grad_norm': 4.509776666760445, 'ratio': 0.73828125},\n",
       " 32: {'grad_norm': 4.579967133700848, 'ratio': 0.73828125},\n",
       " 33: {'grad_norm': 4.650265634059906, 'ratio': 0.71875},\n",
       " 34: {'grad_norm': 4.720716953277588, 'ratio': 0.75390625},\n",
       " 35: {'grad_norm': 4.791325636208057, 'ratio': 0.72265625},\n",
       " 36: {'grad_norm': 4.862018249928951, 'ratio': 0.7578125},\n",
       " 37: {'grad_norm': 4.932861939072609, 'ratio': 0.74609375},\n",
       " 38: {'grad_norm': 5.003791645169258, 'ratio': 0.734375},\n",
       " 39: {'grad_norm': 5.074883036315441, 'ratio': 0.7578125},\n",
       " 40: {'grad_norm': 5.146124266088009, 'ratio': 0.71484375},\n",
       " 41: {'grad_norm': 5.2174337431788445, 'ratio': 0.75390625},\n",
       " 42: {'grad_norm': 5.288908198475838, 'ratio': 0.7265625},\n",
       " 43: {'grad_norm': 5.360563986003399, 'ratio': 0.72265625},\n",
       " 44: {'grad_norm': 5.432384513318539, 'ratio': 0.7109375},\n",
       " 45: {'grad_norm': 5.504337921738625, 'ratio': 0.76953125},\n",
       " 46: {'grad_norm': 5.5764153227210045, 'ratio': 0.73046875},\n",
       " 47: {'grad_norm': 5.648655630648136, 'ratio': 0.73828125},\n",
       " 48: {'grad_norm': 5.721155673265457, 'ratio': 0.7421875},\n",
       " 49: {'grad_norm': 5.793700635433197, 'ratio': 0.71484375},\n",
       " 50: {'grad_norm': 5.8664218708872795, 'ratio': 0.7578125},\n",
       " 51: {'grad_norm': 5.939337119460106, 'ratio': 0.734375},\n",
       " 52: {'grad_norm': 6.012430056929588, 'ratio': 0.73828125},\n",
       " 53: {'grad_norm': 6.085621543228626, 'ratio': 0.73828125},\n",
       " 54: {'grad_norm': 6.158989205956459, 'ratio': 0.75},\n",
       " 55: {'grad_norm': 6.232504948973656, 'ratio': 0.73828125},\n",
       " 56: {'grad_norm': 6.306240797042847, 'ratio': 0.74609375},\n",
       " 57: {'grad_norm': 6.380253925919533, 'ratio': 0.72265625},\n",
       " 58: {'grad_norm': 6.454407215118408, 'ratio': 0.734375},\n",
       " 59: {'grad_norm': 6.528844386339188, 'ratio': 0.75},\n",
       " 60: {'grad_norm': 6.603374347090721, 'ratio': 0.75},\n",
       " 61: {'grad_norm': 6.6781047731637955, 'ratio': 0.71484375},\n",
       " 62: {'grad_norm': 6.75303815305233, 'ratio': 0.7421875},\n",
       " 63: {'grad_norm': 6.828132018446922, 'ratio': 0.73828125},\n",
       " 64: {'grad_norm': 6.903432637453079, 'ratio': 0.734375},\n",
       " 65: {'grad_norm': 6.978922516107559, 'ratio': 0.73828125},\n",
       " 66: {'grad_norm': 7.054721638560295, 'ratio': 0.7578125},\n",
       " 67: {'grad_norm': 7.130680337548256, 'ratio': 0.73828125},\n",
       " 68: {'grad_norm': 7.206823140382767, 'ratio': 0.74609375},\n",
       " 69: {'grad_norm': 7.283098071813583, 'ratio': 0.7421875},\n",
       " 70: {'grad_norm': 7.359678953886032, 'ratio': 0.72265625},\n",
       " 71: {'grad_norm': 7.4364058673381805, 'ratio': 0.7421875},\n",
       " 72: {'grad_norm': 7.513354405760765, 'ratio': 0.734375},\n",
       " 73: {'grad_norm': 7.590603247284889, 'ratio': 0.7265625},\n",
       " 74: {'grad_norm': 7.668080046772957, 'ratio': 0.7578125},\n",
       " 75: {'grad_norm': 7.7456483989953995, 'ratio': 0.73046875},\n",
       " 76: {'grad_norm': 7.823465064167976, 'ratio': 0.73828125},\n",
       " 77: {'grad_norm': 7.901533126831055, 'ratio': 0.7578125},\n",
       " 78: {'grad_norm': 7.979807734489441, 'ratio': 0.7578125},\n",
       " 79: {'grad_norm': 8.058434888720512, 'ratio': 0.74609375},\n",
       " 80: {'grad_norm': 8.137294009327888, 'ratio': 0.75390625},\n",
       " 81: {'grad_norm': 8.21619063615799, 'ratio': 0.73828125},\n",
       " 82: {'grad_norm': 8.295537948608398, 'ratio': 0.74609375},\n",
       " 83: {'grad_norm': 8.374999761581421, 'ratio': 0.75},\n",
       " 84: {'grad_norm': 8.454612106084824, 'ratio': 0.73828125},\n",
       " 85: {'grad_norm': 8.534583747386932, 'ratio': 0.76171875},\n",
       " 86: {'grad_norm': 8.614812359213829, 'ratio': 0.7734375},\n",
       " 87: {'grad_norm': 8.695441499352455, 'ratio': 0.7421875},\n",
       " 88: {'grad_norm': 8.77610468864441, 'ratio': 0.74609375},\n",
       " 89: {'grad_norm': 8.857058107852936, 'ratio': 0.734375},\n",
       " 90: {'grad_norm': 8.938151314854622, 'ratio': 0.76953125},\n",
       " 91: {'grad_norm': 9.019691109657288, 'ratio': 0.76171875},\n",
       " 92: {'grad_norm': 9.10153517127037, 'ratio': 0.73828125},\n",
       " 93: {'grad_norm': 9.183568730950356, 'ratio': 0.76171875},\n",
       " 94: {'grad_norm': 9.26579201221466, 'ratio': 0.7734375},\n",
       " 95: {'grad_norm': 9.348306953907013, 'ratio': 0.73828125},\n",
       " 96: {'grad_norm': 9.431285560131073, 'ratio': 0.7421875},\n",
       " 97: {'grad_norm': 9.514306470751762, 'ratio': 0.765625},\n",
       " 98: {'grad_norm': 9.597608998417854, 'ratio': 0.71875},\n",
       " 99: {'grad_norm': 9.681269243359566, 'ratio': 0.7421875},\n",
       " 100: {'grad_norm': 9.76526328921318, 'ratio': 0.75},\n",
       " 101: {'grad_norm': 9.849454298615456, 'ratio': 0.74609375},\n",
       " 102: {'grad_norm': 9.933882862329483, 'ratio': 0.74609375},\n",
       " 103: {'grad_norm': 10.018625229597092, 'ratio': 0.75},\n",
       " 104: {'grad_norm': 10.103926047682762, 'ratio': 0.75},\n",
       " 105: {'grad_norm': 10.189312726259232, 'ratio': 0.75},\n",
       " 106: {'grad_norm': 10.274968177080154, 'ratio': 0.74609375},\n",
       " 107: {'grad_norm': 10.36083722114563, 'ratio': 0.765625},\n",
       " 108: {'grad_norm': 10.44721993803978, 'ratio': 0.73828125},\n",
       " 109: {'grad_norm': 10.533587276935577, 'ratio': 0.73828125},\n",
       " 110: {'grad_norm': 10.620439499616623, 'ratio': 0.75390625},\n",
       " 111: {'grad_norm': 10.707615673542023, 'ratio': 0.75},\n",
       " 112: {'grad_norm': 10.795046359300613, 'ratio': 0.7578125},\n",
       " 113: {'grad_norm': 10.882850974798203, 'ratio': 0.7421875},\n",
       " 114: {'grad_norm': 10.97094014286995, 'ratio': 0.76953125},\n",
       " 115: {'grad_norm': 11.059223294258118, 'ratio': 0.7578125},\n",
       " 116: {'grad_norm': 11.14782640337944, 'ratio': 0.75},\n",
       " 117: {'grad_norm': 11.237056255340576, 'ratio': 0.76171875},\n",
       " 118: {'grad_norm': 11.326317727565765, 'ratio': 0.74609375},\n",
       " 119: {'grad_norm': 11.41589805483818, 'ratio': 0.73046875},\n",
       " 120: {'grad_norm': 11.505623161792755, 'ratio': 0.75390625},\n",
       " 121: {'grad_norm': 11.59595301747322, 'ratio': 0.76953125},\n",
       " 122: {'grad_norm': 11.686492949724197, 'ratio': 0.75390625},\n",
       " 123: {'grad_norm': 11.777477383613586, 'ratio': 0.7734375},\n",
       " 124: {'grad_norm': 11.868691831827164, 'ratio': 0.734375},\n",
       " 125: {'grad_norm': 11.960382729768753, 'ratio': 0.7578125},\n",
       " 126: {'grad_norm': 12.052390336990356, 'ratio': 0.7578125},\n",
       " 127: {'grad_norm': 12.144598722457886, 'ratio': 0.73828125},\n",
       " 128: {'grad_norm': 12.237121313810349, 'ratio': 0.734375},\n",
       " 129: {'grad_norm': 12.330158650875092, 'ratio': 0.76953125},\n",
       " 130: {'grad_norm': 12.42351296544075, 'ratio': 0.76953125},\n",
       " 131: {'grad_norm': 12.51720416545868, 'ratio': 0.75},\n",
       " 132: {'grad_norm': 12.611348956823349, 'ratio': 0.76171875},\n",
       " 133: {'grad_norm': 12.705711126327515, 'ratio': 0.7421875},\n",
       " 134: {'grad_norm': 12.800436109304428, 'ratio': 0.7578125},\n",
       " 135: {'grad_norm': 12.895476818084717, 'ratio': 0.75390625},\n",
       " 136: {'grad_norm': 12.990723073482513, 'ratio': 0.76171875},\n",
       " 137: {'grad_norm': 13.086650788784027, 'ratio': 0.7265625},\n",
       " 138: {'grad_norm': 13.182730466127396, 'ratio': 0.75},\n",
       " 139: {'grad_norm': 13.279376536607742, 'ratio': 0.7421875},\n",
       " 140: {'grad_norm': 13.376194804906845, 'ratio': 0.75390625},\n",
       " 141: {'grad_norm': 13.473459720611572, 'ratio': 0.75390625},\n",
       " 142: {'grad_norm': 13.57116910815239, 'ratio': 0.7578125},\n",
       " 143: {'grad_norm': 13.669098436832428, 'ratio': 0.75390625},\n",
       " 144: {'grad_norm': 13.767444670200348, 'ratio': 0.73828125},\n",
       " 145: {'grad_norm': 13.866072177886963, 'ratio': 0.74609375},\n",
       " 146: {'grad_norm': 13.965332984924316, 'ratio': 0.76171875},\n",
       " 147: {'grad_norm': 14.064931064844131, 'ratio': 0.7421875},\n",
       " 148: {'grad_norm': 14.164690375328064, 'ratio': 0.74609375},\n",
       " 149: {'grad_norm': 14.265070587396622, 'ratio': 0.7734375},\n",
       " 150: {'grad_norm': 14.36589965224266, 'ratio': 0.75},\n",
       " 151: {'grad_norm': 14.467091470956802, 'ratio': 0.7421875},\n",
       " 152: {'grad_norm': 14.568401247262955, 'ratio': 0.74609375},\n",
       " 153: {'grad_norm': 14.670134007930756, 'ratio': 0.75390625},\n",
       " 154: {'grad_norm': 14.772428214550018, 'ratio': 0.74609375},\n",
       " 155: {'grad_norm': 14.874686151742935, 'ratio': 0.765625},\n",
       " 156: {'grad_norm': 14.977690786123276, 'ratio': 0.74609375},\n",
       " 157: {'grad_norm': 15.081046640872955, 'ratio': 0.7421875},\n",
       " 158: {'grad_norm': 15.184893876314163, 'ratio': 0.78515625},\n",
       " 159: {'grad_norm': 15.28910705447197, 'ratio': 0.74609375},\n",
       " 160: {'grad_norm': 15.393801867961884, 'ratio': 0.7578125},\n",
       " 161: {'grad_norm': 15.498691648244858, 'ratio': 0.76171875},\n",
       " 162: {'grad_norm': 15.604354530572891, 'ratio': 0.734375},\n",
       " 163: {'grad_norm': 15.710083037614822, 'ratio': 0.76953125},\n",
       " 164: {'grad_norm': 15.816643983125687, 'ratio': 0.75390625},\n",
       " 165: {'grad_norm': 15.923375844955444, 'ratio': 0.7578125},\n",
       " 166: {'grad_norm': 16.030416429042816, 'ratio': 0.7578125},\n",
       " 167: {'grad_norm': 16.137880980968475, 'ratio': 0.765625},\n",
       " 168: {'grad_norm': 16.245849311351776, 'ratio': 0.75390625},\n",
       " 169: {'grad_norm': 16.35425692796707, 'ratio': 0.74609375},\n",
       " 170: {'grad_norm': 16.46319943666458, 'ratio': 0.765625},\n",
       " 171: {'grad_norm': 16.57271718978882, 'ratio': 0.765625},\n",
       " 172: {'grad_norm': 16.682234048843384, 'ratio': 0.74609375},\n",
       " 173: {'grad_norm': 16.792250275611877, 'ratio': 0.76171875},\n",
       " 174: {'grad_norm': 16.902564227581024, 'ratio': 0.76953125},\n",
       " 175: {'grad_norm': 17.013811111450195, 'ratio': 0.7734375},\n",
       " 176: {'grad_norm': 17.125120878219604, 'ratio': 0.76171875},\n",
       " 177: {'grad_norm': 17.23724192380905, 'ratio': 0.75390625},\n",
       " 178: {'grad_norm': 17.349246203899384, 'ratio': 0.73828125},\n",
       " 179: {'grad_norm': 17.462013959884644, 'ratio': 0.7421875},\n",
       " 180: {'grad_norm': 17.574851632118225, 'ratio': 0.75390625},\n",
       " 181: {'grad_norm': 17.688296258449554, 'ratio': 0.734375},\n",
       " 182: {'grad_norm': 17.802481770515442, 'ratio': 0.74609375},\n",
       " 183: {'grad_norm': 17.917126715183258, 'ratio': 0.76171875},\n",
       " 184: {'grad_norm': 18.031993806362152, 'ratio': 0.75},\n",
       " 185: {'grad_norm': 18.14726048707962, 'ratio': 0.75390625},\n",
       " 186: {'grad_norm': 18.263128519058228, 'ratio': 0.75390625},\n",
       " 187: {'grad_norm': 18.37922638654709, 'ratio': 0.7421875},\n",
       " 188: {'grad_norm': 18.49575763940811, 'ratio': 0.75},\n",
       " 189: {'grad_norm': 18.61288857460022, 'ratio': 0.75390625},\n",
       " 190: {'grad_norm': 18.730428218841553, 'ratio': 0.75390625},\n",
       " 191: {'grad_norm': 18.84868174791336, 'ratio': 0.7734375},\n",
       " 192: {'grad_norm': 18.967199444770813, 'ratio': 0.7421875},\n",
       " 193: {'grad_norm': 19.086146235466003, 'ratio': 0.74609375},\n",
       " 194: {'grad_norm': 19.205619037151337, 'ratio': 0.75390625},\n",
       " 195: {'grad_norm': 19.32554066181183, 'ratio': 0.7265625},\n",
       " 196: {'grad_norm': 19.446500301361084, 'ratio': 0.76171875},\n",
       " 197: {'grad_norm': 19.56724661588669, 'ratio': 0.75390625},\n",
       " 198: {'grad_norm': 19.688432216644287, 'ratio': 0.74609375},\n",
       " 199: {'grad_norm': 19.81023073196411, 'ratio': 0.76171875},\n",
       " 200: {'grad_norm': 19.93262219429016, 'ratio': 0.75},\n",
       " 201: {'grad_norm': 20.055569410324097, 'ratio': 0.77734375},\n",
       " 202: {'grad_norm': 20.17883324623108, 'ratio': 0.77734375},\n",
       " 203: {'grad_norm': 20.30256086587906, 'ratio': 0.7578125},\n",
       " 204: {'grad_norm': 20.426553428173065, 'ratio': 0.76171875},\n",
       " 205: {'grad_norm': 20.55132907629013, 'ratio': 0.7578125},\n",
       " 206: {'grad_norm': 20.67649668455124, 'ratio': 0.75390625},\n",
       " 207: {'grad_norm': 20.802008032798767, 'ratio': 0.765625},\n",
       " 208: {'grad_norm': 20.92853605747223, 'ratio': 0.7578125},\n",
       " 209: {'grad_norm': 21.05578303337097, 'ratio': 0.7578125},\n",
       " 210: {'grad_norm': 21.18288290500641, 'ratio': 0.77734375},\n",
       " 211: {'grad_norm': 21.310377955436707, 'ratio': 0.77734375},\n",
       " 212: {'grad_norm': 21.43846571445465, 'ratio': 0.7421875},\n",
       " 213: {'grad_norm': 21.56712144613266, 'ratio': 0.78125},\n",
       " 214: {'grad_norm': 21.69621431827545, 'ratio': 0.7578125},\n",
       " 215: {'grad_norm': 21.82627558708191, 'ratio': 0.7578125},\n",
       " 216: {'grad_norm': 21.95632392168045, 'ratio': 0.75},\n",
       " 217: {'grad_norm': 22.08690047264099, 'ratio': 0.765625},\n",
       " 218: {'grad_norm': 22.21819680929184, 'ratio': 0.76171875},\n",
       " 219: {'grad_norm': 22.34980845451355, 'ratio': 0.7578125},\n",
       " 220: {'grad_norm': 22.481708884239197, 'ratio': 0.7734375},\n",
       " 221: {'grad_norm': 22.614685773849487, 'ratio': 0.75},\n",
       " 222: {'grad_norm': 22.748426735401154, 'ratio': 0.77734375},\n",
       " 223: {'grad_norm': 22.88202863931656, 'ratio': 0.75},\n",
       " 224: {'grad_norm': 23.016200721263885, 'ratio': 0.79296875},\n",
       " 225: {'grad_norm': 23.150977551937103, 'ratio': 0.78515625},\n",
       " 226: {'grad_norm': 23.286436915397644, 'ratio': 0.76953125},\n",
       " 227: {'grad_norm': 23.42256671190262, 'ratio': 0.76953125},\n",
       " 228: {'grad_norm': 23.559049665927887, 'ratio': 0.76953125},\n",
       " 229: {'grad_norm': 23.695898711681366, 'ratio': 0.75390625},\n",
       " 230: {'grad_norm': 23.8331977725029, 'ratio': 0.7578125},\n",
       " 231: {'grad_norm': 23.971819281578064, 'ratio': 0.765625},\n",
       " 232: {'grad_norm': 24.109668135643005, 'ratio': 0.76171875},\n",
       " 233: {'grad_norm': 24.248371243476868, 'ratio': 0.74609375},\n",
       " 234: {'grad_norm': 24.38815128803253, 'ratio': 0.7734375},\n",
       " 235: {'grad_norm': 24.52816992998123, 'ratio': 0.76953125},\n",
       " 236: {'grad_norm': 24.669107019901276, 'ratio': 0.7421875},\n",
       " 237: {'grad_norm': 24.81032156944275, 'ratio': 0.76953125},\n",
       " 238: {'grad_norm': 24.95177847146988, 'ratio': 0.765625},\n",
       " 239: {'grad_norm': 25.09411484003067, 'ratio': 0.7734375},\n",
       " 240: {'grad_norm': 25.237659454345703, 'ratio': 0.75},\n",
       " 241: {'grad_norm': 25.380829572677612, 'ratio': 0.765625},\n",
       " 242: {'grad_norm': 25.524913787841797, 'ratio': 0.74609375},\n",
       " 243: {'grad_norm': 25.66932725906372, 'ratio': 0.73828125},\n",
       " 244: {'grad_norm': 25.81397569179535, 'ratio': 0.765625},\n",
       " 245: {'grad_norm': 25.960116624832153, 'ratio': 0.73828125},\n",
       " 246: {'grad_norm': 26.10612964630127, 'ratio': 0.75390625},\n",
       " 247: {'grad_norm': 26.253249168395996, 'ratio': 0.7734375},\n",
       " 248: {'grad_norm': 26.400673747062683, 'ratio': 0.7734375},\n",
       " 249: {'grad_norm': 26.548426032066345, 'ratio': 0.77734375},\n",
       " 250: {'grad_norm': 26.69710397720337, 'ratio': 0.7578125},\n",
       " 251: {'grad_norm': 26.84611690044403, 'ratio': 0.73828125},\n",
       " 252: {'grad_norm': 26.9956556558609, 'ratio': 0.75390625},\n",
       " 253: {'grad_norm': 27.14577305316925, 'ratio': 0.78125},\n",
       " 254: {'grad_norm': 27.296788573265076, 'ratio': 0.7578125},\n",
       " 255: {'grad_norm': 27.448116779327393, 'ratio': 0.734375},\n",
       " 256: {'grad_norm': 27.599910974502563, 'ratio': 0.7890625},\n",
       " 257: {'grad_norm': 27.752044558525085, 'ratio': 0.75},\n",
       " 258: {'grad_norm': 27.90491282939911, 'ratio': 0.75},\n",
       " 259: {'grad_norm': 28.058713793754578, 'ratio': 0.76953125},\n",
       " 260: {'grad_norm': 28.21313440799713, 'ratio': 0.76171875},\n",
       " 261: {'grad_norm': 28.367441296577454, 'ratio': 0.765625},\n",
       " 262: {'grad_norm': 28.523176908493042, 'ratio': 0.74609375},\n",
       " 263: {'grad_norm': 28.679278135299683, 'ratio': 0.75},\n",
       " 264: {'grad_norm': 28.83593261241913, 'ratio': 0.75390625},\n",
       " 265: {'grad_norm': 28.992703795433044, 'ratio': 0.76171875},\n",
       " 266: {'grad_norm': 29.15074932575226, 'ratio': 0.78125},\n",
       " 267: {'grad_norm': 29.309008836746216, 'ratio': 0.73828125},\n",
       " 268: {'grad_norm': 29.467867732048035, 'ratio': 0.77734375},\n",
       " 269: {'grad_norm': 29.627294778823853, 'ratio': 0.765625},\n",
       " 270: {'grad_norm': 29.78752624988556, 'ratio': 0.75390625},\n",
       " 271: {'grad_norm': 29.947952151298523, 'ratio': 0.7734375},\n",
       " 272: {'grad_norm': 30.109870076179504, 'ratio': 0.7578125},\n",
       " 273: {'grad_norm': 30.27177906036377, 'ratio': 0.78515625},\n",
       " 274: {'grad_norm': 30.433817148208618, 'ratio': 0.7578125},\n",
       " 275: {'grad_norm': 30.59691083431244, 'ratio': 0.765625},\n",
       " 276: {'grad_norm': 30.760738492012024, 'ratio': 0.7578125},\n",
       " 277: {'grad_norm': 30.925425171852112, 'ratio': 0.7734375},\n",
       " 278: {'grad_norm': 31.08996045589447, 'ratio': 0.7734375},\n",
       " 279: {'grad_norm': 31.255992531776428, 'ratio': 0.74609375},\n",
       " 280: {'grad_norm': 31.421544313430786, 'ratio': 0.76953125},\n",
       " 281: {'grad_norm': 31.58813488483429, 'ratio': 0.7890625},\n",
       " 282: {'grad_norm': 31.75609064102173, 'ratio': 0.75390625},\n",
       " 283: {'grad_norm': 31.92460608482361, 'ratio': 0.74609375},\n",
       " 284: {'grad_norm': 32.09213054180145, 'ratio': 0.7734375},\n",
       " 285: {'grad_norm': 32.26147019863129, 'ratio': 0.7734375},\n",
       " 286: {'grad_norm': 32.43180799484253, 'ratio': 0.7421875},\n",
       " 287: {'grad_norm': 32.6025972366333, 'ratio': 0.7890625},\n",
       " 288: {'grad_norm': 32.77360212802887, 'ratio': 0.765625},\n",
       " 289: {'grad_norm': 32.94555854797363, 'ratio': 0.7734375},\n",
       " 290: {'grad_norm': 33.117862582206726, 'ratio': 0.7734375},\n",
       " 291: {'grad_norm': 33.29011034965515, 'ratio': 0.7578125},\n",
       " 292: {'grad_norm': 33.46430146694183, 'ratio': 0.74609375},\n",
       " 293: {'grad_norm': 33.63921678066254, 'ratio': 0.7578125},\n",
       " 294: {'grad_norm': 33.8139865398407, 'ratio': 0.76953125},\n",
       " 295: {'grad_norm': 33.98874628543854, 'ratio': 0.74609375},\n",
       " 296: {'grad_norm': 34.16499733924866, 'ratio': 0.77734375},\n",
       " 297: {'grad_norm': 34.342432141304016, 'ratio': 0.76171875},\n",
       " 298: {'grad_norm': 34.51941645145416, 'ratio': 0.765625},\n",
       " 299: {'grad_norm': 34.69704282283783, 'ratio': 0.765625},\n",
       " 300: {'grad_norm': 34.87618160247803, 'ratio': 0.765625},\n",
       " 301: {'grad_norm': 35.05581259727478, 'ratio': 0.76171875},\n",
       " 302: {'grad_norm': 35.23571419715881, 'ratio': 0.77734375},\n",
       " 303: {'grad_norm': 35.41614353656769, 'ratio': 0.75390625},\n",
       " 304: {'grad_norm': 35.59756052494049, 'ratio': 0.7578125},\n",
       " 305: {'grad_norm': 35.780152559280396, 'ratio': 0.7421875},\n",
       " 306: {'grad_norm': 35.96264970302582, 'ratio': 0.76953125},\n",
       " 307: {'grad_norm': 36.14539361000061, 'ratio': 0.77734375},\n",
       " 308: {'grad_norm': 36.32957577705383, 'ratio': 0.74609375},\n",
       " 309: {'grad_norm': 36.514888644218445, 'ratio': 0.76171875},\n",
       " 310: {'grad_norm': 36.699989438056946, 'ratio': 0.74609375},\n",
       " 311: {'grad_norm': 36.88666033744812, 'ratio': 0.76953125},\n",
       " 312: {'grad_norm': 37.073192834854126, 'ratio': 0.76953125},\n",
       " 313: {'grad_norm': 37.260557413101196, 'ratio': 0.78125},\n",
       " 314: {'grad_norm': 37.448660135269165, 'ratio': 0.76953125},\n",
       " 315: {'grad_norm': 37.63686001300812, 'ratio': 0.75390625},\n",
       " 316: {'grad_norm': 37.826616168022156, 'ratio': 0.75390625},\n",
       " 317: {'grad_norm': 38.016056537628174, 'ratio': 0.76953125},\n",
       " 318: {'grad_norm': 38.20631456375122, 'ratio': 0.7734375},\n",
       " 319: {'grad_norm': 38.398481011390686, 'ratio': 0.76171875},\n",
       " 320: {'grad_norm': 38.59002995491028, 'ratio': 0.75390625},\n",
       " 321: {'grad_norm': 38.782787442207336, 'ratio': 0.765625},\n",
       " 322: {'grad_norm': 38.9758585691452, 'ratio': 0.75},\n",
       " 323: {'grad_norm': 39.17031753063202, 'ratio': 0.76953125},\n",
       " 324: {'grad_norm': 39.365124106407166, 'ratio': 0.76953125},\n",
       " 325: {'grad_norm': 39.56108832359314, 'ratio': 0.76171875},\n",
       " 326: {'grad_norm': 39.756930351257324, 'ratio': 0.73828125},\n",
       " 327: {'grad_norm': 39.95394563674927, 'ratio': 0.75},\n",
       " 328: {'grad_norm': 40.15107345581055, 'ratio': 0.76171875},\n",
       " 329: {'grad_norm': 40.34904086589813, 'ratio': 0.76953125},\n",
       " 330: {'grad_norm': 40.54710292816162, 'ratio': 0.78125},\n",
       " 331: {'grad_norm': 40.746164083480835, 'ratio': 0.7578125},\n",
       " 332: {'grad_norm': 40.94727087020874, 'ratio': 0.7578125},\n",
       " 333: {'grad_norm': 41.148630142211914, 'ratio': 0.7578125},\n",
       " 334: {'grad_norm': 41.34969139099121, 'ratio': 0.74609375},\n",
       " 335: {'grad_norm': 41.552260398864746, 'ratio': 0.7578125},\n",
       " 336: {'grad_norm': 41.75459623336792, 'ratio': 0.7578125},\n",
       " 337: {'grad_norm': 41.95848250389099, 'ratio': 0.7890625},\n",
       " 338: {'grad_norm': 42.16236710548401, 'ratio': 0.765625},\n",
       " 339: {'grad_norm': 42.36871886253357, 'ratio': 0.76171875},\n",
       " 340: {'grad_norm': 42.57449412345886, 'ratio': 0.74609375},\n",
       " 341: {'grad_norm': 42.78083682060242, 'ratio': 0.76171875},\n",
       " 342: {'grad_norm': 42.98734474182129, 'ratio': 0.75390625},\n",
       " 343: {'grad_norm': 43.19529414176941, 'ratio': 0.75},\n",
       " 344: {'grad_norm': 43.402700901031494, 'ratio': 0.7578125},\n",
       " 345: {'grad_norm': 43.61233448982239, 'ratio': 0.765625},\n",
       " 346: {'grad_norm': 43.823866844177246, 'ratio': 0.76171875},\n",
       " 347: {'grad_norm': 44.03416895866394, 'ratio': 0.75},\n",
       " 348: {'grad_norm': 44.24622654914856, 'ratio': 0.765625},\n",
       " 349: {'grad_norm': 44.45871305465698, 'ratio': 0.765625},\n",
       " 350: {'grad_norm': 44.671919107437134, 'ratio': 0.77734375},\n",
       " 351: {'grad_norm': 44.884228467941284, 'ratio': 0.765625},\n",
       " 352: {'grad_norm': 45.09905791282654, 'ratio': 0.76953125},\n",
       " 353: {'grad_norm': 45.31497931480408, 'ratio': 0.7578125},\n",
       " 354: {'grad_norm': 45.53135561943054, 'ratio': 0.7578125},\n",
       " 355: {'grad_norm': 45.7482635974884, 'ratio': 0.75390625},\n",
       " 356: {'grad_norm': 45.96435880661011, 'ratio': 0.76953125},\n",
       " 357: {'grad_norm': 46.18254518508911, 'ratio': 0.76171875},\n",
       " 358: {'grad_norm': 46.40163040161133, 'ratio': 0.75},\n",
       " 359: {'grad_norm': 46.62065768241882, 'ratio': 0.7734375},\n",
       " 360: {'grad_norm': 46.84077501296997, 'ratio': 0.765625},\n",
       " 361: {'grad_norm': 47.062350273132324, 'ratio': 0.76171875},\n",
       " 362: {'grad_norm': 47.284034967422485, 'ratio': 0.78125},\n",
       " 363: {'grad_norm': 47.50651168823242, 'ratio': 0.76171875},\n",
       " 364: {'grad_norm': 47.73041081428528, 'ratio': 0.75},\n",
       " 365: {'grad_norm': 47.95358324050903, 'ratio': 0.75},\n",
       " 366: {'grad_norm': 48.178462266922, 'ratio': 0.765625},\n",
       " 367: {'grad_norm': 48.404425859451294, 'ratio': 0.75},\n",
       " 368: {'grad_norm': 48.63096022605896, 'ratio': 0.765625},\n",
       " 369: {'grad_norm': 48.857720375061035, 'ratio': 0.75390625},\n",
       " 370: {'grad_norm': 49.08577370643616, 'ratio': 0.78515625},\n",
       " 371: {'grad_norm': 49.31451058387756, 'ratio': 0.7578125},\n",
       " 372: {'grad_norm': 49.543490171432495, 'ratio': 0.75},\n",
       " 373: {'grad_norm': 49.77356576919556, 'ratio': 0.76953125},\n",
       " 374: {'grad_norm': 50.00429439544678, 'ratio': 0.76171875},\n",
       " 375: {'grad_norm': 50.236199140548706, 'ratio': 0.75},\n",
       " 376: {'grad_norm': 50.469078063964844, 'ratio': 0.78125},\n",
       " 377: {'grad_norm': 50.701332330703735, 'ratio': 0.75390625},\n",
       " 378: {'grad_norm': 50.93556070327759, 'ratio': 0.76171875},\n",
       " 379: {'grad_norm': 51.168251276016235, 'ratio': 0.76953125},\n",
       " 380: {'grad_norm': 51.40399479866028, 'ratio': 0.796875},\n",
       " 381: {'grad_norm': 51.640867710113525, 'ratio': 0.78515625},\n",
       " 382: {'grad_norm': 51.8778121471405, 'ratio': 0.78515625},\n",
       " 383: {'grad_norm': 52.11614727973938, 'ratio': 0.765625},\n",
       " 384: {'grad_norm': 52.353084564208984, 'ratio': 0.76953125},\n",
       " 385: {'grad_norm': 52.59196639060974, 'ratio': 0.75390625},\n",
       " 386: {'grad_norm': 52.832587003707886, 'ratio': 0.7578125},\n",
       " 387: {'grad_norm': 53.07323145866394, 'ratio': 0.78515625},\n",
       " 388: {'grad_norm': 53.31424880027771, 'ratio': 0.7734375},\n",
       " 389: {'grad_norm': 53.55626392364502, 'ratio': 0.75},\n",
       " 390: {'grad_norm': 53.799694538116455, 'ratio': 0.7734375},\n",
       " 391: {'grad_norm': 54.04426550865173, 'ratio': 0.76953125},\n",
       " 392: {'grad_norm': 54.2896933555603, 'ratio': 0.76953125},\n",
       " 393: {'grad_norm': 54.534576177597046, 'ratio': 0.7734375},\n",
       " 394: {'grad_norm': 54.77922320365906, 'ratio': 0.7578125},\n",
       " 395: {'grad_norm': 55.026556730270386, 'ratio': 0.75390625},\n",
       " 396: {'grad_norm': 55.27536487579346, 'ratio': 0.77734375},\n",
       " 397: {'grad_norm': 55.523857831954956, 'ratio': 0.76953125},\n",
       " 398: {'grad_norm': 55.77439641952515, 'ratio': 0.7421875},\n",
       " 399: {'grad_norm': 56.022892236709595, 'ratio': 0.76171875},\n",
       " 400: {'grad_norm': 56.27314352989197, 'ratio': 0.765625},\n",
       " 401: {'grad_norm': 56.52516770362854, 'ratio': 0.765625},\n",
       " 402: {'grad_norm': 56.77766180038452, 'ratio': 0.75390625},\n",
       " 403: {'grad_norm': 57.03120756149292, 'ratio': 0.7734375},\n",
       " 404: {'grad_norm': 57.28631114959717, 'ratio': 0.78125},\n",
       " 405: {'grad_norm': 57.53877234458923, 'ratio': 0.7578125},\n",
       " 406: {'grad_norm': 57.795634031295776, 'ratio': 0.74609375},\n",
       " 407: {'grad_norm': 58.05189871788025, 'ratio': 0.78125},\n",
       " 408: {'grad_norm': 58.30952787399292, 'ratio': 0.76171875},\n",
       " 409: {'grad_norm': 58.567272424697876, 'ratio': 0.75390625},\n",
       " 410: {'grad_norm': 58.82602667808533, 'ratio': 0.76953125},\n",
       " 411: {'grad_norm': 59.085577964782715, 'ratio': 0.78125},\n",
       " 412: {'grad_norm': 59.346057415008545, 'ratio': 0.76953125},\n",
       " 413: {'grad_norm': 59.60784029960632, 'ratio': 0.765625},\n",
       " 414: {'grad_norm': 59.87087607383728, 'ratio': 0.78125},\n",
       " 415: {'grad_norm': 60.13318872451782, 'ratio': 0.77734375},\n",
       " 416: {'grad_norm': 60.3980667591095, 'ratio': 0.7734375},\n",
       " 417: {'grad_norm': 60.66166019439697, 'ratio': 0.77734375},\n",
       " 418: {'grad_norm': 60.92742300033569, 'ratio': 0.7890625},\n",
       " 419: {'grad_norm': 61.19349217414856, 'ratio': 0.75390625},\n",
       " 420: {'grad_norm': 61.459431171417236, 'ratio': 0.765625},\n",
       " 421: {'grad_norm': 61.72613310813904, 'ratio': 0.76953125},\n",
       " 422: {'grad_norm': 61.996354818344116, 'ratio': 0.7421875},\n",
       " 423: {'grad_norm': 62.26566791534424, 'ratio': 0.76953125},\n",
       " 424: {'grad_norm': 62.53561472892761, 'ratio': 0.7265625},\n",
       " 425: {'grad_norm': 62.80679893493652, 'ratio': 0.76171875},\n",
       " 426: {'grad_norm': 63.07743573188782, 'ratio': 0.77734375},\n",
       " 427: {'grad_norm': 63.34892821311951, 'ratio': 0.765625},\n",
       " 428: {'grad_norm': 63.62363529205322, 'ratio': 0.76953125},\n",
       " 429: {'grad_norm': 63.897340297698975, 'ratio': 0.78125},\n",
       " 430: {'grad_norm': 64.17355298995972, 'ratio': 0.78515625},\n",
       " 431: {'grad_norm': 64.44921827316284, 'ratio': 0.76953125},\n",
       " 432: {'grad_norm': 64.72468304634094, 'ratio': 0.74609375},\n",
       " 433: {'grad_norm': 65.00341153144836, 'ratio': 0.75390625},\n",
       " 434: {'grad_norm': 65.28257632255554, 'ratio': 0.75390625},\n",
       " 435: {'grad_norm': 65.5620174407959, 'ratio': 0.75},\n",
       " 436: {'grad_norm': 65.84201669692993, 'ratio': 0.76171875},\n",
       " 437: {'grad_norm': 66.12373542785645, 'ratio': 0.76953125},\n",
       " 438: {'grad_norm': 66.40465641021729, 'ratio': 0.7734375},\n",
       " 439: {'grad_norm': 66.68761920928955, 'ratio': 0.74609375},\n",
       " 440: {'grad_norm': 66.97050857543945, 'ratio': 0.76171875},\n",
       " 441: {'grad_norm': 67.25554704666138, 'ratio': 0.75390625},\n",
       " 442: {'grad_norm': 67.54095983505249, 'ratio': 0.7578125},\n",
       " 443: {'grad_norm': 67.82740592956543, 'ratio': 0.7734375},\n",
       " 444: {'grad_norm': 68.11342859268188, 'ratio': 0.75},\n",
       " 445: {'grad_norm': 68.40114545822144, 'ratio': 0.76171875},\n",
       " 446: {'grad_norm': 68.69062089920044, 'ratio': 0.734375},\n",
       " 447: {'grad_norm': 68.98026752471924, 'ratio': 0.74609375},\n",
       " 448: {'grad_norm': 69.27091407775879, 'ratio': 0.7890625},\n",
       " 449: {'grad_norm': 69.56216478347778, 'ratio': 0.7421875},\n",
       " 450: {'grad_norm': 69.85294914245605, 'ratio': 0.765625},\n",
       " 451: {'grad_norm': 70.14482879638672, 'ratio': 0.765625},\n",
       " 452: {'grad_norm': 70.43859100341797, 'ratio': 0.76953125},\n",
       " 453: {'grad_norm': 70.73418474197388, 'ratio': 0.76953125},\n",
       " 454: {'grad_norm': 71.03041410446167, 'ratio': 0.7578125},\n",
       " 455: {'grad_norm': 71.32839965820312, 'ratio': 0.7578125},\n",
       " 456: {'grad_norm': 71.62413311004639, 'ratio': 0.74609375},\n",
       " 457: {'grad_norm': 71.92159843444824, 'ratio': 0.77734375},\n",
       " 458: {'grad_norm': 72.22073554992676, 'ratio': 0.75},\n",
       " 459: {'grad_norm': 72.5205979347229, 'ratio': 0.73828125},\n",
       " 460: {'grad_norm': 72.82008457183838, 'ratio': 0.76953125},\n",
       " 461: {'grad_norm': 73.12277603149414, 'ratio': 0.75},\n",
       " 462: {'grad_norm': 73.42544317245483, 'ratio': 0.76953125},\n",
       " 463: {'grad_norm': 73.72897291183472, 'ratio': 0.7734375},\n",
       " 464: {'grad_norm': 74.03433179855347, 'ratio': 0.75},\n",
       " 465: {'grad_norm': 74.33957242965698, 'ratio': 0.76953125},\n",
       " 466: {'grad_norm': 74.6459436416626, 'ratio': 0.7578125},\n",
       " 467: {'grad_norm': 74.95230007171631, 'ratio': 0.765625},\n",
       " 468: {'grad_norm': 75.25855493545532, 'ratio': 0.7734375},\n",
       " 469: {'grad_norm': 75.56823062896729, 'ratio': 0.77734375},\n",
       " 470: {'grad_norm': 75.87981700897217, 'ratio': 0.75},\n",
       " 471: {'grad_norm': 76.19032621383667, 'ratio': 0.75390625},\n",
       " 472: {'grad_norm': 76.50173950195312, 'ratio': 0.734375},\n",
       " 473: {'grad_norm': 76.81346654891968, 'ratio': 0.76953125},\n",
       " 474: {'grad_norm': 77.12632036209106, 'ratio': 0.79296875},\n",
       " 475: {'grad_norm': 77.43747854232788, 'ratio': 0.76171875},\n",
       " 476: {'grad_norm': 77.75199222564697, 'ratio': 0.76953125},\n",
       " 477: {'grad_norm': 78.06898403167725, 'ratio': 0.76171875},\n",
       " 478: {'grad_norm': 78.38557147979736, 'ratio': 0.765625},\n",
       " 479: {'grad_norm': 78.70461416244507, 'ratio': 0.7578125},\n",
       " 480: {'grad_norm': 79.0244951248169, 'ratio': 0.79296875},\n",
       " 481: {'grad_norm': 79.34211301803589, 'ratio': 0.78125},\n",
       " 482: {'grad_norm': 79.66175842285156, 'ratio': 0.7734375},\n",
       " 483: {'grad_norm': 79.98289346694946, 'ratio': 0.7578125},\n",
       " 484: {'grad_norm': 80.30612564086914, 'ratio': 0.76171875},\n",
       " 485: {'grad_norm': 80.6290397644043, 'ratio': 0.76171875},\n",
       " 486: {'grad_norm': 80.95302295684814, 'ratio': 0.7578125},\n",
       " 487: {'grad_norm': 81.2783203125, 'ratio': 0.78125},\n",
       " 488: {'grad_norm': 81.60169219970703, 'ratio': 0.76171875},\n",
       " 489: {'grad_norm': 81.92883253097534, 'ratio': 0.765625},\n",
       " 490: {'grad_norm': 82.25745916366577, 'ratio': 0.7421875},\n",
       " 491: {'grad_norm': 82.58568859100342, 'ratio': 0.77734375},\n",
       " 492: {'grad_norm': 82.91600799560547, 'ratio': 0.74609375},\n",
       " 493: {'grad_norm': 83.24736022949219, 'ratio': 0.78125},\n",
       " 494: {'grad_norm': 83.5763144493103, 'ratio': 0.78515625},\n",
       " 495: {'grad_norm': 83.90696573257446, 'ratio': 0.7734375},\n",
       " 496: {'grad_norm': 84.24267625808716, 'ratio': 0.76171875},\n",
       " 497: {'grad_norm': 84.57646656036377, 'ratio': 0.76171875},\n",
       " 498: {'grad_norm': 84.91184186935425, 'ratio': 0.76171875},\n",
       " 499: {'grad_norm': 85.24801111221313, 'ratio': 0.76953125},\n",
       " 500: {'grad_norm': 85.5854058265686, 'ratio': 0.7734375},\n",
       " 501: {'grad_norm': 85.92085075378418, 'ratio': 0.76171875},\n",
       " 502: {'grad_norm': 86.25727033615112, 'ratio': 0.7734375},\n",
       " 503: {'grad_norm': 86.59701681137085, 'ratio': 0.7734375},\n",
       " 504: {'grad_norm': 86.93803930282593, 'ratio': 0.765625},\n",
       " 505: {'grad_norm': 87.28078317642212, 'ratio': 0.765625},\n",
       " 506: {'grad_norm': 87.62424945831299, 'ratio': 0.765625},\n",
       " 507: {'grad_norm': 87.96642780303955, 'ratio': 0.7734375},\n",
       " 508: {'grad_norm': 88.31021928787231, 'ratio': 0.7734375},\n",
       " 509: {'grad_norm': 88.65510654449463, 'ratio': 0.76171875},\n",
       " 510: {'grad_norm': 89.00214910507202, 'ratio': 0.7734375},\n",
       " 511: {'grad_norm': 89.34949254989624, 'ratio': 0.76953125},\n",
       " 512: {'grad_norm': 89.69781970977783, 'ratio': 0.76171875},\n",
       " 513: {'grad_norm': 90.04899883270264, 'ratio': 0.7734375},\n",
       " 514: {'grad_norm': 90.39919757843018, 'ratio': 0.77734375},\n",
       " 515: {'grad_norm': 90.748863697052, 'ratio': 0.765625},\n",
       " 516: {'grad_norm': 91.10167980194092, 'ratio': 0.75},\n",
       " 517: {'grad_norm': 91.45238780975342, 'ratio': 0.73828125},\n",
       " 518: {'grad_norm': 91.80395317077637, 'ratio': 0.7421875},\n",
       " 519: {'grad_norm': 92.15781307220459, 'ratio': 0.78515625},\n",
       " 520: {'grad_norm': 92.51254558563232, 'ratio': 0.765625},\n",
       " 521: {'grad_norm': 92.87196445465088, 'ratio': 0.7578125},\n",
       " 522: {'grad_norm': 93.22879123687744, 'ratio': 0.75390625},\n",
       " 523: {'grad_norm': 93.58857107162476, 'ratio': 0.77734375},\n",
       " 524: {'grad_norm': 93.94759607315063, 'ratio': 0.75},\n",
       " 525: {'grad_norm': 94.30775022506714, 'ratio': 0.7578125},\n",
       " 526: {'grad_norm': 94.66979455947876, 'ratio': 0.76171875},\n",
       " 527: {'grad_norm': 95.03053760528564, 'ratio': 0.7578125},\n",
       " 528: {'grad_norm': 95.39438009262085, 'ratio': 0.74609375},\n",
       " 529: {'grad_norm': 95.75847959518433, 'ratio': 0.76171875},\n",
       " 530: {'grad_norm': 96.12232637405396, 'ratio': 0.76953125},\n",
       " 531: {'grad_norm': 96.49117660522461, 'ratio': 0.765625},\n",
       " 532: {'grad_norm': 96.85404014587402, 'ratio': 0.75},\n",
       " 533: {'grad_norm': 97.22051095962524, 'ratio': 0.77734375},\n",
       " 534: {'grad_norm': 97.59006309509277, 'ratio': 0.76171875},\n",
       " 535: {'grad_norm': 97.96097230911255, 'ratio': 0.76171875},\n",
       " 536: {'grad_norm': 98.3308367729187, 'ratio': 0.77734375},\n",
       " 537: {'grad_norm': 98.70049333572388, 'ratio': 0.7578125},\n",
       " 538: {'grad_norm': 99.07344388961792, 'ratio': 0.75390625},\n",
       " 539: {'grad_norm': 99.4453821182251, 'ratio': 0.7734375},\n",
       " 540: {'grad_norm': 99.82214641571045, 'ratio': 0.765625},\n",
       " 541: {'grad_norm': 100.19782209396362, 'ratio': 0.79296875},\n",
       " 542: {'grad_norm': 100.57486057281494, 'ratio': 0.76171875},\n",
       " 543: {'grad_norm': 100.95313119888306, 'ratio': 0.75390625},\n",
       " 544: {'grad_norm': 101.33299207687378, 'ratio': 0.765625},\n",
       " 545: {'grad_norm': 101.71084785461426, 'ratio': 0.76171875},\n",
       " 546: {'grad_norm': 102.09078073501587, 'ratio': 0.78515625},\n",
       " 547: {'grad_norm': 102.47188520431519, 'ratio': 0.75390625},\n",
       " 548: {'grad_norm': 102.85548782348633, 'ratio': 0.76171875},\n",
       " 549: {'grad_norm': 103.24006652832031, 'ratio': 0.78125},\n",
       " 550: {'grad_norm': 103.62545442581177, 'ratio': 0.765625},\n",
       " 551: {'grad_norm': 104.0098385810852, 'ratio': 0.74609375},\n",
       " 552: {'grad_norm': 104.39543533325195, 'ratio': 0.765625},\n",
       " 553: {'grad_norm': 104.7842526435852, 'ratio': 0.79296875},\n",
       " 554: {'grad_norm': 105.17296266555786, 'ratio': 0.75390625},\n",
       " 555: {'grad_norm': 105.56052684783936, 'ratio': 0.76953125},\n",
       " 556: {'grad_norm': 105.95098781585693, 'ratio': 0.75},\n",
       " 557: {'grad_norm': 106.34277296066284, 'ratio': 0.7734375},\n",
       " 558: {'grad_norm': 106.73738384246826, 'ratio': 0.77734375},\n",
       " 559: {'grad_norm': 107.13086748123169, 'ratio': 0.76953125},\n",
       " 560: {'grad_norm': 107.52331829071045, 'ratio': 0.75},\n",
       " 561: {'grad_norm': 107.91894006729126, 'ratio': 0.76953125},\n",
       " 562: {'grad_norm': 108.31394529342651, 'ratio': 0.75390625},\n",
       " 563: {'grad_norm': 108.712571144104, 'ratio': 0.7734375},\n",
       " 564: {'grad_norm': 109.11020517349243, 'ratio': 0.78515625},\n",
       " 565: {'grad_norm': 109.50987386703491, 'ratio': 0.76953125},\n",
       " 566: {'grad_norm': 109.90852880477905, 'ratio': 0.75},\n",
       " 567: {'grad_norm': 110.3116807937622, 'ratio': 0.76171875},\n",
       " 568: {'grad_norm': 110.71266317367554, 'ratio': 0.75},\n",
       " 569: {'grad_norm': 111.11722707748413, 'ratio': 0.75390625},\n",
       " 570: {'grad_norm': 111.52201890945435, 'ratio': 0.76171875},\n",
       " 571: {'grad_norm': 111.92773294448853, 'ratio': 0.78125},\n",
       " 572: {'grad_norm': 112.33088493347168, 'ratio': 0.76953125},\n",
       " 573: {'grad_norm': 112.73785591125488, 'ratio': 0.7734375},\n",
       " 574: {'grad_norm': 113.14847183227539, 'ratio': 0.78515625},\n",
       " 575: {'grad_norm': 113.55796337127686, 'ratio': 0.7734375},\n",
       " 576: {'grad_norm': 113.9653754234314, 'ratio': 0.78125},\n",
       " 577: {'grad_norm': 114.3778977394104, 'ratio': 0.78515625},\n",
       " 578: {'grad_norm': 114.78942489624023, 'ratio': 0.7578125},\n",
       " 579: {'grad_norm': 115.2043571472168, 'ratio': 0.7734375},\n",
       " 580: {'grad_norm': 115.61828899383545, 'ratio': 0.78125},\n",
       " 581: {'grad_norm': 116.03563404083252, 'ratio': 0.7421875},\n",
       " 582: {'grad_norm': 116.45255756378174, 'ratio': 0.76953125},\n",
       " 583: {'grad_norm': 116.87035274505615, 'ratio': 0.76953125},\n",
       " 584: {'grad_norm': 117.28597640991211, 'ratio': 0.76171875},\n",
       " 585: {'grad_norm': 117.70453834533691, 'ratio': 0.74609375},\n",
       " 586: {'grad_norm': 118.1250638961792, 'ratio': 0.76171875},\n",
       " 587: {'grad_norm': 118.54733371734619, 'ratio': 0.75390625},\n",
       " 588: {'grad_norm': 118.9703598022461, 'ratio': 0.78125},\n",
       " 589: {'grad_norm': 119.39238834381104, 'ratio': 0.76953125},\n",
       " 590: {'grad_norm': 119.81697273254395, 'ratio': 0.7578125},\n",
       " 591: {'grad_norm': 120.24225425720215, 'ratio': 0.76953125},\n",
       " 592: {'grad_norm': 120.66893291473389, 'ratio': 0.75},\n",
       " 593: {'grad_norm': 121.0971565246582, 'ratio': 0.7578125},\n",
       " 594: {'grad_norm': 121.5257568359375, 'ratio': 0.75390625},\n",
       " 595: {'grad_norm': 121.95589351654053, 'ratio': 0.77734375},\n",
       " 596: {'grad_norm': 122.38674449920654, 'ratio': 0.7578125},\n",
       " 597: {'grad_norm': 122.81871223449707, 'ratio': 0.76953125},\n",
       " 598: {'grad_norm': 123.2513837814331, 'ratio': 0.77734375},\n",
       " 599: {'grad_norm': 123.68654823303223, 'ratio': 0.7578125},\n",
       " 600: {'grad_norm': 124.11956024169922, 'ratio': 0.79296875},\n",
       " 601: {'grad_norm': 124.55451583862305, 'ratio': 0.76171875},\n",
       " 602: {'grad_norm': 124.99168872833252, 'ratio': 0.76171875},\n",
       " 603: {'grad_norm': 125.4314603805542, 'ratio': 0.78125},\n",
       " 604: {'grad_norm': 125.87157821655273, 'ratio': 0.74609375},\n",
       " 605: {'grad_norm': 126.31478214263916, 'ratio': 0.74609375},\n",
       " 606: {'grad_norm': 126.7556381225586, 'ratio': 0.7578125},\n",
       " 607: {'grad_norm': 127.19762325286865, 'ratio': 0.765625},\n",
       " 608: {'grad_norm': 127.64164638519287, 'ratio': 0.75},\n",
       " 609: {'grad_norm': 128.08289432525635, 'ratio': 0.75},\n",
       " 610: {'grad_norm': 128.5288953781128, 'ratio': 0.76171875},\n",
       " 611: {'grad_norm': 128.9753761291504, 'ratio': 0.765625},\n",
       " 612: {'grad_norm': 129.42492485046387, 'ratio': 0.7578125},\n",
       " 613: {'grad_norm': 129.87546825408936, 'ratio': 0.75},\n",
       " 614: {'grad_norm': 130.32434844970703, 'ratio': 0.734375},\n",
       " 615: {'grad_norm': 130.7746343612671, 'ratio': 0.7578125},\n",
       " 616: {'grad_norm': 131.22593593597412, 'ratio': 0.765625},\n",
       " 617: {'grad_norm': 131.67305374145508, 'ratio': 0.75},\n",
       " 618: {'grad_norm': 132.1243715286255, 'ratio': 0.7578125},\n",
       " 619: {'grad_norm': 132.57940673828125, 'ratio': 0.7421875},\n",
       " 620: {'grad_norm': 133.03790855407715, 'ratio': 0.74609375},\n",
       " 621: {'grad_norm': 133.49716091156006, 'ratio': 0.78125},\n",
       " 622: {'grad_norm': 133.9560031890869, 'ratio': 0.78515625},\n",
       " 623: {'grad_norm': 134.41729831695557, 'ratio': 0.73828125},\n",
       " 624: {'grad_norm': 134.87801933288574, 'ratio': 0.76953125},\n",
       " 625: {'grad_norm': 135.33633041381836, 'ratio': 0.76171875},\n",
       " 626: {'grad_norm': 135.7953338623047, 'ratio': 0.76953125},\n",
       " 627: {'grad_norm': 136.25837898254395, 'ratio': 0.78515625},\n",
       " 628: {'grad_norm': 136.7243309020996, 'ratio': 0.765625},\n",
       " 629: {'grad_norm': 137.18968772888184, 'ratio': 0.73828125},\n",
       " 630: {'grad_norm': 137.65711879730225, 'ratio': 0.78125},\n",
       " 631: {'grad_norm': 138.1256504058838, 'ratio': 0.76953125},\n",
       " 632: {'grad_norm': 138.5947380065918, 'ratio': 0.75},\n",
       " 633: {'grad_norm': 139.0648317337036, 'ratio': 0.76953125},\n",
       " 634: {'grad_norm': 139.53231239318848, 'ratio': 0.7578125},\n",
       " 635: {'grad_norm': 140.00317001342773, 'ratio': 0.74609375},\n",
       " 636: {'grad_norm': 140.4775447845459, 'ratio': 0.76953125},\n",
       " 637: {'grad_norm': 140.9530487060547, 'ratio': 0.74609375},\n",
       " 638: {'grad_norm': 141.4300298690796, 'ratio': 0.7734375},\n",
       " 639: {'grad_norm': 141.90306568145752, 'ratio': 0.75390625},\n",
       " 640: {'grad_norm': 142.3819923400879, 'ratio': 0.76171875},\n",
       " 641: {'grad_norm': 142.86279773712158, 'ratio': 0.75390625},\n",
       " 642: {'grad_norm': 143.3430814743042, 'ratio': 0.75390625},\n",
       " 643: {'grad_norm': 143.8222131729126, 'ratio': 0.7734375},\n",
       " 644: {'grad_norm': 144.30152988433838, 'ratio': 0.7578125},\n",
       " 645: {'grad_norm': 144.78664207458496, 'ratio': 0.7578125},\n",
       " 646: {'grad_norm': 145.2737216949463, 'ratio': 0.75},\n",
       " 647: {'grad_norm': 145.75862503051758, 'ratio': 0.78515625},\n",
       " 648: {'grad_norm': 146.2437219619751, 'ratio': 0.76171875},\n",
       " 649: {'grad_norm': 146.73257064819336, 'ratio': 0.7734375},\n",
       " 650: {'grad_norm': 147.21960353851318, 'ratio': 0.74609375},\n",
       " 651: {'grad_norm': 147.70810317993164, 'ratio': 0.76171875},\n",
       " 652: {'grad_norm': 148.20088005065918, 'ratio': 0.7421875},\n",
       " 653: {'grad_norm': 148.68560981750488, 'ratio': 0.734375},\n",
       " 654: {'grad_norm': 149.1810817718506, 'ratio': 0.75390625},\n",
       " 655: {'grad_norm': 149.67544174194336, 'ratio': 0.76953125},\n",
       " 656: {'grad_norm': 150.17520904541016, 'ratio': 0.765625},\n",
       " 657: {'grad_norm': 150.67305850982666, 'ratio': 0.76171875},\n",
       " 658: {'grad_norm': 151.1725368499756, 'ratio': 0.75390625},\n",
       " 659: {'grad_norm': 151.66970252990723, 'ratio': 0.78125},\n",
       " 660: {'grad_norm': 152.16495323181152, 'ratio': 0.734375},\n",
       " 661: {'grad_norm': 152.66796970367432, 'ratio': 0.74609375},\n",
       " 662: {'grad_norm': 153.16966152191162, 'ratio': 0.73828125},\n",
       " 663: {'grad_norm': 153.66932678222656, 'ratio': 0.7421875},\n",
       " 664: {'grad_norm': 154.17418670654297, 'ratio': 0.7265625},\n",
       " 665: {'grad_norm': 154.6805591583252, 'ratio': 0.75},\n",
       " 666: {'grad_norm': 155.18912887573242, 'ratio': 0.7734375},\n",
       " 667: {'grad_norm': 155.69935035705566, 'ratio': 0.7578125},\n",
       " 668: {'grad_norm': 156.2043924331665, 'ratio': 0.734375},\n",
       " 669: {'grad_norm': 156.71589183807373, 'ratio': 0.78515625},\n",
       " 670: {'grad_norm': 157.2251377105713, 'ratio': 0.73828125},\n",
       " 671: {'grad_norm': 157.73806476593018, 'ratio': 0.7578125},\n",
       " 672: {'grad_norm': 158.2480983734131, 'ratio': 0.75390625},\n",
       " 673: {'grad_norm': 158.76079654693604, 'ratio': 0.76953125},\n",
       " 674: {'grad_norm': 159.2719373703003, 'ratio': 0.75390625},\n",
       " 675: {'grad_norm': 159.79010105133057, 'ratio': 0.765625},\n",
       " 676: {'grad_norm': 160.30805110931396, 'ratio': 0.76171875},\n",
       " 677: {'grad_norm': 160.82884120941162, 'ratio': 0.7421875},\n",
       " 678: {'grad_norm': 161.34808254241943, 'ratio': 0.765625},\n",
       " 679: {'grad_norm': 161.8728265762329, 'ratio': 0.75},\n",
       " 680: {'grad_norm': 162.3981647491455, 'ratio': 0.75},\n",
       " 681: {'grad_norm': 162.92241764068604, 'ratio': 0.7578125},\n",
       " 682: {'grad_norm': 163.44368839263916, 'ratio': 0.7421875},\n",
       " 683: {'grad_norm': 163.96714401245117, 'ratio': 0.73828125},\n",
       " 684: {'grad_norm': 164.49287128448486, 'ratio': 0.7578125},\n",
       " 685: {'grad_norm': 165.0204620361328, 'ratio': 0.76171875},\n",
       " 686: {'grad_norm': 165.55071640014648, 'ratio': 0.75390625},\n",
       " 687: {'grad_norm': 166.08056449890137, 'ratio': 0.75390625},\n",
       " 688: {'grad_norm': 166.61186122894287, 'ratio': 0.75390625},\n",
       " 689: {'grad_norm': 167.14712047576904, 'ratio': 0.74609375},\n",
       " 690: {'grad_norm': 167.6797571182251, 'ratio': 0.76953125},\n",
       " 691: {'grad_norm': 168.21324062347412, 'ratio': 0.75390625},\n",
       " 692: {'grad_norm': 168.74839687347412, 'ratio': 0.75390625},\n",
       " 693: {'grad_norm': 169.28685092926025, 'ratio': 0.76171875},\n",
       " 694: {'grad_norm': 169.8217945098877, 'ratio': 0.76953125},\n",
       " 695: {'grad_norm': 170.36086177825928, 'ratio': 0.765625},\n",
       " 696: {'grad_norm': 170.90283203125, 'ratio': 0.75390625},\n",
       " 697: {'grad_norm': 171.44616031646729, 'ratio': 0.7421875},\n",
       " 698: {'grad_norm': 171.99178886413574, 'ratio': 0.72265625},\n",
       " 699: {'grad_norm': 172.53764247894287, 'ratio': 0.76953125},\n",
       " 700: {'grad_norm': 173.08216190338135, 'ratio': 0.7734375},\n",
       " 701: {'grad_norm': 173.62462043762207, 'ratio': 0.73828125},\n",
       " 702: {'grad_norm': 174.17228317260742, 'ratio': 0.75390625},\n",
       " 703: {'grad_norm': 174.72143650054932, 'ratio': 0.734375},\n",
       " 704: {'grad_norm': 175.2704620361328, 'ratio': 0.76171875},\n",
       " 705: {'grad_norm': 175.8200273513794, 'ratio': 0.7578125},\n",
       " 706: {'grad_norm': 176.37398529052734, 'ratio': 0.76953125},\n",
       " 707: {'grad_norm': 176.93130207061768, 'ratio': 0.75},\n",
       " 708: {'grad_norm': 177.48903846740723, 'ratio': 0.75390625},\n",
       " 709: {'grad_norm': 178.03905773162842, 'ratio': 0.76171875},\n",
       " 710: {'grad_norm': 178.59712028503418, 'ratio': 0.78125},\n",
       " 711: {'grad_norm': 179.15424156188965, 'ratio': 0.76953125},\n",
       " 712: {'grad_norm': 179.7167510986328, 'ratio': 0.7578125},\n",
       " 713: {'grad_norm': 180.27805519104004, 'ratio': 0.75},\n",
       " 714: {'grad_norm': 180.83900356292725, 'ratio': 0.73828125},\n",
       " 715: {'grad_norm': 181.39863109588623, 'ratio': 0.75},\n",
       " 716: {'grad_norm': 181.96398258209229, 'ratio': 0.765625},\n",
       " 717: {'grad_norm': 182.5300817489624, 'ratio': 0.7578125},\n",
       " 718: {'grad_norm': 183.09763050079346, 'ratio': 0.75390625},\n",
       " 719: {'grad_norm': 183.66762161254883, 'ratio': 0.75390625},\n",
       " 720: {'grad_norm': 184.24112796783447, 'ratio': 0.734375},\n",
       " 721: {'grad_norm': 184.80867958068848, 'ratio': 0.74609375},\n",
       " 722: {'grad_norm': 185.38124561309814, 'ratio': 0.76953125},\n",
       " 723: {'grad_norm': 185.9513292312622, 'ratio': 0.76171875},\n",
       " 724: {'grad_norm': 186.5253438949585, 'ratio': 0.75},\n",
       " 725: {'grad_norm': 187.10463047027588, 'ratio': 0.74609375},\n",
       " 726: {'grad_norm': 187.6753807067871, 'ratio': 0.75390625},\n",
       " 727: {'grad_norm': 188.25335597991943, 'ratio': 0.734375},\n",
       " 728: {'grad_norm': 188.82951068878174, 'ratio': 0.74609375},\n",
       " 729: {'grad_norm': 189.4109525680542, 'ratio': 0.765625},\n",
       " 730: {'grad_norm': 189.98804473876953, 'ratio': 0.7578125},\n",
       " 731: {'grad_norm': 190.5704927444458, 'ratio': 0.75},\n",
       " 732: {'grad_norm': 191.1575689315796, 'ratio': 0.75390625},\n",
       " 733: {'grad_norm': 191.74062156677246, 'ratio': 0.74609375},\n",
       " 734: {'grad_norm': 192.3296184539795, 'ratio': 0.75390625},\n",
       " 735: {'grad_norm': 192.9152956008911, 'ratio': 0.765625},\n",
       " 736: {'grad_norm': 193.50410270690918, 'ratio': 0.76953125},\n",
       " 737: {'grad_norm': 194.09242057800293, 'ratio': 0.7578125},\n",
       " 738: {'grad_norm': 194.68553733825684, 'ratio': 0.76171875},\n",
       " 739: {'grad_norm': 195.2788906097412, 'ratio': 0.77734375},\n",
       " 740: {'grad_norm': 195.87175178527832, 'ratio': 0.7578125},\n",
       " 741: {'grad_norm': 196.4597396850586, 'ratio': 0.765625},\n",
       " 742: {'grad_norm': 197.058837890625, 'ratio': 0.76171875},\n",
       " 743: {'grad_norm': 197.65188217163086, 'ratio': 0.765625},\n",
       " 744: {'grad_norm': 198.24929809570312, 'ratio': 0.76953125},\n",
       " 745: {'grad_norm': 198.85034084320068, 'ratio': 0.76953125},\n",
       " 746: {'grad_norm': 199.4498405456543, 'ratio': 0.76953125},\n",
       " 747: {'grad_norm': 200.05383968353271, 'ratio': 0.76171875},\n",
       " 748: {'grad_norm': 200.6507043838501, 'ratio': 0.76953125},\n",
       " 749: {'grad_norm': 201.2571029663086, 'ratio': 0.76953125},\n",
       " 750: {'grad_norm': 201.8654146194458, 'ratio': 0.74609375},\n",
       " 751: {'grad_norm': 202.46758937835693, 'ratio': 0.76171875},\n",
       " 752: {'grad_norm': 203.077299118042, 'ratio': 0.7734375},\n",
       " 753: {'grad_norm': 203.68724822998047, 'ratio': 0.7421875},\n",
       " 754: {'grad_norm': 204.29768562316895, 'ratio': 0.7734375},\n",
       " 755: {'grad_norm': 204.91044425964355, 'ratio': 0.75390625},\n",
       " 756: {'grad_norm': 205.52136993408203, 'ratio': 0.76953125},\n",
       " 757: {'grad_norm': 206.1379566192627, 'ratio': 0.7734375},\n",
       " 758: {'grad_norm': 206.7569694519043, 'ratio': 0.77734375},\n",
       " 759: {'grad_norm': 207.37418365478516, 'ratio': 0.75390625},\n",
       " 760: {'grad_norm': 207.98565673828125, 'ratio': 0.75390625},\n",
       " 761: {'grad_norm': 208.60451889038086, 'ratio': 0.7890625},\n",
       " 762: {'grad_norm': 209.21910285949707, 'ratio': 0.7578125},\n",
       " 763: {'grad_norm': 209.83942985534668, 'ratio': 0.75},\n",
       " 764: {'grad_norm': 210.46383666992188, 'ratio': 0.77734375},\n",
       " 765: {'grad_norm': 211.08842086791992, 'ratio': 0.7734375},\n",
       " 766: {'grad_norm': 211.71555519104004, 'ratio': 0.7421875},\n",
       " 767: {'grad_norm': 212.3456802368164, 'ratio': 0.7734375},\n",
       " 768: {'grad_norm': 212.97288513183594, 'ratio': 0.76953125},\n",
       " 769: {'grad_norm': 213.60509490966797, 'ratio': 0.7578125},\n",
       " 770: {'grad_norm': 214.23051261901855, 'ratio': 0.76171875},\n",
       " 771: {'grad_norm': 214.86104774475098, 'ratio': 0.76171875},\n",
       " 772: {'grad_norm': 215.48808479309082, 'ratio': 0.78515625},\n",
       " 773: {'grad_norm': 216.12031745910645, 'ratio': 0.77734375},\n",
       " 774: {'grad_norm': 216.75568389892578, 'ratio': 0.76953125},\n",
       " 775: {'grad_norm': 217.38745880126953, 'ratio': 0.765625},\n",
       " 776: {'grad_norm': 218.0228214263916, 'ratio': 0.75390625},\n",
       " 777: {'grad_norm': 218.66382789611816, 'ratio': 0.76953125},\n",
       " 778: {'grad_norm': 219.30091667175293, 'ratio': 0.77734375},\n",
       " 779: {'grad_norm': 219.94188499450684, 'ratio': 0.78125},\n",
       " 780: {'grad_norm': 220.5858211517334, 'ratio': 0.78125},\n",
       " 781: {'grad_norm': 221.2314682006836, 'ratio': 0.7734375},\n",
       " 782: {'grad_norm': 221.87822723388672, 'ratio': 0.76953125},\n",
       " 783: {'grad_norm': 222.52209281921387, 'ratio': 0.7578125},\n",
       " 784: {'grad_norm': 223.1684741973877, 'ratio': 0.765625},\n",
       " 785: {'grad_norm': 223.81733894348145, 'ratio': 0.7578125},\n",
       " 786: {'grad_norm': 224.46654510498047, 'ratio': 0.765625},\n",
       " 787: {'grad_norm': 225.11706352233887, 'ratio': 0.76171875},\n",
       " 788: {'grad_norm': 225.76699256896973, 'ratio': 0.7578125},\n",
       " 789: {'grad_norm': 226.41657257080078, 'ratio': 0.75390625},\n",
       " 790: {'grad_norm': 227.07500648498535, 'ratio': 0.78125},\n",
       " 791: {'grad_norm': 227.724702835083, 'ratio': 0.765625},\n",
       " 792: {'grad_norm': 228.38405227661133, 'ratio': 0.76953125},\n",
       " 793: {'grad_norm': 229.03941535949707, 'ratio': 0.79296875},\n",
       " 794: {'grad_norm': 229.7028408050537, 'ratio': 0.7578125},\n",
       " 795: {'grad_norm': 230.36627769470215, 'ratio': 0.7578125},\n",
       " 796: {'grad_norm': 231.0244255065918, 'ratio': 0.78515625},\n",
       " 797: {'grad_norm': 231.6845474243164, 'ratio': 0.7890625},\n",
       " 798: {'grad_norm': 232.3515224456787, 'ratio': 0.765625},\n",
       " 799: {'grad_norm': 233.01478385925293, 'ratio': 0.77734375},\n",
       " 800: {'grad_norm': 233.68203163146973, 'ratio': 0.7578125},\n",
       " 801: {'grad_norm': 234.35345649719238, 'ratio': 0.76171875},\n",
       " 802: {'grad_norm': 235.0214786529541, 'ratio': 0.77734375},\n",
       " 803: {'grad_norm': 235.69007873535156, 'ratio': 0.765625},\n",
       " 804: {'grad_norm': 236.3570098876953, 'ratio': 0.75},\n",
       " 805: {'grad_norm': 237.03436851501465, 'ratio': 0.7734375},\n",
       " 806: {'grad_norm': 237.71148872375488, 'ratio': 0.76953125},\n",
       " 807: {'grad_norm': 238.38426971435547, 'ratio': 0.76171875},\n",
       " 808: {'grad_norm': 239.05909156799316, 'ratio': 0.7734375},\n",
       " 809: {'grad_norm': 239.73890686035156, 'ratio': 0.77734375},\n",
       " 810: {'grad_norm': 240.4150791168213, 'ratio': 0.75390625},\n",
       " 811: {'grad_norm': 241.09649276733398, 'ratio': 0.74609375},\n",
       " 812: {'grad_norm': 241.77899742126465, 'ratio': 0.75390625},\n",
       " 813: {'grad_norm': 242.4678020477295, 'ratio': 0.7578125},\n",
       " 814: {'grad_norm': 243.15731811523438, 'ratio': 0.76953125},\n",
       " 815: {'grad_norm': 243.84019088745117, 'ratio': 0.76171875},\n",
       " 816: {'grad_norm': 244.52678680419922, 'ratio': 0.7578125},\n",
       " 817: {'grad_norm': 245.2127456665039, 'ratio': 0.76171875},\n",
       " 818: {'grad_norm': 245.89757919311523, 'ratio': 0.7578125},\n",
       " 819: {'grad_norm': 246.58731269836426, 'ratio': 0.74609375},\n",
       " 820: {'grad_norm': 247.2816162109375, 'ratio': 0.76171875},\n",
       " 821: {'grad_norm': 247.96646308898926, 'ratio': 0.78125},\n",
       " 822: {'grad_norm': 248.66213607788086, 'ratio': 0.73828125},\n",
       " 823: {'grad_norm': 249.35950469970703, 'ratio': 0.76953125},\n",
       " 824: {'grad_norm': 250.06064414978027, 'ratio': 0.75},\n",
       " 825: {'grad_norm': 250.7611083984375, 'ratio': 0.7421875},\n",
       " 826: {'grad_norm': 251.46214866638184, 'ratio': 0.75390625},\n",
       " 827: {'grad_norm': 252.1625461578369, 'ratio': 0.7421875},\n",
       " 828: {'grad_norm': 252.86611938476562, 'ratio': 0.76953125},\n",
       " 829: {'grad_norm': 253.57407188415527, 'ratio': 0.76171875},\n",
       " 830: {'grad_norm': 254.278226852417, 'ratio': 0.7734375},\n",
       " 831: {'grad_norm': 254.97782135009766, 'ratio': 0.75390625},\n",
       " 832: {'grad_norm': 255.68466758728027, 'ratio': 0.76171875},\n",
       " 833: {'grad_norm': 256.39051818847656, 'ratio': 0.7578125},\n",
       " 834: {'grad_norm': 257.09543228149414, 'ratio': 0.7421875},\n",
       " 835: {'grad_norm': 257.8034496307373, 'ratio': 0.76953125},\n",
       " 836: {'grad_norm': 258.5137424468994, 'ratio': 0.74609375},\n",
       " 837: {'grad_norm': 259.22532081604004, 'ratio': 0.77734375},\n",
       " 838: {'grad_norm': 259.9466361999512, 'ratio': 0.74609375},\n",
       " 839: {'grad_norm': 260.66834449768066, 'ratio': 0.78515625},\n",
       " 840: {'grad_norm': 261.3855571746826, 'ratio': 0.76171875},\n",
       " 841: {'grad_norm': 262.1046142578125, 'ratio': 0.7734375},\n",
       " 842: {'grad_norm': 262.82494163513184, 'ratio': 0.78515625},\n",
       " 843: {'grad_norm': 263.5432643890381, 'ratio': 0.77734375},\n",
       " 844: {'grad_norm': 264.26733589172363, 'ratio': 0.765625},\n",
       " 845: {'grad_norm': 264.9888496398926, 'ratio': 0.7734375},\n",
       " 846: {'grad_norm': 265.71742820739746, 'ratio': 0.76171875},\n",
       " 847: {'grad_norm': 266.4279384613037, 'ratio': 0.76953125},\n",
       " 848: {'grad_norm': 267.16050720214844, 'ratio': 0.7734375},\n",
       " 849: {'grad_norm': 267.89269256591797, 'ratio': 0.75390625},\n",
       " 850: {'grad_norm': 268.6274299621582, 'ratio': 0.74609375},\n",
       " 851: {'grad_norm': 269.36461067199707, 'ratio': 0.80078125},\n",
       " 852: {'grad_norm': 270.09802055358887, 'ratio': 0.7421875},\n",
       " 853: {'grad_norm': 270.8290901184082, 'ratio': 0.75},\n",
       " 854: {'grad_norm': 271.5656337738037, 'ratio': 0.78125},\n",
       " 855: {'grad_norm': 272.2992115020752, 'ratio': 0.76953125},\n",
       " 856: {'grad_norm': 273.0392265319824, 'ratio': 0.7578125},\n",
       " 857: {'grad_norm': 273.7752742767334, 'ratio': 0.76953125},\n",
       " 858: {'grad_norm': 274.5110912322998, 'ratio': 0.77734375},\n",
       " 859: {'grad_norm': 275.25034523010254, 'ratio': 0.7734375},\n",
       " 860: {'grad_norm': 275.99643325805664, 'ratio': 0.73828125},\n",
       " 861: {'grad_norm': 276.7377338409424, 'ratio': 0.80078125},\n",
       " 862: {'grad_norm': 277.48395347595215, 'ratio': 0.73828125},\n",
       " 863: {'grad_norm': 278.23134422302246, 'ratio': 0.7734375},\n",
       " 864: {'grad_norm': 278.98570823669434, 'ratio': 0.7421875},\n",
       " 865: {'grad_norm': 279.7333755493164, 'ratio': 0.74609375},\n",
       " 866: {'grad_norm': 280.4849510192871, 'ratio': 0.76171875},\n",
       " 867: {'grad_norm': 281.2406425476074, 'ratio': 0.74609375},\n",
       " 868: {'grad_norm': 281.99514961242676, 'ratio': 0.75},\n",
       " 869: {'grad_norm': 282.74545097351074, 'ratio': 0.765625},\n",
       " 870: {'grad_norm': 283.50451278686523, 'ratio': 0.7578125},\n",
       " 871: {'grad_norm': 284.267578125, 'ratio': 0.7578125},\n",
       " 872: {'grad_norm': 285.0260257720947, 'ratio': 0.765625},\n",
       " 873: {'grad_norm': 285.7902145385742, 'ratio': 0.75},\n",
       " 874: {'grad_norm': 286.53192710876465, 'ratio': 0.734375},\n",
       " 875: {'grad_norm': 287.29207611083984, 'ratio': 0.74609375},\n",
       " 876: {'grad_norm': 288.0589199066162, 'ratio': 0.77734375},\n",
       " 877: {'grad_norm': 288.8233108520508, 'ratio': 0.75390625},\n",
       " 878: {'grad_norm': 289.59229850769043, 'ratio': 0.765625},\n",
       " 879: {'grad_norm': 290.36114501953125, 'ratio': 0.73828125},\n",
       " 880: {'grad_norm': 291.1253604888916, 'ratio': 0.73828125},\n",
       " 881: {'grad_norm': 291.8956546783447, 'ratio': 0.77734375},\n",
       " 882: {'grad_norm': 292.6682872772217, 'ratio': 0.76953125},\n",
       " 883: {'grad_norm': 293.44787788391113, 'ratio': 0.7734375},\n",
       " 884: {'grad_norm': 294.226261138916, 'ratio': 0.7578125},\n",
       " 885: {'grad_norm': 295.00378036499023, 'ratio': 0.7578125},\n",
       " 886: {'grad_norm': 295.7815341949463, 'ratio': 0.76171875},\n",
       " 887: {'grad_norm': 296.56577491760254, 'ratio': 0.7578125},\n",
       " 888: {'grad_norm': 297.33583068847656, 'ratio': 0.75390625},\n",
       " 889: {'grad_norm': 298.11437225341797, 'ratio': 0.75},\n",
       " 890: {'grad_norm': 298.8898468017578, 'ratio': 0.7578125},\n",
       " 891: {'grad_norm': 299.66894149780273, 'ratio': 0.765625},\n",
       " 892: {'grad_norm': 300.4530944824219, 'ratio': 0.75390625},\n",
       " 893: {'grad_norm': 301.23950576782227, 'ratio': 0.76171875},\n",
       " 894: {'grad_norm': 302.0262756347656, 'ratio': 0.78125},\n",
       " 895: {'grad_norm': 302.813928604126, 'ratio': 0.765625},\n",
       " 896: {'grad_norm': 303.6106262207031, 'ratio': 0.7421875},\n",
       " 897: {'grad_norm': 304.4067077636719, 'ratio': 0.7421875},\n",
       " 898: {'grad_norm': 305.2015151977539, 'ratio': 0.78125},\n",
       " 899: {'grad_norm': 305.9933032989502, 'ratio': 0.75390625},\n",
       " 900: {'grad_norm': 306.7838764190674, 'ratio': 0.75390625},\n",
       " 901: {'grad_norm': 307.5868682861328, 'ratio': 0.7578125},\n",
       " 902: {'grad_norm': 308.3759517669678, 'ratio': 0.76953125},\n",
       " 903: {'grad_norm': 309.1775550842285, 'ratio': 0.76171875},\n",
       " 904: {'grad_norm': 309.98422622680664, 'ratio': 0.76953125},\n",
       " 905: {'grad_norm': 310.7842960357666, 'ratio': 0.78125},\n",
       " 906: {'grad_norm': 311.59137535095215, 'ratio': 0.765625},\n",
       " 907: {'grad_norm': 312.3973445892334, 'ratio': 0.7578125},\n",
       " 908: {'grad_norm': 313.20823097229004, 'ratio': 0.765625},\n",
       " 909: {'grad_norm': 314.0129871368408, 'ratio': 0.78125},\n",
       " 910: {'grad_norm': 314.8171501159668, 'ratio': 0.765625},\n",
       " 911: {'grad_norm': 315.63027000427246, 'ratio': 0.765625},\n",
       " 912: {'grad_norm': 316.4431381225586, 'ratio': 0.76171875},\n",
       " 913: {'grad_norm': 317.25100898742676, 'ratio': 0.75},\n",
       " 914: {'grad_norm': 318.06618309020996, 'ratio': 0.78515625},\n",
       " 915: {'grad_norm': 318.8810062408447, 'ratio': 0.76953125},\n",
       " 916: {'grad_norm': 319.69456481933594, 'ratio': 0.7578125},\n",
       " 917: {'grad_norm': 320.5044231414795, 'ratio': 0.7578125},\n",
       " 918: {'grad_norm': 321.3220844268799, 'ratio': 0.7578125},\n",
       " 919: {'grad_norm': 322.1464557647705, 'ratio': 0.75},\n",
       " 920: {'grad_norm': 322.9686279296875, 'ratio': 0.75390625},\n",
       " 921: {'grad_norm': 323.7877445220947, 'ratio': 0.765625},\n",
       " 922: {'grad_norm': 324.6091251373291, 'ratio': 0.76171875},\n",
       " 923: {'grad_norm': 325.43972969055176, 'ratio': 0.76953125},\n",
       " 924: {'grad_norm': 326.26635360717773, 'ratio': 0.77734375},\n",
       " 925: {'grad_norm': 327.0952682495117, 'ratio': 0.765625},\n",
       " 926: {'grad_norm': 327.92623710632324, 'ratio': 0.765625},\n",
       " 927: {'grad_norm': 328.75829124450684, 'ratio': 0.77734375},\n",
       " 928: {'grad_norm': 329.5899257659912, 'ratio': 0.77734375},\n",
       " 929: {'grad_norm': 330.43069648742676, 'ratio': 0.765625},\n",
       " 930: {'grad_norm': 331.2692623138428, 'ratio': 0.76953125},\n",
       " 931: {'grad_norm': 332.11121940612793, 'ratio': 0.76953125},\n",
       " 932: {'grad_norm': 332.94214820861816, 'ratio': 0.78125},\n",
       " 933: {'grad_norm': 333.7831344604492, 'ratio': 0.7421875},\n",
       " 934: {'grad_norm': 334.61136627197266, 'ratio': 0.75390625},\n",
       " 935: {'grad_norm': 335.4591751098633, 'ratio': 0.734375},\n",
       " 936: {'grad_norm': 336.2994441986084, 'ratio': 0.765625},\n",
       " 937: {'grad_norm': 337.1476879119873, 'ratio': 0.74609375},\n",
       " 938: {'grad_norm': 337.99478340148926, 'ratio': 0.765625},\n",
       " 939: {'grad_norm': 338.84391593933105, 'ratio': 0.7890625},\n",
       " 940: {'grad_norm': 339.69272232055664, 'ratio': 0.7734375},\n",
       " 941: {'grad_norm': 340.54455947875977, 'ratio': 0.765625},\n",
       " 942: {'grad_norm': 341.394495010376, 'ratio': 0.76171875},\n",
       " 943: {'grad_norm': 342.2481803894043, 'ratio': 0.76953125},\n",
       " 944: {'grad_norm': 343.0970802307129, 'ratio': 0.7578125},\n",
       " 945: {'grad_norm': 343.9504585266113, 'ratio': 0.765625},\n",
       " 946: {'grad_norm': 344.8147602081299, 'ratio': 0.76953125},\n",
       " 947: {'grad_norm': 345.6576385498047, 'ratio': 0.76171875},\n",
       " 948: {'grad_norm': 346.51891899108887, 'ratio': 0.76171875},\n",
       " 949: {'grad_norm': 347.37560844421387, 'ratio': 0.76171875},\n",
       " 950: {'grad_norm': 348.2386703491211, 'ratio': 0.74609375},\n",
       " 951: {'grad_norm': 349.1059875488281, 'ratio': 0.75390625},\n",
       " 952: {'grad_norm': 349.97721672058105, 'ratio': 0.76171875},\n",
       " 953: {'grad_norm': 350.84457206726074, 'ratio': 0.75},\n",
       " 954: {'grad_norm': 351.7088928222656, 'ratio': 0.75390625},\n",
       " 955: {'grad_norm': 352.5773754119873, 'ratio': 0.7734375},\n",
       " 956: {'grad_norm': 353.4434585571289, 'ratio': 0.765625},\n",
       " 957: {'grad_norm': 354.3186740875244, 'ratio': 0.75},\n",
       " 958: {'grad_norm': 355.1893539428711, 'ratio': 0.75390625},\n",
       " 959: {'grad_norm': 356.06624031066895, 'ratio': 0.765625},\n",
       " 960: {'grad_norm': 356.94362449645996, 'ratio': 0.7734375},\n",
       " 961: {'grad_norm': 357.82280349731445, 'ratio': 0.73046875},\n",
       " 962: {'grad_norm': 358.69776916503906, 'ratio': 0.75390625},\n",
       " 963: {'grad_norm': 359.5810203552246, 'ratio': 0.76171875},\n",
       " 964: {'grad_norm': 360.45973205566406, 'ratio': 0.75390625},\n",
       " 965: {'grad_norm': 361.344913482666, 'ratio': 0.76171875},\n",
       " 966: {'grad_norm': 362.21774101257324, 'ratio': 0.76953125},\n",
       " 967: {'grad_norm': 363.09547996520996, 'ratio': 0.78125},\n",
       " 968: {'grad_norm': 363.98557472229004, 'ratio': 0.75390625},\n",
       " 969: {'grad_norm': 364.87141036987305, 'ratio': 0.75390625},\n",
       " 970: {'grad_norm': 365.76588439941406, 'ratio': 0.74609375},\n",
       " 971: {'grad_norm': 366.65528869628906, 'ratio': 0.76171875},\n",
       " 972: {'grad_norm': 367.5454502105713, 'ratio': 0.77734375},\n",
       " 973: {'grad_norm': 368.4431037902832, 'ratio': 0.7578125},\n",
       " 974: {'grad_norm': 369.3418769836426, 'ratio': 0.7421875},\n",
       " 975: {'grad_norm': 370.2367477416992, 'ratio': 0.74609375},\n",
       " 976: {'grad_norm': 371.12835121154785, 'ratio': 0.765625},\n",
       " 977: {'grad_norm': 372.0234889984131, 'ratio': 0.75},\n",
       " 978: {'grad_norm': 372.9272861480713, 'ratio': 0.76171875},\n",
       " 979: {'grad_norm': 373.82835388183594, 'ratio': 0.7421875},\n",
       " 980: {'grad_norm': 374.7385482788086, 'ratio': 0.7734375},\n",
       " 981: {'grad_norm': 375.6396903991699, 'ratio': 0.78125},\n",
       " 982: {'grad_norm': 376.5537967681885, 'ratio': 0.75390625},\n",
       " 983: {'grad_norm': 377.46872329711914, 'ratio': 0.7734375},\n",
       " 984: {'grad_norm': 378.3842086791992, 'ratio': 0.75},\n",
       " 985: {'grad_norm': 379.2972717285156, 'ratio': 0.765625},\n",
       " 986: {'grad_norm': 380.20423889160156, 'ratio': 0.75390625},\n",
       " 987: {'grad_norm': 381.10893630981445, 'ratio': 0.7578125},\n",
       " 988: {'grad_norm': 382.0242233276367, 'ratio': 0.7734375},\n",
       " 989: {'grad_norm': 382.9353332519531, 'ratio': 0.76171875},\n",
       " 990: {'grad_norm': 383.8512496948242, 'ratio': 0.75},\n",
       " 991: {'grad_norm': 384.7624740600586, 'ratio': 0.75390625},\n",
       " 992: {'grad_norm': 385.684757232666, 'ratio': 0.76953125},\n",
       " 993: {'grad_norm': 386.60255432128906, 'ratio': 0.76953125},\n",
       " 994: {'grad_norm': 387.50822830200195, 'ratio': 0.75390625},\n",
       " 995: {'grad_norm': 388.4356880187988, 'ratio': 0.7734375},\n",
       " 996: {'grad_norm': 389.3681640625, 'ratio': 0.765625},\n",
       " 997: {'grad_norm': 390.2944984436035, 'ratio': 0.7578125},\n",
       " 998: {'grad_norm': 391.22535705566406, 'ratio': 0.7578125},\n",
       " 999: {'grad_norm': 392.16272735595703, 'ratio': 0.78125},\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_norm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "687c3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses_1 = [r['val_loss'] for r in history_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e2f5c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f062c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_ratio_eps = [i['ratio'] for i in grad_norm_1.values() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f17a6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(minimal_ratio_eps,val_losses_1)\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('minimal ratio')\n",
    "plt.title('loss_vs_minimal ratio');\n",
    "plt.plot()\n",
    "plt.savefig(\"loss_vs_minimal ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e1036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
